{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Model \n",
    "## no trainer\n",
    "\n",
    "- dataset\n",
    "- torch\n",
    "- transformers\n",
    "- transformers[torch]\n",
    "- evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, disable_caching\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    BertConfig\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/mount/arbeitsdaten31/studenten1/linku/.cache\n",
      "env: HF_MODULES_CACHE=/mount/arbeitsdaten31/studenten1/linku/.cache\n",
      "env: HF_DATASETS_CACHE=/mount/arbeitsdaten31/studenten1/linku/.cache\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR='/mount/arbeitsdaten31/studenten1/linku/.cache'\n",
    "%set_env TRANSFORMERS_CACHE $CACHE_DIR\n",
    "%set_env HF_MODULES_CACHE $CACHE_DIR\n",
    "%set_env HF_DATASETS_CACHE $CACHE_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arguments.py\n",
    "\n",
    "args_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_input_ALstrategy = 'RandomSampling'\n",
    "args_input_initseed = 100 # 1000\n",
    "args_input_quota = 200 # 200\n",
    "args_input_batch = 50 # 50\n",
    "args_input_dataset_name = 'SQuAD'\n",
    "args_input_expe_round = 5\n",
    "args_input_model_batch = 8 # already add in arguments.py\n",
    "args_input_max_length = None\n",
    "args_input_learning_rate = 3e-5\n",
    "args_input_model = 'RoBERTa'\n",
    "\n",
    "stride = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = args_input_max_length\n",
    "NUM_QUERY = args_input_batch\n",
    "NUM_INIT_LB = args_input_initseed\n",
    "ITERATION = int(args_input_quota / args_input_batch)\n",
    "DATA_NAME = args_input_dataset_name\n",
    "STRATEGY_NAME = args_input_ALstrategy\n",
    "\n",
    "MODEL_NAME = args_input_model\n",
    "LEARNING_RATE = args_input_learning_rate\n",
    "EXPE_ROUND = args_input_expe_round\n",
    "MODEL_BATCH = args_input_model_batch\n",
    "NUM_TRAIN_EPOCH = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/mount/arbeitsdaten31/studenten1/linku/models'\n",
    "pretrain_model_dir = '/mount/arbeitsdaten31/studenten1/linku/pretrain_models' + '/' + 'RoBERTa_SQuAD_full_dataset'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/mount/arbeitsdaten31/studenten1/linku/.cache/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddba0d1a6291495da44c2bf69890c758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = load_dataset('squad', cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = squad['train'].select(range(1000))\n",
    "val_data = squad['validation'].select(range(100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will preprocess the dataset (training and evaluation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples, tokenizer):\n",
    "    # no ['offset_mapping'], for .train() and .eval()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        # print('answer[\"answer_start\"]:', answer)\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        example_ids.append(examples[\"id\"][sample_idx]) # newly added for used in unlabel data predict\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_features(examples, tokenizer):\n",
    "    # keep [\"offset_mapping\"] and [\"example_id\"], for compute_metrics()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        # print('answer[\"answer_start\"]:', answer)\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        example_ids.append(examples[\"id\"][sample_idx]) # newly added for used in unlabel data predict\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples, tokenizer):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer for dataset preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bc48d8f763415d9186196cd6a8ced7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf9f905b92f416fa2368e10834a208b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54e307e6fa44b1ba514aa008e8b32a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf27f60270804dbe91d3586037ec6a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: The data name different!!!\n",
    "\n",
    "# preprocess data\n",
    "train_dataset = train_data.map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=train_data.column_names,\n",
    "    fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "train_features = train_data.map(\n",
    "    preprocess_training_features,\n",
    "    batched=True,\n",
    "    remove_columns=train_data.column_names,\n",
    "    fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "val_dataset = val_data.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=val_data.column_names,\n",
    "    fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "val_features = val_data.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=val_data.column_names,\n",
    "    fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "train_features.set_format(\"torch\")\n",
    "val_dataset = val_dataset.remove_columns([\"offset_mapping\"])\n",
    "val_dataset.set_format(\"torch\")\n",
    "val_features.set_format(\"torch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_model_dir = model_dir + '/lowRes_' + str(args_input_quota) + '_' + STRATEGY_NAME + '_' + 'RoBERTa' +  '_' + DATA_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train(num_train_epochs, train_dataloader, device, model, optimizer, lr_scheduler, record_loss=False):\n",
    "\tprint('Training was performed using the sum of {} initial data and {} query data, i.e. {} data.'.format(NUM_INIT_LB, NUM_QUERY, len(train_dataloader.dataset)))\n",
    "\tfor epoch in range(num_train_epochs):\n",
    "\t\tmodel.train()\n",
    "\t\tfor step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "\t\t\tbatch = {key: value.to(device) for key, value in batch.items()}\n",
    "\t\t\toutputs = model(**batch)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tlr_scheduler.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tif record_loss:\n",
    "\t\t\tprint('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "\tmodel_to_save = model.module if hasattr(model, 'module') else model \n",
    "\tmodel_to_save.save_pretrained(strategy_model_dir)\n",
    "\tprint('TRAIN done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(dataloader, device, features, examples):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating_pred\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    return compute_metrics(start_logits, end_logits, features, examples)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_lowRes.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unlabel_data(n_pool, labeled_idxs, train_dataset):\n",
    "    unlabeled_idxs = np.arange(n_pool)[~labeled_idxs]\n",
    "    unlabeled_data = train_dataset.select(indices=unlabeled_idxs)\n",
    "    return unlabeled_idxs, unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    softmax_num = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    return np.round(softmax_num, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aubc(quota, bsize, resseq):\n",
    "\t# it is equal to use np.trapz for calculation\n",
    "\tressum = 0.0\n",
    "\tif quota % bsize == 0:\n",
    "\t\tfor i in range(len(resseq)-1):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\n",
    "\telse:\n",
    "\t\tfor i in range(len(resseq)-2):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\t\tk = quota % bsize\n",
    "\t\tressum = ressum + ((resseq[-1] + resseq[-2]) * k / 2)\n",
    "\tressum = round(ressum / quota,3)\n",
    "\t\n",
    "\treturn ressum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_stddev(datax):\n",
    "\treturn round(np.mean(datax),4),round(np.std(datax),4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling_query(labeled_idxs, n):\n",
    "    print('Random querying starts!')\n",
    "    return np.random.choice(np.where(labeled_idxs==0)[0], n, replace=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of extra data after preprocessing\n",
    "extra = len(train_dataset) - len(train_data)\n",
    "\n",
    "SEED = 4666\n",
    "# os.environ['TORCH_HOME']='./basicmodel'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(2)\n",
    "\n",
    "# fix random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acc = []\n",
    "acq_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate initial labeled pool\n",
    "n_pool = len(train_dataset)\n",
    "labeled_idxs = np.zeros(n_pool, dtype=bool)\n",
    "\n",
    "## record acc performance \n",
    "acc = np.zeros(ITERATION) # quota/batch runs\n",
    "acc_em = np.zeros(ITERATION)\n",
    "\n",
    "## load the selected train data to DataLoader\n",
    "eval_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=MODEL_BATCH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random querying starts!\n",
      "Use pretrain model in iteration  1\n",
      "Training was performed using the sum of 100 initial data and 50 query data, i.e. 50 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b830d465ae4c9c9114c42f3148e58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb3a57a09a046569bd9b977ca79dd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d47abd712f4e11a9bcd98875b57cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "## use total_query (NUM_QUERY + extra) to query instead of just NUM_QUERY\n",
    "total_query = NUM_QUERY + extra\n",
    "\n",
    "## query\n",
    "q_idxs = random_sampling_query(labeled_idxs, total_query)\n",
    "\n",
    "# print('Time spent for querying:', (datetime.datetime.now() - time))\n",
    "# time = datetime.datetime.now()\n",
    "\n",
    "## update\n",
    "    \n",
    "## goal of total query data: NUM_QUERY * i\n",
    "num_set_query_i = NUM_QUERY * i\n",
    "\n",
    "difference_i = 0\n",
    "num_set_ex_id_i = 0\n",
    "\n",
    "while num_set_ex_id_i != num_set_query_i:        \n",
    "    labeled_idxs[q_idxs[:NUM_QUERY + difference_i]] = True\n",
    "    run_i_labeled_idxs = np.arange(n_pool)[labeled_idxs]\n",
    "\n",
    "    run_i_samples = train_features.select(indices=run_i_labeled_idxs)\n",
    "    num_set_ex_id_i = len(set(run_i_samples['example_id']))\n",
    "\n",
    "    difference_i = num_set_query_i - num_set_ex_id_i\n",
    "\n",
    "train_dataloader_i = DataLoader(\n",
    "    train_dataset.select(indices=run_i_labeled_idxs),\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=MODEL_BATCH,\n",
    ")\n",
    "\n",
    "num_update_steps_per_epoch_i = len(train_dataloader_i)\n",
    "num_training_steps_i = NUM_TRAIN_EPOCH * num_update_steps_per_epoch_i\n",
    "\n",
    "if i == 1:\n",
    "    print('Use pretrain model in iteration ', i)\n",
    "    model_i = AutoModelForQuestionAnswering.from_pretrained(pretrain_model_dir).to(device)\n",
    "else:\n",
    "    print('Use strategy model in iteration ', i)\n",
    "    model_i = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "\n",
    "optimizer_i = AdamW(model_i.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "lr_scheduler_i = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer_i,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps_i,\n",
    ")\n",
    "\n",
    "## train\n",
    "to_train(NUM_TRAIN_EPOCH, train_dataloader_i, device, model_i, optimizer_i, lr_scheduler_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = dict()\n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers[example_id] = best_answer[\"text\"]\n",
    "        else:\n",
    "            predicted_answers[example_id] = \"\"\n",
    "\n",
    "    theoretical_answers = dict()\n",
    "    for ex in examples: theoretical_answers[ex[\"id\"]] = ex[\"answers\"]['text']\n",
    "    # theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return evaluate(theoretical_answers, predicted_answers)\n",
    "    # return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd42467e4304ed09cbe313631c3479a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1802aa185f40d1a30c4d5d805c2d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 91.0, 'f1': 92.72857142857143}\n"
     ]
    }
   ],
   "source": [
    "acc_scores_i = get_pred(eval_dataloader, device, val_features, val_data)\n",
    "print(acc_scores_i)\n",
    "# predicted_answers, theoretical_answers = get_pred(eval_dataloader, device, val_features, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56be4db0acb8001400a502ed', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56be4db0acb8001400a502ee',\n",
       "  'prediction_text': \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\"},\n",
       " {'id': '56be4db0acb8001400a502ef', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56be4db0acb8001400a502f0', 'prediction_text': 'gold'},\n",
       " {'id': '56be8e613aeaaa14008c90d1', 'prediction_text': 'gold'},\n",
       " {'id': '56be8e613aeaaa14008c90d2', 'prediction_text': 'February 7, 2016'},\n",
       " {'id': '56be8e613aeaaa14008c90d3',\n",
       "  'prediction_text': 'American Football Conference'},\n",
       " {'id': '56bea9923aeaaa14008c91b9', 'prediction_text': 'gold'},\n",
       " {'id': '56bea9923aeaaa14008c91ba',\n",
       "  'prediction_text': 'American Football Conference'},\n",
       " {'id': '56bea9923aeaaa14008c91bb', 'prediction_text': 'February 7, 2016'},\n",
       " {'id': '56beace93aeaaa14008c91df', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56beace93aeaaa14008c91e0', 'prediction_text': \"Levi's Stadium\"},\n",
       " {'id': '56beace93aeaaa14008c91e1', 'prediction_text': 'Santa Clara'},\n",
       " {'id': '56beace93aeaaa14008c91e2', 'prediction_text': 'Super Bowl L'},\n",
       " {'id': '56beace93aeaaa14008c91e3', 'prediction_text': '2015'},\n",
       " {'id': '56bf10f43aeaaa14008c94fd', 'prediction_text': '2016'},\n",
       " {'id': '56bf10f43aeaaa14008c94fe', 'prediction_text': 'Santa Clara'},\n",
       " {'id': '56bf10f43aeaaa14008c94ff', 'prediction_text': \"Levi's Stadium\"},\n",
       " {'id': '56bf10f43aeaaa14008c9500', 'prediction_text': '24–10'},\n",
       " {'id': '56bf10f43aeaaa14008c9501', 'prediction_text': 'February'},\n",
       " {'id': '56d20362e7d4791d009025e8', 'prediction_text': '2015'},\n",
       " {'id': '56d20362e7d4791d009025e9', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56d20362e7d4791d009025ea', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56d20362e7d4791d009025eb', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56d600e31c85041400946eae', 'prediction_text': '2015'},\n",
       " {'id': '56d600e31c85041400946eb0', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56d600e31c85041400946eb1',\n",
       "  'prediction_text': \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\"},\n",
       " {'id': '56d9895ddc89441400fdb50e', 'prediction_text': 'Super Bowl 50'},\n",
       " {'id': '56d9895ddc89441400fdb510', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56be4e1facb8001400a502f6', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56be4e1facb8001400a502f9', 'prediction_text': 'eight'},\n",
       " {'id': '56be4e1facb8001400a502fa', 'prediction_text': '1995'},\n",
       " {'id': '56beaa4a3aeaaa14008c91c2', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56beaa4a3aeaaa14008c91c3', 'prediction_text': 'New England Patriots'},\n",
       " {'id': '56bead5a3aeaaa14008c91e9', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56bead5a3aeaaa14008c91ea', 'prediction_text': 'New England Patriots'},\n",
       " {'id': '56bead5a3aeaaa14008c91eb', 'prediction_text': 'New England Patriots'},\n",
       " {'id': '56bead5a3aeaaa14008c91ec', 'prediction_text': 'four'},\n",
       " {'id': '56bead5a3aeaaa14008c91ed', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56bf159b3aeaaa14008c9507', 'prediction_text': '15'},\n",
       " {'id': '56bf159b3aeaaa14008c9508', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56bf159b3aeaaa14008c9509', 'prediction_text': '12–4'},\n",
       " {'id': '56bf159b3aeaaa14008c950a', 'prediction_text': 'four'},\n",
       " {'id': '56bf159b3aeaaa14008c950b', 'prediction_text': 'New England Patriots'},\n",
       " {'id': '56d2045de7d4791d009025f3', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56d2045de7d4791d009025f4', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56d2045de7d4791d009025f5', 'prediction_text': 'eight'},\n",
       " {'id': '56d2045de7d4791d009025f6', 'prediction_text': 'New England Patriots'},\n",
       " {'id': '56d6017d1c85041400946ebe', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56d6017d1c85041400946ec1', 'prediction_text': 'New England Patriots'},\n",
       " {'id': '56d6017d1c85041400946ec2', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56d98a59dc89441400fdb52a', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56d98a59dc89441400fdb52b', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56d98a59dc89441400fdb52e', 'prediction_text': '1995'},\n",
       " {'id': '56be4eafacb8001400a50302', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56be4eafacb8001400a50303', 'prediction_text': 'two'},\n",
       " {'id': '56be4eafacb8001400a50304', 'prediction_text': 'The Broncos'},\n",
       " {'id': '56beab833aeaaa14008c91d2', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56beab833aeaaa14008c91d3', 'prediction_text': 'five'},\n",
       " {'id': '56beab833aeaaa14008c91d4', 'prediction_text': 'Newton'},\n",
       " {'id': '56beae423aeaaa14008c91f4', 'prediction_text': 'seven'},\n",
       " {'id': '56beae423aeaaa14008c91f5', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56beae423aeaaa14008c91f6', 'prediction_text': 'three'},\n",
       " {'id': '56beae423aeaaa14008c91f7', 'prediction_text': 'two'},\n",
       " {'id': '56bf17653aeaaa14008c9511', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56bf17653aeaaa14008c9513', 'prediction_text': 'linebacker'},\n",
       " {'id': '56bf17653aeaaa14008c9514', 'prediction_text': 'five'},\n",
       " {'id': '56bf17653aeaaa14008c9515', 'prediction_text': 'two'},\n",
       " {'id': '56d204ade7d4791d00902603', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56d204ade7d4791d00902604', 'prediction_text': 'five'},\n",
       " {'id': '56d601e41c85041400946ece', 'prediction_text': 'seven'},\n",
       " {'id': '56d601e41c85041400946ecf', 'prediction_text': 'three'},\n",
       " {'id': '56d601e41c85041400946ed0', 'prediction_text': 'fumble'},\n",
       " {'id': '56d601e41c85041400946ed1', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56d601e41c85041400946ed2', 'prediction_text': 'linebacker'},\n",
       " {'id': '56d98b33dc89441400fdb53b', 'prediction_text': 'seven'},\n",
       " {'id': '56d98b33dc89441400fdb53c', 'prediction_text': 'three'},\n",
       " {'id': '56d98b33dc89441400fdb53d', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56d98b33dc89441400fdb53e', 'prediction_text': 'five'},\n",
       " {'id': '56be5333acb8001400a5030a', 'prediction_text': 'CBS'},\n",
       " {'id': '56be5333acb8001400a5030b', 'prediction_text': '$5 million'},\n",
       " {'id': '56be5333acb8001400a5030c', 'prediction_text': 'Coldplay'},\n",
       " {'id': '56be5333acb8001400a5030d',\n",
       "  'prediction_text': 'Beyoncé and Bruno Mars'},\n",
       " {'id': '56be5333acb8001400a5030e', 'prediction_text': 'Super Bowl XLVII'},\n",
       " {'id': '56beaf5e3aeaaa14008c91fd', 'prediction_text': 'CBS'},\n",
       " {'id': '56beaf5e3aeaaa14008c91fe', 'prediction_text': '$5 million'},\n",
       " {'id': '56beaf5e3aeaaa14008c91ff', 'prediction_text': 'Bruno Mars'},\n",
       " {'id': '56beaf5e3aeaaa14008c9200', 'prediction_text': 'Bruno Mars'},\n",
       " {'id': '56beaf5e3aeaaa14008c9201', 'prediction_text': 'Coldplay'},\n",
       " {'id': '56bf1ae93aeaaa14008c951b', 'prediction_text': 'CBS'},\n",
       " {'id': '56bf1ae93aeaaa14008c951c', 'prediction_text': '$5 million'},\n",
       " {'id': '56bf1ae93aeaaa14008c951e', 'prediction_text': 'Bruno Mars'},\n",
       " {'id': '56bf1ae93aeaaa14008c951f', 'prediction_text': 'third'},\n",
       " {'id': '56d2051ce7d4791d00902608', 'prediction_text': 'CBS'},\n",
       " {'id': '56d2051ce7d4791d00902609', 'prediction_text': '$5 million'},\n",
       " {'id': '56d2051ce7d4791d0090260a', 'prediction_text': 'Coldplay'},\n",
       " {'id': '56d2051ce7d4791d0090260b',\n",
       "  'prediction_text': 'Beyoncé and Bruno Mars'},\n",
       " {'id': '56d602631c85041400946ed8', 'prediction_text': 'CBS'},\n",
       " {'id': '56d602631c85041400946eda', 'prediction_text': 'Coldplay'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ans = dict()\n",
    "for pt in predicted_answers:\n",
    "    pred_ans[pt['id']] = pt['prediction_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'56be4db0acb8001400a502ec': ['Denver Broncos',\n",
       "  'Denver Broncos',\n",
       "  'Denver Broncos'],\n",
       " '56be4db0acb8001400a502ed': ['Carolina Panthers',\n",
       "  'Carolina Panthers',\n",
       "  'Carolina Panthers'],\n",
       " '56be4db0acb8001400a502ee': ['Santa Clara, California',\n",
       "  \"Levi's Stadium\",\n",
       "  \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"],\n",
       " '56be4db0acb8001400a502ef': ['Denver Broncos',\n",
       "  'Denver Broncos',\n",
       "  'Denver Broncos'],\n",
       " '56be4db0acb8001400a502f0': ['gold', 'gold', 'gold'],\n",
       " '56be8e613aeaaa14008c90d1': ['\"golden anniversary\"',\n",
       "  'gold-themed',\n",
       "  '\"golden anniversary'],\n",
       " '56be8e613aeaaa14008c90d2': ['February 7, 2016',\n",
       "  'February 7',\n",
       "  'February 7, 2016'],\n",
       " '56be8e613aeaaa14008c90d3': ['American Football Conference',\n",
       "  'American Football Conference',\n",
       "  'American Football Conference'],\n",
       " '56bea9923aeaaa14008c91b9': ['\"golden anniversary\"', 'gold-themed', 'gold'],\n",
       " '56bea9923aeaaa14008c91ba': ['American Football Conference',\n",
       "  'American Football Conference',\n",
       "  'American Football Conference'],\n",
       " '56bea9923aeaaa14008c91bb': ['February 7, 2016',\n",
       "  'February 7',\n",
       "  'February 7, 2016'],\n",
       " '56beace93aeaaa14008c91df': ['Denver Broncos',\n",
       "  'Denver Broncos',\n",
       "  'Denver Broncos'],\n",
       " '56beace93aeaaa14008c91e0': [\"Levi's Stadium\",\n",
       "  \"Levi's Stadium\",\n",
       "  \"Levi's Stadium in the San Francisco Bay Area at Santa Clara\"],\n",
       " '56beace93aeaaa14008c91e1': ['Santa Clara', 'Santa Clara', 'Santa Clara'],\n",
       " '56beace93aeaaa14008c91e2': ['Super Bowl L', 'L', 'Super Bowl L'],\n",
       " '56beace93aeaaa14008c91e3': ['2015', 'the 2015 season', '2015'],\n",
       " '56bf10f43aeaaa14008c94fd': ['2015', '2016', '2015'],\n",
       " '56bf10f43aeaaa14008c94fe': ['Santa Clara', 'Santa Clara', 'Santa Clara'],\n",
       " '56bf10f43aeaaa14008c94ff': [\"Levi's Stadium\",\n",
       "  \"Levi's Stadium\",\n",
       "  \"Levi's Stadium\"],\n",
       " '56bf10f43aeaaa14008c9500': ['24–10', '24–10', '24–10'],\n",
       " '56bf10f43aeaaa14008c9501': ['February 7, 2016',\n",
       "  'February 7, 2016',\n",
       "  'February 7, 2016'],\n",
       " '56d20362e7d4791d009025e8': ['2015', '2016', '2016'],\n",
       " '56d20362e7d4791d009025e9': ['Denver Broncos',\n",
       "  'Denver Broncos',\n",
       "  'Denver Broncos'],\n",
       " '56d20362e7d4791d009025ea': ['Carolina Panthers',\n",
       "  'Carolina Panthers',\n",
       "  'Carolina Panthers'],\n",
       " '56d20362e7d4791d009025eb': ['Denver Broncos',\n",
       "  'Denver Broncos',\n",
       "  'Denver Broncos'],\n",
       " '56d600e31c85041400946eae': ['2015', 'the 2015 season', '2015'],\n",
       " '56d600e31c85041400946eb0': ['Denver Broncos',\n",
       "  'Denver Broncos',\n",
       "  'Denver Broncos'],\n",
       " '56d600e31c85041400946eb1': ['Santa Clara, California.',\n",
       "  \"Levi's Stadium\",\n",
       "  \"Levi's Stadium\"],\n",
       " '56d9895ddc89441400fdb50e': ['Super Bowl', 'Super Bowl', 'Super Bowl'],\n",
       " '56d9895ddc89441400fdb510': ['Denver Broncos',\n",
       "  'Denver Broncos',\n",
       "  'Denver Broncos'],\n",
       " '56be4e1facb8001400a502f6': ['Cam Newton', 'Cam Newton', 'Cam Newton'],\n",
       " '56be4e1facb8001400a502f9': ['8', 'eight', 'eight'],\n",
       " '56be4e1facb8001400a502fa': ['1995', '1995', '1995'],\n",
       " '56beaa4a3aeaaa14008c91c2': ['Arizona Cardinals',\n",
       "  'the Arizona Cardinals',\n",
       "  'Arizona Cardinals'],\n",
       " '56beaa4a3aeaaa14008c91c3': ['New England Patriots',\n",
       "  'the New England Patriots',\n",
       "  'New England Patriots'],\n",
       " '56bead5a3aeaaa14008c91e9': ['Arizona Cardinals',\n",
       "  'the Arizona Cardinals',\n",
       "  'Arizona Cardinals'],\n",
       " '56bead5a3aeaaa14008c91ea': ['New England Patriots',\n",
       "  'the New England Patriots',\n",
       "  'New England Patriots'],\n",
       " '56bead5a3aeaaa14008c91eb': ['New England Patriots',\n",
       "  'the New England Patriots',\n",
       "  'New England Patriots'],\n",
       " '56bead5a3aeaaa14008c91ec': ['four', 'four', 'four'],\n",
       " '56bead5a3aeaaa14008c91ed': ['Cam Newton', 'Cam Newton', 'Cam Newton'],\n",
       " '56bf159b3aeaaa14008c9507': ['15–1', '15–1', '15–1'],\n",
       " '56bf159b3aeaaa14008c9508': ['Cam Newton', 'Cam Newton', 'Cam Newton'],\n",
       " '56bf159b3aeaaa14008c9509': ['12–4', '12–4', '12–4'],\n",
       " '56bf159b3aeaaa14008c950a': ['4', 'four', 'four'],\n",
       " '56bf159b3aeaaa14008c950b': ['New England Patriots',\n",
       "  'the New England Patriots',\n",
       "  'New England Patriots'],\n",
       " '56d2045de7d4791d009025f3': ['Cam Newton', 'Cam Newton', 'Cam Newton'],\n",
       " '56d2045de7d4791d009025f4': ['Arizona Cardinals',\n",
       "  'the Arizona Cardinals',\n",
       "  'Arizona Cardinals'],\n",
       " '56d2045de7d4791d009025f5': ['2', 'second', 'second'],\n",
       " '56d2045de7d4791d009025f6': ['New England Patriots',\n",
       "  'the New England Patriots',\n",
       "  'New England Patriots'],\n",
       " '56d6017d1c85041400946ebe': ['Cam Newton', 'Cam Newton', 'Cam Newton'],\n",
       " '56d6017d1c85041400946ec1': ['New England Patriots',\n",
       "  'the New England Patriots',\n",
       "  'New England Patriots'],\n",
       " '56d6017d1c85041400946ec2': ['Arizona Cardinals',\n",
       "  'the Arizona Cardinals',\n",
       "  'Arizona Cardinals'],\n",
       " '56d98a59dc89441400fdb52a': ['Cam Newton', 'Cam Newton', 'Cam Newton'],\n",
       " '56d98a59dc89441400fdb52b': ['Arizona Cardinals',\n",
       "  'the Arizona Cardinals',\n",
       "  'Arizona Cardinals'],\n",
       " '56d98a59dc89441400fdb52e': ['1995.', '1995', '1995'],\n",
       " '56be4eafacb8001400a50302': ['Von Miller', 'Von Miller', 'Miller'],\n",
       " '56be4eafacb8001400a50303': ['2', 'two', 'two'],\n",
       " '56be4eafacb8001400a50304': ['Broncos', 'The Broncos', 'Broncos'],\n",
       " '56beab833aeaaa14008c91d2': ['linebacker Von Miller', 'Von Miller', 'Miller'],\n",
       " '56beab833aeaaa14008c91d3': ['five solo tackles', 'five', 'five'],\n",
       " '56beab833aeaaa14008c91d4': [\"Newton was limited by Denver's defense\",\n",
       "  'Newton',\n",
       "  'Newton'],\n",
       " '56beae423aeaaa14008c91f4': ['seven', 'seven', 'seven'],\n",
       " '56beae423aeaaa14008c91f5': ['Von Miller', 'The Broncos', 'Miller'],\n",
       " '56beae423aeaaa14008c91f6': ['three', 'three', 'three'],\n",
       " '56beae423aeaaa14008c91f7': ['two', 'two', 'two'],\n",
       " '56bf17653aeaaa14008c9511': ['Von Miller', 'Von Miller', 'Miller'],\n",
       " '56bf17653aeaaa14008c9513': ['linebacker', 'linebacker', 'linebacker'],\n",
       " '56bf17653aeaaa14008c9514': ['5', 'five', 'five'],\n",
       " '56bf17653aeaaa14008c9515': ['2', 'two', 'two'],\n",
       " '56d204ade7d4791d00902603': ['Von Miller', 'Von Miller', 'Von Miller'],\n",
       " '56d204ade7d4791d00902604': ['5', 'five', 'five'],\n",
       " '56d601e41c85041400946ece': ['seven', 'seven', 'seven'],\n",
       " '56d601e41c85041400946ecf': ['three', 'three', 'three'],\n",
       " '56d601e41c85041400946ed0': ['a fumble', 'a fumble', 'fumble'],\n",
       " '56d601e41c85041400946ed1': ['Von Miller', 'Von Miller', 'Von Miller'],\n",
       " '56d601e41c85041400946ed2': ['linebacker', 'linebacker', 'linebacker'],\n",
       " '56d98b33dc89441400fdb53b': ['seven', 'seven', 'seven'],\n",
       " '56d98b33dc89441400fdb53c': ['three', 'three', 'three'],\n",
       " '56d98b33dc89441400fdb53d': ['Von Miller', 'Von Miller', 'Von Miller'],\n",
       " '56d98b33dc89441400fdb53e': ['five', 'five', 'five'],\n",
       " '56be5333acb8001400a5030a': ['CBS', 'CBS', 'CBS'],\n",
       " '56be5333acb8001400a5030b': ['$5 million', '$5 million', '$5 million'],\n",
       " '56be5333acb8001400a5030c': ['Coldplay', 'Coldplay', 'Coldplay'],\n",
       " '56be5333acb8001400a5030d': ['Beyoncé and Bruno Mars',\n",
       "  'Beyoncé and Bruno Mars',\n",
       "  'Beyoncé and Bruno Mars'],\n",
       " '56be5333acb8001400a5030e': ['Super Bowl XLVII', 'Super Bowl XLVII', 'XLVII'],\n",
       " '56beaf5e3aeaaa14008c91fd': ['CBS', 'CBS', 'CBS'],\n",
       " '56beaf5e3aeaaa14008c91fe': ['$5 million', '$5 million', '$5 million'],\n",
       " '56beaf5e3aeaaa14008c91ff': ['Beyoncé', 'Beyoncé', 'Beyoncé'],\n",
       " '56beaf5e3aeaaa14008c9200': ['Bruno Mars', 'Bruno Mars', 'Mars'],\n",
       " '56beaf5e3aeaaa14008c9201': ['Coldplay', 'Coldplay', 'Coldplay'],\n",
       " '56bf1ae93aeaaa14008c951b': ['CBS', 'CBS', 'CBS'],\n",
       " '56bf1ae93aeaaa14008c951c': ['$5 million', '$5 million', '$5 million'],\n",
       " '56bf1ae93aeaaa14008c951e': ['Bruno Mars', 'Bruno Mars', 'Bruno Mars,'],\n",
       " '56bf1ae93aeaaa14008c951f': ['third', 'third', 'third'],\n",
       " '56d2051ce7d4791d00902608': ['CBS', 'CBS', 'CBS'],\n",
       " '56d2051ce7d4791d00902609': ['$5 million', '$5 million', '$5 million'],\n",
       " '56d2051ce7d4791d0090260a': ['Coldplay', 'Coldplay', 'Coldplay'],\n",
       " '56d2051ce7d4791d0090260b': ['Beyoncé and Bruno Mars',\n",
       "  'Beyoncé and Bruno Mars',\n",
       "  'Beyoncé and Bruno Mars'],\n",
       " '56d602631c85041400946ed8': ['CBS', 'CBS', 'CBS'],\n",
       " '56d602631c85041400946eda': ['Coldplay', 'Coldplay', 'Coldplay']}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theoretical_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(theoretical_answers, predicted_answers, skip_no_answer=False):\n",
    "    f1 = exact_match = total = 0\n",
    "    for qid, ground_truths in theoretical_answers.items():\n",
    "        if qid not in predicted_answers:\n",
    "            if not skip_no_answer:\n",
    "                message = 'Unanswered question %s will receive score 0.' % qid\n",
    "                print(message)\n",
    "                total += 1\n",
    "            continue\n",
    "        total += 1\n",
    "        prediction = predicted_answers[qid]\n",
    "        exact_match += metric_max_over_ground_truths(\n",
    "            exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(\n",
    "            f1_score, prediction, ground_truths)\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 91.0, 'f1': 92.72857142857143}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(theoretical_answers, pred_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 1 in experiment round 1\n",
      "Random querying starts!\n",
      "Time spent for querying: 0:00:00.000447\n",
      "Use pretrain model in iteration  1\n",
      "Training was performed using the sum of 100 initial data and 50 query data, i.e. 50 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4384109742ce4334b7ff25f3385ab5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1d94ed9a9441518ff93d834cab617a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143e1e0713704b0eaa8ab0ddbd7c0085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "iter_1 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4981a98c348e47bab0cf3c35cd7c2356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m## iteration i accuracy\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39miter_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m get_pred!\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i))\n\u001b[0;32m---> 84\u001b[0m acc_scores_i \u001b[39m=\u001b[39m get_pred(eval_dataloader, device, val_features, val_data)\n\u001b[1;32m     85\u001b[0m acc[i] \u001b[39m=\u001b[39m acc_scores_i[\u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     86\u001b[0m acc_em[i] \u001b[39m=\u001b[39m acc_scores_i[\u001b[39m'\u001b[39m\u001b[39mexact_match\u001b[39m\u001b[39m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[18], line 16\u001b[0m, in \u001b[0;36mget_pred\u001b[0;34m(dataloader, device, features, examples, record_loss, rd_0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     15\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mprint\u001b[39;49m(outputs)\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(outputs\u001b[39m.\u001b[39mloss)\n\u001b[1;32m     18\u001b[0m     test_loss\u001b[39m.\u001b[39mappend(outputs\u001b[39m.\u001b[39mloss)\n",
      "File \u001b[0;32m/usr/lib64/python3.10/dataclasses.py:405\u001b[0m, in \u001b[0;36m_recursive_repr.<locals>.wrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    403\u001b[0m repr_running\u001b[39m.\u001b[39madd(key)\n\u001b[1;32m    404\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     result \u001b[39m=\u001b[39m user_function(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    406\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     repr_running\u001b[39m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/linku/.venv/lib64/python3.10/site-packages/torch/_tensor.py:426\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    423\u001b[0m         Tensor\u001b[39m.\u001b[39m\u001b[39m__repr__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, tensor_contents\u001b[39m=\u001b[39mtensor_contents\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m \u001b[39m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_tensor_str\u001b[39m.\u001b[39;49m_str(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/linku/.venv/lib64/python3.10/site-packages/torch/_tensor_str.py:636\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(), torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39m_python_dispatch\u001b[39m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    635\u001b[0m     guard \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 636\u001b[0m     \u001b[39mreturn\u001b[39;00m _str_intern(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/linku/.venv/lib64/python3.10/site-packages/torch/_tensor_str.py:567\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    565\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    566\u001b[0m                 \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39;49m, indent)\n\u001b[1;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mstrided:\n\u001b[1;32m    570\u001b[0m     suffixes\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mlayout=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout))\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/linku/.venv/lib64/python3.10/site-packages/torch/_tensor_str.py:327\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    324\u001b[0m         \u001b[39mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    325\u001b[0m     )\n\u001b[1;32m    326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     formatter \u001b[39m=\u001b[39m _Formatter(get_summarized_data(\u001b[39mself\u001b[39;49m) \u001b[39mif\u001b[39;49;00m summarize \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[39mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/linku/.venv/lib64/python3.10/site-packages/torch/_tensor_str.py:115\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mlen\u001b[39m(value_str))\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     nonzero_finite_vals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmasked_select(\n\u001b[1;32m    116\u001b[0m         tensor_view, torch\u001b[39m.\u001b[39;49misfinite(tensor_view) \u001b[39m&\u001b[39;49m tensor_view\u001b[39m.\u001b[39;49mne(\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m nonzero_finite_vals\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    120\u001b[0m         \u001b[39m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    121\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# while (EXPE_ROUND > 0): \n",
    "# \tEXPE_ROUND = EXPE_ROUND - 1\n",
    "\n",
    "# \tstart = datetime.datetime.now()\n",
    "\t\n",
    "# \t## generate initial labeled pool\n",
    "# \tn_pool = len(train_dataset)\n",
    "# \tlabeled_idxs = np.zeros(n_pool, dtype=bool)\n",
    "\t\n",
    "# \t## record acc performance \n",
    "# \tacc = np.zeros(ITERATION) # quota/batch runs\n",
    "# \tacc_em = np.zeros(ITERATION)\n",
    "\n",
    "# \t## load the selected train data to DataLoader\n",
    "# \teval_dataloader = DataLoader(\n",
    "# \t\tval_dataset, \n",
    "# \t\tcollate_fn=default_data_collator, \n",
    "# \t\tbatch_size=MODEL_BATCH\n",
    "# \t)\n",
    "\n",
    "# \ttime = datetime.datetime.now()\n",
    "\t\n",
    "# \t## iteration 1 to i\n",
    "# \tfor i in range(1, ITERATION+1):\n",
    "# \t\tprint('Iteraion {} in experiment round {}'.format(i, 5 - EXPE_ROUND))\n",
    "\n",
    "# \t\t## use total_query (NUM_QUERY + extra) to query instead of just NUM_QUERY\n",
    "# \t\ttotal_query = NUM_QUERY + extra\n",
    "\n",
    "# \t\t## query\n",
    "# \t\tq_idxs = random_sampling_query(labeled_idxs, total_query)\n",
    "\n",
    "# \t\tprint('Time spent for querying:', (datetime.datetime.now() - time))\n",
    "# \t\ttime = datetime.datetime.now()\n",
    "\n",
    "# \t\t## update\n",
    "\t\t \n",
    "# \t\t## goal of total query data: NUM_QUERY * i\n",
    "# \t\tnum_set_query_i = NUM_QUERY * i\n",
    "\t\t\n",
    "# \t\tdifference_i = 0\n",
    "# \t\tnum_set_ex_id_i = 0\n",
    "\n",
    "# \t\twhile num_set_ex_id_i != num_set_query_i:        \n",
    "# \t\t\tlabeled_idxs[q_idxs[:NUM_QUERY + difference_i]] = True\n",
    "# \t\t\trun_i_labeled_idxs = np.arange(n_pool)[labeled_idxs]\n",
    "\n",
    "# \t\t\trun_i_samples = train_features.select(indices=run_i_labeled_idxs)\n",
    "# \t\t\tnum_set_ex_id_i = len(set(run_i_samples['example_id']))\n",
    "\n",
    "# \t\t\tdifference_i = num_set_query_i - num_set_ex_id_i\n",
    "\n",
    "# \t\ttrain_dataloader_i = DataLoader(\n",
    "# \t\t\ttrain_dataset.select(indices=run_i_labeled_idxs),\n",
    "# \t\t\tshuffle=True,\n",
    "# \t\t\tcollate_fn=default_data_collator,\n",
    "# \t\t\tbatch_size=MODEL_BATCH,\n",
    "# \t\t)\n",
    "\n",
    "# \t\tnum_update_steps_per_epoch_i = len(train_dataloader_i)\n",
    "# \t\tnum_training_steps_i = NUM_TRAIN_EPOCH * num_update_steps_per_epoch_i\n",
    "\n",
    "# \t\tif i == 1:\n",
    "# \t\t\tprint('Use pretrain model in iteration ', i)\n",
    "# \t\t\tmodel_i = AutoModelForQuestionAnswering.from_pretrained(pretrain_model_dir).to(device)\n",
    "# \t\telse:\n",
    "# \t\t\tprint('Use strategy model in iteration ', i)\n",
    "# \t\t\tmodel_i = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "\n",
    "# \t\toptimizer_i = AdamW(model_i.parameters(), lr=LEARNING_RATE)\n",
    "\t\t\n",
    "# \t\tlr_scheduler_i = get_scheduler(\n",
    "# \t\t\t\"linear\",\n",
    "# \t\t\toptimizer=optimizer_i,\n",
    "# \t\t\tnum_warmup_steps=0,\n",
    "# \t\t\tnum_training_steps=num_training_steps_i,\n",
    "# \t\t)\n",
    "\t\t\n",
    "# \t\t## train\n",
    "# \t\tto_train(NUM_TRAIN_EPOCH, train_dataloader_i, device, model_i, optimizer_i, lr_scheduler_i)\n",
    "\n",
    "# \t\t## iteration i accuracy\n",
    "# \t\tprint('iter_{} get_pred!'.format(i))\n",
    "# \t\tacc_scores_i = get_pred(eval_dataloader, device, val_features, val_data)\n",
    "# \t\tacc[i] = acc_scores_i['f1']\n",
    "# \t\tacc_em[i] = acc_scores_i['exact_match']\n",
    "# \t\tprint('testing accuracy {}'.format(acc[i]))\n",
    "# \t\tprint('testing accuracy em {}'.format(acc_em[i]))\n",
    "# \t\tprint('Time spent for training after querying:', (datetime.datetime.now() - time))\n",
    "# \t\ttime = datetime.datetime.now()\n",
    "# \t\tprint('\\n')\n",
    "\n",
    "# \t\ttorch.cuda.empty_cache()\n",
    "\t\n",
    "# \t## print results\n",
    "# \tprint('SEED {}'.format(SEED))\n",
    "# \tprint(STRATEGY_NAME)\n",
    "# \tprint(acc)\n",
    "# \tall_acc.append(acc)\n",
    "\t\n",
    "# \t## save model and record acq time\n",
    "# \ttimestamp = re.sub('\\.[0-9]*','_',str(datetime.datetime.now())).replace(\" \", \"_\").replace(\"-\", \"\").replace(\":\",\"\")\n",
    "# \tfinal_model_dir = model_dir + '/' + timestamp + DATA_NAME+ '_'  + STRATEGY_NAME + '_' + str(NUM_QUERY) + '_' + str(args_input_quota)\n",
    "# \tos.makedirs(final_model_dir, exist_ok=True)\n",
    "# \tend = datetime.datetime.now()\n",
    "# \tacq_time.append(round(float((end-start).seconds), 3))\n",
    "\n",
    "# \tfinal_model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "# \tmodel_to_save = final_model.module if hasattr(final_model, 'module') else final_model \n",
    "# \tmodel_to_save.save_pretrained(final_model_dir)\n",
    "\n",
    "# # cal mean & standard deviation\n",
    "# acc_m = []\n",
    "# file_name_res_tot = str(args_input_quota) + '_' + STRATEGY_NAME + '_' + MODEL_NAME + '_' + DATA_NAME + '_normal_res_tot.txt'\n",
    "# file_res_tot =  open(os.path.join(os.path.abspath('') + '/results_lowRes', '%s' % file_name_res_tot),'w')\n",
    "\n",
    "# file_res_tot.writelines('dataset: {}'.format(DATA_NAME) + '\\n')\n",
    "# file_res_tot.writelines('AL strategy: {}'.format(STRATEGY_NAME) + '\\n')\n",
    "# file_res_tot.writelines('number of unlabeled pool: {}'.format(len(train_dataset)) + '\\n')\n",
    "# file_res_tot.writelines('number of testing pool: {}'.format(len(val_dataset)) + '\\n')\n",
    "# file_res_tot.writelines('batch size: {}'.format(NUM_QUERY) + '\\n')\n",
    "# file_res_tot.writelines('quota: {}'.format(ITERATION*NUM_QUERY)+ '\\n')\n",
    "# file_res_tot.writelines('time of repeat experiments: {}'.format(args_input_expe_round)+ '\\n')\n",
    "\n",
    "# # result\n",
    "# for i in range(len(all_acc)):\n",
    "# \tacc_m.append(get_aubc(args_input_quota, NUM_QUERY, all_acc[i]))\n",
    "# \tprint(str(i)+': '+str(acc_m[i]))\n",
    "# \tfile_res_tot.writelines(str(i)+': '+str(acc_m[i])+'\\n')\n",
    "# mean_acc, stddev_acc = get_mean_stddev(acc_m)\n",
    "# mean_time, stddev_time = get_mean_stddev(acq_time)\n",
    "\n",
    "# print('mean AUBC(acc): '+str(mean_acc)+'. std dev AUBC(acc): '+str(stddev_acc))\n",
    "# print('mean time: '+str(mean_time)+'. std dev time: '+str(stddev_time))\n",
    "\n",
    "# file_res_tot.writelines('mean acc: '+str(mean_acc)+'. std dev acc: '+str(stddev_acc)+'\\n')\n",
    "# file_res_tot.writelines('mean time: '+str(mean_time)+'. std dev acc: '+str(stddev_time)+'\\n')\n",
    "\n",
    "# # save result\n",
    "# file_name_res = str(args_input_quota) + '_' + STRATEGY_NAME + '_' + MODEL_NAME + '_' + DATA_NAME + '_normal_res.txt'\n",
    "# file_res =  open(os.path.join(os.path.abspath('') + '/results_lowRes', '%s' % file_name_res),'w')\n",
    "\n",
    "\n",
    "# file_res.writelines('dataset: {}'.format(DATA_NAME) + '\\n')\n",
    "# file_res.writelines('AL strategy: {}'.format(STRATEGY_NAME) + '\\n')\n",
    "# file_res.writelines('number of unlabeled pool: {}'.format(len(train_dataset)) + '\\n')\n",
    "# file_res.writelines('number of testing pool: {}'.format(len(val_dataset)) + '\\n')\n",
    "# file_res.writelines('batch size: {}'.format(NUM_QUERY) + '\\n')\n",
    "# file_res.writelines('quota: {}'.format(ITERATION*NUM_QUERY)+ '\\n')\n",
    "# file_res.writelines('time of repeat experiments: {}'.format(args_input_expe_round)+ '\\n')\n",
    "# avg_acc = np.mean(np.array(all_acc),axis=0)\n",
    "# for i in range(len(avg_acc)):\n",
    "# \ttmp = 'Size of training set is ' + str(i*NUM_QUERY) + ', ' + 'accuracy is ' + str(round(avg_acc[i],4)) + '.' + '\\n'\n",
    "# \tfile_res.writelines(tmp)\n",
    "\n",
    "# file_res.close()\n",
    "# file_res_tot.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alqa",
   "language": "python",
   "name": "alqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6317ac8a4c65d05b4cf6bac76f72bfaae40b2e9380067c26c20c9afff1d8528e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
