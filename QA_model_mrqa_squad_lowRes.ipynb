{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Model \n",
    "## no trainer\n",
    "\n",
    "- dataset\n",
    "- torch\n",
    "- transformers\n",
    "- transformers[torch]\n",
    "- evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, disable_caching\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    AutoModelForQuestionAnswering\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/mount/arbeitsdaten31/studenten1/linku/.cache\n",
      "env: HF_MODULES_CACHE=/mount/arbeitsdaten31/studenten1/linku/.cache\n",
      "env: HF_DATASETS_CACHE=/mount/arbeitsdaten31/studenten1/linku/.cache\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR='/mount/arbeitsdaten31/studenten1/linku/.cache'\n",
    "%set_env TRANSFORMERS_CACHE $CACHE_DIR\n",
    "%set_env HF_MODULES_CACHE $CACHE_DIR\n",
    "%set_env HF_DATASETS_CACHE $CACHE_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arguments.py\n",
    "\n",
    "args_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_input_ALstrategy = 'MarginSampling'\n",
    "# args_input_initseed = 100 # 1000\n",
    "args_input_quota = 200 # 200\n",
    "args_input_batch = 50 # 50\n",
    "args_input_dataset_name = 'DROP'\n",
    "args_input_expe_round = 2\n",
    "args_input_model_batch = 8 # already add in arguments.py\n",
    "args_input_max_length = None\n",
    "args_input_learning_rate = 3e-5\n",
    "args_input_model = 'RoBERTa'\n",
    "\n",
    "stride = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = args_input_max_length\n",
    "NUM_QUERY = args_input_batch\n",
    "# NUM_INIT_LB = args_input_initseed\n",
    "ITERATION = int(args_input_quota / args_input_batch)\n",
    "DATA_NAME = args_input_dataset_name\n",
    "STRATEGY_NAME = args_input_ALstrategy\n",
    "\n",
    "MODEL_NAME = args_input_model\n",
    "LEARNING_RATE = args_input_learning_rate\n",
    "EXPE_ROUND = args_input_expe_round\n",
    "MODEL_BATCH = args_input_model_batch\n",
    "NUM_TRAIN_EPOCH = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/mount/arbeitsdaten31/studenten1/linku/models'\n",
    "pretrain_model_dir = '/mount/arbeitsdaten31/studenten1/linku/pretrain_models' + '/' + 'RoBERTa_SQuAD_full_dataset'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_mrqa(d):\n",
    "\t'''\n",
    "\treturn train_set, val_set\n",
    "\t'''\n",
    "\tdata = load_dataset(\"mrqa\", cache_dir=CACHE_DIR)\n",
    "\tif d == 'squad':\n",
    "\t\t# the first to 86588th in train set\n",
    "\t\t# the first to 10507th in val set\t\t\n",
    "\t\treturn data['train'].select(range(86588)), data['validation'].select(range(10507))\n",
    "\telif d == 'newsqa':\n",
    "\t\t# the 86589th to 160748th in train set\n",
    "\t\t# the 10508th to 14719th in val set\t\t\n",
    "\t\treturn data['train'].select(range(86588, 160748)), data['validation'].select(range(10507, 14719))\n",
    "\telif d == 'searchqa':\n",
    "\t\t# the 222437th to 339820th in train set\n",
    "\t\t# the 22505th to 39484th in val set\n",
    "\t\treturn data['train'].select(range(222436, 339820)), data['validation'].select(range(22504, 39484))\n",
    "\telif d == 'bioasq':\n",
    "\t\t# the first to the 1504th in the test set\n",
    "\t\tsub = data['test'].select(range(1504))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\treturn sub.select(range(len_sub_val, len(sub))), sub.select(range(len_sub_val))\n",
    "\telif d == 'textbookqa':\n",
    "\t\t# the 8131st to 9633rd\n",
    "\t\tsub = data['test'].select(range(8130, 9633))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\treturn sub.select(range(len_sub_val, len(sub))), sub.select(range(len_sub_val)) \n",
    "\telif d == 'drop': # Discrete Reasoning Over Paragraphs\n",
    "\t\t# the 1505th to 3007th in test set\n",
    "\t\tsub = data['test'].select(range(1504, 3007))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\treturn sub.select(range(len_sub_val, len(sub))), sub.select(range(len_sub_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mrqa (/mount/arbeitsdaten31/studenten1/linku/.cache/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06014472f09b4cf3a5bda2bedf5bbeaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: function different!!!\n",
    "\n",
    "# train_data, val_data = load_dataset_mrqa(DATA_NAME.lower())\n",
    "# train_data, val_data = load_dataset_mrqa('squad')\n",
    "train_data, val_data = load_dataset_mrqa('drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data.select(range(1000))\n",
    "# val_data = val_data.select(range(100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will preprocess the dataset (training and evaluation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples_lowRes(examples, tokenizer):\n",
    "    # no ['offset_mapping'], for .train() and .eval()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    detected_answers = examples[\"detected_answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = detected_answers[sample_idx]\n",
    "        start_char = answer[\"char_spans\"][0][\"start\"][0]\n",
    "        end_char = answer[\"char_spans\"][0][\"end\"][0]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        example_ids.append(examples[\"qid\"][sample_idx]) # newly added for used in unlabel data predict\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_features_lowRes(examples, tokenizer):\n",
    "    # keep [\"offset_mapping\"] and [\"example_id\"], for compute_metrics()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"detected_answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"char_spans\"][0][\"start\"][0]\n",
    "        end_char = answer[\"char_spans\"][0][\"end\"][0]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        example_ids.append(examples[\"qid\"][sample_idx]) # newly added for used in unlabel data predict\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples_lowRes(examples, tokenizer):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"qid\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer for dataset preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b277e622881d49acb58939f881083ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1353 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2c000f04cc42238b155fc37de215b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1353 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e89dc5057954edc959c09fdf8529771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2adad6a72044e3c8ea31d0c1f4c4373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: The data name different!!!\n",
    "\n",
    "# preprocess data\n",
    "train_dataset = train_data.map(\n",
    "    preprocess_training_examples_lowRes,\n",
    "    batched=True,\n",
    "    remove_columns=train_data.column_names,\n",
    "    fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "train_features = train_data.map(\n",
    "    preprocess_training_features_lowRes,\n",
    "    batched=True,\n",
    "    remove_columns=train_data.column_names,\n",
    "    fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "val_dataset = val_data.map(\n",
    "    preprocess_validation_examples_lowRes,\n",
    "    batched=True,\n",
    "    remove_columns=val_data.column_names,\n",
    "    fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "val_features = val_data.map(\n",
    "    preprocess_validation_examples_lowRes,\n",
    "    batched=True,\n",
    "    remove_columns=val_data.column_names,\n",
    "    fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "train_features.set_format(\"torch\")\n",
    "val_dataset = val_dataset.remove_columns([\"offset_mapping\"])\n",
    "val_dataset.set_format(\"torch\")\n",
    "val_features.set_format(\"torch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_lowRes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_model_dir = model_dir + '/lowRes_' + str(args_input_quota) + '_' + STRATEGY_NAME + '_' + 'RoBERTa' +  '_' + DATA_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train_lowRes(num_train_epochs, train_dataloader, device, model, optimizer, lr_scheduler, record_loss=False):\n",
    "\tprint('Training was performed using {} query data, i.e. {} data.'.format(NUM_QUERY, len(train_dataloader.dataset)))\n",
    "\tfor epoch in range(num_train_epochs):\n",
    "\t\tmodel.train()\n",
    "\t\tfor step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "\t\t\tbatch = {key: value.to(device) for key, value in batch.items()}\n",
    "\t\t\toutputs = model(**batch)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tlr_scheduler.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tif record_loss:\n",
    "\t\t\tprint('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "\tmodel_to_save = model.module if hasattr(model, 'module') else model \n",
    "\tmodel_to_save.save_pretrained(strategy_model_dir)\n",
    "\tprint('TRAIN done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_lowRes(dataloader, device, features, examples):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating_pred\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    return compute_metrics(start_logits, end_logits, features, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = dict()\n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        example_id = example[\"qid\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers[example_id] = best_answer[\"text\"]\n",
    "        else:\n",
    "            predicted_answers[example_id] = \"\"\n",
    "\n",
    "    theoretical_answers = dict()\n",
    "    for ex in examples: theoretical_answers[ex[\"qid\"]] = ex[\"answers\"]\n",
    "    return evaluate(theoretical_answers, predicted_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_lowRes(dataloader, device, features, examples, rd=0):\n",
    "    if rd == 1:\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(pretrain_model_dir).to(device)\n",
    "    else:\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating_prob\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    prob_dict = {}\n",
    "    example_to_features = defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"qid\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "        \n",
    "            if len(answers) > 1:\n",
    "                prob_dict[feature_index] = softmax(answers)\n",
    "            elif example_to_features[example_id] != []:\n",
    "                prob_dict[feature_index] = np.array([0])\n",
    "    \n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_dropout_lowRes(dataloader, device, features, examples, n_drop=10, rd=0):\n",
    "    if rd == 1:\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(pretrain_model_dir).to(device)\n",
    "    else:\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.train()\n",
    "    prob_dict = {}\n",
    "    \n",
    "    for i in range(n_drop):\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating_prob_dropout\"):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "            end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "        start_logits = np.concatenate(start_logits)\n",
    "        end_logits = np.concatenate(end_logits)\n",
    "        start_logits = start_logits[: len(features)]\n",
    "        end_logits = end_logits[: len(features)]\n",
    "\n",
    "        example_to_features = defaultdict(list)\n",
    "        max_answer_length = 30\n",
    "        n_best = 20\n",
    "            \n",
    "        for idx, feature in enumerate(features):\n",
    "            example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "        n = 0\n",
    "        for example in tqdm(examples):\n",
    "            example_id = example[\"qid\"]\n",
    "            answers = []\n",
    "\n",
    "            # Loop through all features associated with that example\n",
    "            for feature_index in example_to_features[example_id]:\n",
    "                start_logit = start_logits[feature_index]\n",
    "                end_logit = end_logits[feature_index]\n",
    "                offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # Skip answers that are not fully in the context\n",
    "                        if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                            continue\n",
    "                        # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                        if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                        ):\n",
    "                            continue\n",
    "\n",
    "                        answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "\n",
    "            if 1 < len(answers) < 200: # pad to same numbers of possible answers\n",
    "                zero_list = [0] * (200 - len(answers))\n",
    "                answers.extend(zero_list)\n",
    "            elif len(answers) >= 200:\n",
    "                answers = answers[:200]\n",
    "\n",
    "            if len(answers) > 1:\n",
    "                if example_to_features[example_id][0] not in prob_dict:\n",
    "                    prob_dict[example_to_features[example_id][0]] = softmax(answers)\n",
    "                else:\n",
    "                    prob_dict[example_to_features[example_id][0]] += softmax(answers)\n",
    "            elif example_to_features[example_id] != []:\n",
    "                if example_to_features[example_id][0] not in prob_dict:\n",
    "                    prob_dict[example_to_features[example_id][0]] = np.array([0])   \n",
    "\n",
    "    for key in prob_dict.keys():\n",
    "        prob_dict[key] /= n_drop\n",
    "\n",
    "    return prob_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unlabel_data(n_pool, labeled_idxs, train_dataset):\n",
    "    unlabeled_idxs = np.arange(n_pool)[~labeled_idxs]\n",
    "    unlabeled_data = train_dataset.select(indices=unlabeled_idxs)\n",
    "    return unlabeled_idxs, unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    softmax_num = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    return np.round(softmax_num, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aubc(quota, bsize, resseq):\n",
    "\t# it is equal to use np.trapz for calculation\n",
    "\tressum = 0.0\n",
    "\tif quota % bsize == 0:\n",
    "\t\tfor i in range(len(resseq)-1):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\n",
    "\telse:\n",
    "\t\tfor i in range(len(resseq)-2):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\t\tk = quota % bsize\n",
    "\t\tressum = ressum + ((resseq[-1] + resseq[-2]) * k / 2)\n",
    "\tressum = round(ressum / quota,3)\n",
    "\t\n",
    "\treturn ressum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_stddev(datax):\n",
    "\treturn round(np.mean(datax),4),round(np.std(datax),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(answers, predictions, skip_no_answer=False):\n",
    "    f1 = exact_match = total = 0\n",
    "    for qid, ground_truths in answers.items():\n",
    "        if qid not in predictions:\n",
    "            if not skip_no_answer:\n",
    "                message = 'Unanswered question %s will receive score 0.' % qid\n",
    "                print(message)\n",
    "                total += 1\n",
    "            continue\n",
    "        total += 1\n",
    "        prediction = predictions[qid]\n",
    "        exact_match += metric_max_over_ground_truths(\n",
    "            exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(\n",
    "            f1_score, prediction, ground_truths)\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_sampling_query(labeled_idxs, n):\n",
    "#     print('Random querying starts!')\n",
    "#     return np.random.choice(np.where(labeled_idxs==0)[0], n, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_sampling_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n, rd=0):\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=8,\n",
    "\t)\n",
    "    print('Margin querying starts!')\n",
    "    print('Query {} data from {} unlabeled training data.'.format(n, len(unlabeled_data)))\n",
    "    prob_dict = get_prob_lowRes(unlabeled_dataloader, device, unlabeled_features, examples, rd)\n",
    "    print('Got probability!')\n",
    "    uncertainties_dict = {}\n",
    "    for idx, probs in prob_dict.items():\n",
    "        if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "            sort_probs = np.sort(probs)[::-1] # This method returns a copy of the array, leaving the original array unchanged.\n",
    "            uncertainties_dict[idx] = sort_probs[0] - sort_probs[1]\n",
    "        elif idx:\n",
    "            uncertainties_dict[idx] = np.array([0])\n",
    "\n",
    "    sorted_uncertainties_list = sorted(uncertainties_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    return unlabeled_idxs[[idx for (idx, uncertainties) in sorted_uncertainties_list[:n]]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of extra data after preprocessing\n",
    "extra = min(NUM_QUERY, len(train_dataset) - len(train_data))\n",
    "\n",
    "SEED = 4666\n",
    "# os.environ['TORCH_HOME']='./basicmodel'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1)\n",
    "\n",
    "# fix random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acc = []\n",
    "acq_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate initial labeled pool\n",
    "n_pool = len(train_dataset)\n",
    "labeled_idxs = np.zeros(n_pool, dtype=bool)\n",
    "\n",
    "## record acc performance \n",
    "acc = np.zeros(ITERATION) # quota/batch runs\n",
    "acc_em = np.zeros(ITERATION)\n",
    "\n",
    "## load the selected train data to DataLoader\n",
    "eval_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=MODEL_BATCH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Margin querying starts!\n",
      "Query 100 data from 1415 unlabeled training data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821ed13f8ef24640b583d6d49f2ba892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37403eda59af4e33a26dc169f848dc51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got probability!\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "## use total_query (NUM_QUERY + extra) to query instead of just NUM_QUERY\n",
    "total_query = NUM_QUERY + extra\n",
    "\n",
    "## query\n",
    "q_idxs = margin_sampling_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query, i)\n",
    "\n",
    "# print('Time spent for querying:', (datetime.datetime.now() - time))\n",
    "# time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update\n",
    "    \n",
    "## goal of total query data: NUM_QUERY * i\n",
    "num_set_query_i = NUM_QUERY * i\n",
    "\n",
    "# difference_i = 0\n",
    "# num_set_ex_id_i = 0\n",
    "labeled_idxs[q_idxs[:NUM_QUERY]] = True\n",
    "run_i_labeled_idxs = np.arange(n_pool)[labeled_idxs]\n",
    "\n",
    "run_i_samples = train_features.select(indices=run_i_labeled_idxs)\n",
    "num_set_ex_id_i = len(set(run_i_samples['example_id']))\n",
    "\n",
    "difference_i = num_set_query_i - num_set_ex_id_i\n",
    "\n",
    "if difference_i:\n",
    "    labeled_idxs[q_idxs[NUM_QUERY:(NUM_QUERY + difference_i)]] = True\n",
    "    run_i_labeled_idxs = np.arange(n_pool)[labeled_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run_i_samples = train_features.select(indices=run_i_labeled_idxs)\n",
    "# num_set_ex_id_i = len(set(run_i_samples['example_id']))\n",
    "\n",
    "# num_set_query_i - num_set_ex_id_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use pretrain model in iteration  1\n",
      "Training was performed using 50 query data, i.e. 52 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcfafbc45cc45d0801989548f127eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e32cd30f42647df9982fe11b59b2036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baac1192453c411bbf2a22ca1e403fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_i = DataLoader(\n",
    "    train_dataset.select(indices=run_i_labeled_idxs),\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=MODEL_BATCH,\n",
    ")\n",
    "\n",
    "num_update_steps_per_epoch_i = len(train_dataloader_i)\n",
    "num_training_steps_i = NUM_TRAIN_EPOCH * num_update_steps_per_epoch_i\n",
    "\n",
    "if i == 1:\n",
    "    print('Use pretrain model in iteration ', i)\n",
    "    model_i = AutoModelForQuestionAnswering.from_pretrained(pretrain_model_dir).to(device)\n",
    "else:\n",
    "    print('Use strategy model in iteration ', i)\n",
    "    model_i = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "\n",
    "optimizer_i = AdamW(model_i.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "lr_scheduler_i = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer_i,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps_i,\n",
    ")\n",
    "\n",
    "## train\n",
    "to_train_lowRes(NUM_TRAIN_EPOCH, train_dataloader_i, device, model_i, optimizer_i, lr_scheduler_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_1 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab22f71ffc2f4f649ab05fdd5bbcfabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cda254444484bd496fcb5092586b67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 49.63968253968253\n",
      "testing accuracy em 40.666666666666664\n"
     ]
    }
   ],
   "source": [
    "# iteration i accuracy\n",
    "print('iter_{} get_pred!'.format(i))\n",
    "acc_scores_i = get_pred_lowRes(eval_dataloader, device, val_features, val_data)\n",
    "acc[i] = acc_scores_i['f1']\n",
    "acc_em[i] = acc_scores_i['exact_match']\n",
    "print('testing accuracy {}'.format(acc[i]))\n",
    "print('testing accuracy em {}'.format(acc_em[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Margin querying starts!\n",
      "Query 100 data from 1363 unlabeled training data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db59d757bcb94f39b3f4572cbe820153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob:   0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a69e147a0f4c5ca66301370a7df330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got probability!\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "## use total_query (NUM_QUERY + extra) to query instead of just NUM_QUERY\n",
    "total_query = NUM_QUERY + extra\n",
    "\n",
    "## query\n",
    "q_idxs = margin_sampling_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query, i)\n",
    "\n",
    "# print('Time spent for querying:', (datetime.datetime.now() - time))\n",
    "# time = datetime.datetime.now()\n",
    "\n",
    "## update\n",
    "    \n",
    "## goal of total query data: NUM_QUERY * i\n",
    "num_set_query_i = NUM_QUERY * i\n",
    "\n",
    "# difference_i = 0\n",
    "# num_set_ex_id_i = 0\n",
    "labeled_idxs[q_idxs[:NUM_QUERY]] = True\n",
    "run_i_labeled_idxs = np.arange(n_pool)[labeled_idxs]\n",
    "\n",
    "run_i_samples = train_features.select(indices=run_i_labeled_idxs)\n",
    "num_set_ex_id_i = len(set(run_i_samples['example_id']))\n",
    "\n",
    "difference_i = num_set_query_i - num_set_ex_id_i\n",
    "\n",
    "if difference_i:\n",
    "    labeled_idxs[q_idxs[NUM_QUERY:(NUM_QUERY + difference_i)]] = True\n",
    "    run_i_labeled_idxs = np.arange(n_pool)[labeled_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use strategy model in iteration  2\n",
      "Training was performed using 50 query data, i.e. 106 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5e582db7f94a18a337ae571ea2fe5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0941844e592f4e3aa907041309c8f8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94186b187bfb4ad9bee551f547f5e2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "iter_2 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4cbfe869da41f9b420ab1ad553f64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6460b70a1ea44382b203d0faa098997c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 56.44481074481076\n",
      "testing accuracy em 48.0\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_i = DataLoader(\n",
    "    train_dataset.select(indices=run_i_labeled_idxs),\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=MODEL_BATCH,\n",
    ")\n",
    "\n",
    "num_update_steps_per_epoch_i = len(train_dataloader_i)\n",
    "num_training_steps_i = NUM_TRAIN_EPOCH * num_update_steps_per_epoch_i\n",
    "\n",
    "if i == 1:\n",
    "    print('Use pretrain model in iteration ', i)\n",
    "    model_i = AutoModelForQuestionAnswering.from_pretrained(pretrain_model_dir).to(device)\n",
    "else:\n",
    "    print('Use strategy model in iteration ', i)\n",
    "    model_i = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "\n",
    "optimizer_i = AdamW(model_i.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "lr_scheduler_i = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer_i,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps_i,\n",
    ")\n",
    "\n",
    "## train\n",
    "to_train_lowRes(NUM_TRAIN_EPOCH, train_dataloader_i, device, model_i, optimizer_i, lr_scheduler_i)\n",
    "\n",
    "## iteration i accuracy\n",
    "print('iter_{} get_pred!'.format(i))\n",
    "acc_scores_i = get_pred_lowRes(eval_dataloader, device, val_features, val_data)\n",
    "acc[i] = acc_scores_i['f1']\n",
    "acc_em[i] = acc_scores_i['exact_match']\n",
    "print('testing accuracy {}'.format(acc[i]))\n",
    "print('testing accuracy em {}'.format(acc_em[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alqa",
   "language": "python",
   "name": "alqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6317ac8a4c65d05b4cf6bac76f72bfaae40b2e9380067c26c20c9afff1d8528e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
