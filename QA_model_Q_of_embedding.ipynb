{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Model \n",
    "## no trainer\n",
    "\n",
    "- dataset\n",
    "- torch\n",
    "- transformers\n",
    "- transformers[torch]\n",
    "- evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/arbeitsdaten31/studenten1/linku/.venv/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    BertConfig\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import os\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/mount/arbeitsdaten31/studenten1/linku/cache\n",
      "env: HF_MODULES_CACHE=/mount/arbeitsdaten31/studenten1/linku/cache\n",
      "env: HF_DATASETS_CACHE=/mount/arbeitsdaten31/studenten1/linku/cache\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR='/mount/arbeitsdaten31/studenten1/linku/cache'\n",
    "%set_env TRANSFORMERS_CACHE $CACHE_DIR\n",
    "%set_env HF_MODULES_CACHE $CACHE_DIR\n",
    "%set_env HF_DATASETS_CACHE $CACHE_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arguments.py\n",
    "\n",
    "args_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_input_ALstrategy = 'KMeansSampling'\n",
    "args_input_initseed = 100 # 1000\n",
    "args_input_quota = 100 # 1000\n",
    "args_input_batch = 35 # 128\n",
    "args_input_dataset_name = 'SQuAD'\n",
    "args_input_iteration = 1\n",
    "args_input_model_batch = 8 # already add in arguments.py\n",
    "args_input_max_length = 384\n",
    "\n",
    "stride = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = args_input_max_length\n",
    "NUM_QUERY = args_input_batch\n",
    "NUM_INIT_LB = args_input_initseed\n",
    "NUM_ROUND = int(args_input_quota / args_input_batch)\n",
    "DATA_NAME = args_input_dataset_name\n",
    "STRATEGY_NAME = args_input_ALstrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/mount/arbeitsdaten31/studenten1/linku/models'\n",
    "pretrain_model_dir = model_dir + '/' + 'SQuAD_100_Bert'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/users1/linku/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00, 232.95it/s]\n"
     ]
    }
   ],
   "source": [
    "squad = load_dataset(args_input_dataset_name.lower())\n",
    "# squad[\"train\"] = squad[\"train\"].shuffle(42).select(range(2000))\n",
    "squad[\"train\"] = squad[\"train\"].select(range(4000))\n",
    "squad[\"validation\"] = squad[\"validation\"].select(range(1500))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will preprocess the dataset (training and evaluation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_features(examples):\n",
    "    # keep [\"offset_mapping\"], for compute_metrics()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        example_ids.append(examples[\"id\"][sample_idx]) # newly added for used in unlabel data predict\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples):\n",
    "    # no ['offset_mapping'], for .train() and .eval()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        example_ids.append(examples[\"id\"][sample_idx]) # newly added for used in unlabel data predict\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/users1/linku/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-321eeeaef86c6587.arrow\n",
      "Loading cached processed dataset at /home/users1/linku/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-17c5bd481660301a.arrow\n",
      "Loading cached processed dataset at /home/users1/linku/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-196906632b81856e.arrow\n",
      "Loading cached processed dataset at /home/users1/linku/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-196906632b81856e.arrow\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer for dataset preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# preprocess data\n",
    "train_dataset = squad[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"train\"].column_names,\n",
    ")\n",
    "train_features = squad[\"train\"].map(\n",
    "    preprocess_training_features,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"train\"].column_names,\n",
    ")\n",
    "val_dataset = squad[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"validation\"].column_names,\n",
    ")\n",
    "val_features = squad[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "train_features.set_format(\"torch\")\n",
    "val_dataset = val_dataset.remove_columns([\"offset_mapping\"])\n",
    "val_dataset.set_format(\"torch\")\n",
    "val_features.set_format(\"torch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train(num_train_epochs, train_dataloader, device, model, optimizer, lr_scheduler, record_loss=False):\n",
    "\tprint('Num of train dataset:', len(train_dataloader.dataset))\n",
    "\tfor epoch in range(num_train_epochs):\n",
    "\t\tmodel.train()\n",
    "\t\tfor step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "\t\t\tbatch = {key: value.to(device) for key, value in batch.items()}\n",
    "\t\t\toutputs = model(**batch)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tlr_scheduler.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tif record_loss:\n",
    "\t\t\tprint('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "\tmodel_to_save = model.module if hasattr(model, 'module') else model \n",
    "\tmodel_to_save.save_pretrained(model_dir)\n",
    "\tprint('TRAIN done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    \n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(eval_dataloader, device, features, examples, record_loss=False, rd_0=False):\n",
    "    if rd_0:\n",
    "        config = BertConfig.from_pretrained(pretrain_model_dir, output_hidden_states=True)\n",
    "    else:\n",
    "        config = BertConfig.from_pretrained(model_dir, output_hidden_states=True)\n",
    "    model = AutoModelForQuestionAnswering.from_config(config).to(device)\n",
    "    \n",
    "    test_loss = []\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating_pred\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            print(outputs)\n",
    "            print(outputs.loss)\n",
    "            test_loss.append(outputs.loss)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    if record_loss:\n",
    "        test_loss /= len(eval_dataloader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n",
    "\n",
    "    return compute_metrics(start_logits, end_logits, features, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(eval_dataloader, device, features, examples):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_dir).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating_prob\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    prob_dict = {}\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20 # TODO: if set n_best as 5, will it effect the time??\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        example_id = example[\"id\"]\n",
    "        # context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "        \n",
    "            if len(answers) > 1:\n",
    "                prob_dict[feature_index] = softmax(answers)\n",
    "            elif example_to_features[example_id] != []:\n",
    "                prob_dict[feature_index] = np.array([0])\n",
    "    \n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_dropout(eval_dataloader, device, features, examples, n_drop=10):\n",
    "    # deepAL+: self.clf.train()\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_dir).to(device)\n",
    "    model.train()\n",
    "    # deepAL+: probs = torch.zeros([len(data), len(np.unique(data.Y))])\n",
    "    # deepAL+: loader = DataLoader(data, shuffle=False, **self.params['loader_te_args'])\n",
    "    prob_dict = {}\n",
    "    # deepAL+: for i in range(n_drop):\n",
    "    # deepAL+:     with torch.no_grad():\n",
    "    # deepAL+:         for x, y, idxs in loader:\n",
    "    # deepAL+:             x, y = x.to(self.device), y.to(self.device)\n",
    "    # deepAL+:             out, e1 = self.clf(x)\n",
    "    # deepAL+:             prob = F.softmax(out, dim=1)\n",
    "    # deepAL+:             probs[idxs] += prob.cpu()\n",
    "    for i in range(n_drop):\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating_prob_dropout\"):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "            end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "        start_logits = np.concatenate(start_logits)\n",
    "        end_logits = np.concatenate(end_logits)\n",
    "        start_logits = start_logits[: len(features)]\n",
    "        end_logits = end_logits[: len(features)]\n",
    "\n",
    "        example_to_features = collections.defaultdict(list)\n",
    "        max_answer_length = 30\n",
    "        n_best = 20\n",
    "            \n",
    "        for idx, feature in enumerate(features):\n",
    "            example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "        n = 0\n",
    "        for example in tqdm(examples):\n",
    "            example_id = example[\"id\"]\n",
    "            answers = []\n",
    "\n",
    "            # Loop through all features associated with that example\n",
    "            for feature_index in example_to_features[example_id]:\n",
    "                start_logit = start_logits[feature_index]\n",
    "                end_logit = end_logits[feature_index]\n",
    "                offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # Skip answers that are not fully in the context\n",
    "                        if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                            continue\n",
    "                        # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                        if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                        ):\n",
    "                            continue\n",
    "\n",
    "                        answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "\n",
    "            if 1 < len(answers) < 200: # pad to same numbers of possible answers\n",
    "                zero_list = [0] * (200 - len(answers))\n",
    "                answers.extend(zero_list)\n",
    "            elif len(answers) >= 200:\n",
    "                answers = answers[:200]\n",
    "\n",
    "            if len(answers) > 1:\n",
    "                if feature_index not in prob_dict:\n",
    "                    prob_dict[feature_index] = softmax(answers)\n",
    "                else:\n",
    "                    prob_dict[feature_index] += softmax(answers)\n",
    "            elif example_to_features[example_id] != []:\n",
    "                if feature_index not in prob_dict:\n",
    "                    prob_dict[feature_index] = np.array([0])   \n",
    "\n",
    "    for key in prob_dict.keys():\n",
    "        prob_dict[key] /= n_drop\n",
    "\n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_dropout_split(eval_dataloader, device, features, examples, n_drop=10):\n",
    "    ## use tensor to save the answers\n",
    "    \n",
    "    # deepAL+: self.clf.train()\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_dir).to(device)\n",
    "    model.train()\n",
    "    # deepAL+: probs = torch.zeros([len(data), len(np.unique(data.Y))])\n",
    "    # deepAL+: loader = DataLoader(data, shuffle=False, **self.params['loader_te_args'])\n",
    "    probs = torch.zeros([n_drop, len(eval_dataloader.dataset), 200])\n",
    "    for_check = []\n",
    "    # deepAL+: for i in range(n_drop):\n",
    "    # deepAL+:     with torch.no_grad():\n",
    "    # deepAL+:         for x, y, idxs in loader:\n",
    "    # deepAL+:             x, y = x.to(self.device), y.to(self.device)\n",
    "    # deepAL+:             out, e1 = self.clf(x)\n",
    "    # deepAL+:             prob = F.softmax(out, dim=1)\n",
    "    # deepAL+:             probs[i][idxs] += F.softmax(out, dim=1).cpu()\n",
    "    for i in range(n_drop):\n",
    "        prob_dict = {}\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating_prob_dropout\"):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "            end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "        start_logits = np.concatenate(start_logits)\n",
    "        end_logits = np.concatenate(end_logits)\n",
    "        start_logits = start_logits[: len(features)]\n",
    "        end_logits = end_logits[: len(features)]\n",
    "\n",
    "        example_to_features = collections.defaultdict(list)\n",
    "        max_answer_length = 30\n",
    "        n_best = 20\n",
    "            \n",
    "        for idx, feature in enumerate(features):\n",
    "            example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "        n = 0\n",
    "        for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "            example_id = example[\"id\"]\n",
    "            # context = example[\"context\"]\n",
    "            answers = []\n",
    "\n",
    "            # Loop through all features associated with that example\n",
    "            for feature_index in example_to_features[example_id]:\n",
    "                start_logit = start_logits[feature_index]\n",
    "                end_logit = end_logits[feature_index]\n",
    "                offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # Skip answers that are not fully in the context\n",
    "                        if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                            continue\n",
    "                        # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                        if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                        ):\n",
    "                            continue\n",
    "\n",
    "                        answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "\n",
    "            \n",
    "                if 1 < len(answers) < 200: # pad to same numbers of possible answers\n",
    "                    zero_list = [0] * (200 - len(answers))\n",
    "                    answers.extend(zero_list)\n",
    "                elif len(answers) >= 200:\n",
    "                    answers = answers[:200]\n",
    "\n",
    "                probs[i][feature_index] += torch.tensor(softmax(answers))\n",
    "\n",
    "            # if n == 0 and len(softmax(answers)) > 1:\n",
    "            #     for_check.append(answers[:5])\n",
    "            #     n += 1 \n",
    "\n",
    "    # return prob_dict, for_check\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(eval_dataloader, device, features, examples, rd):\n",
    "    if rd == 1:\n",
    "        config = BertConfig.from_pretrained(pretrain_model_dir, output_hidden_states=True)\n",
    "    else: \n",
    "        config = BertConfig.from_pretrained(model_dir, output_hidden_states=True)\n",
    "    model = AutoModelForQuestionAnswering.from_config(config).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    embeddings = torch.zeros([len(eval_dataloader.dataset), model.config.to_dict()['hidden_size']])\n",
    "    idxs_start = 0\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating_prob\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            # print('len_output:', len(outputs)) # 4\n",
    "            # print('outputs:', outputs) # (loss, start_logits, end_logits, hidden_states)\n",
    "\n",
    "        hidden_states = outputs.hidden_states\n",
    "        # print('len_hidden_states:', len(hidden_states)) # 13 # each one has: (batch_size, sequence_length, hidden_size)\n",
    "        # # hidden_states[0] -> last hidden states\n",
    "        # print('len_hidden_states[0]:', len(hidden_states[0])) # 8, 8, 4\n",
    "        # print('len_hidden_states[0][0]:', len(hidden_states[0][0])) # 384, 384, 384 # tokens in each sequence\n",
    "        # print('len_hidden_states[0][0][0]:', len(hidden_states[0][0][0])) # 768, 768, 768 # number of hidden units\n",
    "        # print('hidden_states:', hidden_states) \n",
    "\n",
    "        # TODO: Question!!!!!\n",
    "        embedding_of_last_layer = hidden_states[0][:, 1, :] # [:, 0, :] -> to get [cls], but all the same\n",
    "        # print(embedding_of_last_layer[0][0])\n",
    "        idxs_end = idxs_start + len(hidden_states[0])\n",
    "        # print(idxs_start)\n",
    "        # print(idxs_end)\n",
    "        embeddings[idxs_start:idxs_end] = embedding_of_last_layer.cpu()\n",
    "        idxs_start = idxs_end\n",
    "        \n",
    "    return embeddings "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unlabel_data(n_pool, labeled_idxs, train_dataset):\n",
    "    unlabeled_idxs = np.arange(n_pool)[~labeled_idxs]\n",
    "    unlabeled_data = train_dataset.select(indices=unlabeled_idxs)\n",
    "    return unlabeled_idxs, unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling_query(labeled_idxs, n, rd):\n",
    "    return np.random.choice(np.where(labeled_idxs==0)[0], n, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_sampling_query(n_pool, labeled_idxs, train_dataset, train_features, examples, model, device, n, rd):\n",
    "    # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data,\n",
    "\t\tshuffle=True,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=8,\n",
    "\t)\n",
    "    # TODO: print for recording\n",
    "    print('Margin querying starts!')\n",
    "    # deepAL+: probs = self.predict_prob(unlabeled_data)\n",
    "    prob_dict = get_prob(unlabeled_dataloader, device, unlabeled_features, examples)\n",
    "    # TODO: print for recording\n",
    "    print('Got probability!')\n",
    "    # deepAL+: probs_sorted, _ = probs.sort(descending=True)\n",
    "    # deepAL+: uncertainties = probs_sorted[:, 0] - probs_sorted[:,1]\n",
    "    uncertainties_dict = {}\n",
    "    for idx, probs in prob_dict.items():\n",
    "        if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "            sort_probs = np.sort(probs)[::-1] # This method returns a copy of the array, leaving the original array unchanged.\n",
    "            uncertainties_dict[idx] = sort_probs[0] - sort_probs[1]\n",
    "        elif idx:\n",
    "            uncertainties_dict[idx] = np.array([0])\n",
    "\n",
    "    # deepAL+: return unlabeled_idxs[uncertainties.sort()[1][:n]] \n",
    "    sorted_uncertainties_list = sorted(uncertainties_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return unlabeled_idxs[[idx for (idx, uncertainties) in sorted_uncertainties_list[:n]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_confidence_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n, rd):\n",
    "    # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data,\n",
    "\t\tshuffle=True,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=8,\n",
    "\t)\n",
    "    # TODO: print for recording\n",
    "    print('LC querying starts!')\n",
    "    # deepAL+: probs = self.predict_prob(unlabeled_data)\n",
    "    prob_dict = get_prob(unlabeled_dataloader, device, unlabeled_features, examples)\n",
    "    # TODO: print for recording\n",
    "    print('Got probability!')\n",
    "    # deepAL+: uncertainties = probs.max(1)[0]\n",
    "    confidence_dict = {}\n",
    "    for idx, probs in prob_dict.items():\n",
    "        if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "            confidence_dict[idx] = max(probs)\n",
    "        elif idx:\n",
    "            confidence_dict[idx] = np.array([0])\n",
    "\n",
    "    # deepAL+: return unlabeled_idxs[uncertainties.sort()[1][:n]]\n",
    "    sorted_confidence_list = sorted(confidence_dict.items, key=lambda x: x[1])\n",
    "    return unlabeled_idxs[[idx for (idx, confidence) in sorted_confidence_list[:n]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_ratio_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n, rd):\n",
    "    # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data,\n",
    "\t\tshuffle=True,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=8,\n",
    "\t)\n",
    "    # TODO: print for recording\n",
    "    print('Var Ratio querying starts!')\n",
    "    # deepAL+: probs = self.predict_prob(unlabeled_data)\n",
    "    prob_dict = get_prob(unlabeled_dataloader, device, unlabeled_features, examples)\n",
    "    # TODO: print for recording\n",
    "    print('Got probability!')\n",
    "    # deepAL+: preds = torch.max(probs, 1)[0]\n",
    "    # deepAL+: uncertainties = 1.0 - preds\n",
    "    confidence_dict = {}\n",
    "    for idx, probs in prob_dict.items():\n",
    "        if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "            confidence_dict[idx] = 1.0 - max(probs)\n",
    "        elif idx:\n",
    "            confidence_dict[idx] = np.array([0])\n",
    "\n",
    "    # deepAL+: return unlabeled_idxs[uncertainties.sort(descending=True)[1][:n]]\n",
    "    sorted_confidence_list = sorted(confidence_dict.items, key=lambda x: x[1], reverse=True)\n",
    "    return unlabeled_idxs[[idx for (idx, confidence) in sorted_confidence_list[:n]]]\n",
    "# comment for the same query as LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n, rd):\n",
    "    # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data,\n",
    "\t\tshuffle=True,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=8,\n",
    "\t)\n",
    "    # deepAL+: probs = self.predict_prob(unlabeled_data)\n",
    "    # TODO: print for recording\n",
    "    print('Entropy querying starts!')\n",
    "    prob_dict = get_prob(unlabeled_dataloader, device, unlabeled_features, examples)\n",
    "    # TODO: print for recording\n",
    "    print('Got probability!')\n",
    "    # deepAL+: log_probs = torch.log(probs)\n",
    "    # deepAL+: uncertainties = (probs*log_probs).sum(1)\n",
    "    entropy_dict = {}\n",
    "    for idx, probs in prob_dict.items():\n",
    "        if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "            log_probs = np.log(probs)\n",
    "            entropy_dict[idx] = (probs*log_probs).sum()\n",
    "        elif idx:\n",
    "            entropy_dict[idx] = np.array([0])\n",
    "    # deepAL+: return unlabeled_idxs[uncertainties.sort()[1][:n]]\n",
    "    sorted_entropy_list = sorted(entropy_dict.items(), key=lambda x: x[1])\n",
    "    return unlabeled_idxs[[idx for (idx, entropy) in sorted_entropy_list[:n]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_sampling_dropout_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n, rd):\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data,\n",
    "\t\tshuffle=True,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=8,\n",
    "\t)\n",
    "    # TODO: print for recording\n",
    "    print('Margin dropout querying starts!')\n",
    "    prob_dict = get_prob_dropout(unlabeled_dataloader, device, unlabeled_features, examples)\n",
    "    # TODO: print for recording\n",
    "    print('Got probability!')\n",
    "    uncertainties_dict = {}\n",
    "    for idx, probs in prob_dict.items():\n",
    "        if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "            sort_probs = np.sort(probs)[::-1] # This method returns a copy of the array, leaving the original array unchanged.\n",
    "            uncertainties_dict[idx] = sort_probs[0] - sort_probs[1]\n",
    "        elif idx:\n",
    "            uncertainties_dict[idx] = np.array([0])\n",
    "\n",
    "    sorted_uncertainties_list = sorted(uncertainties_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    return unlabeled_idxs[[idx for (idx, uncertainties) in sorted_uncertainties_list[:n]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n, rd):\n",
    "    # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "  \t\tunlabeled_data,\n",
    "      shuffle=True,\n",
    "      collate_fn=default_data_collator,\n",
    "      batch_size=8,\n",
    "    )\n",
    "    # TODO: print for recording\n",
    "    print('Mean STD querying starts!')\n",
    "    # deepAL+: probs = self.predict_prob_dropout_split(unlabeled_data, n_drop=self.n_drop).numpy()\n",
    "    probs = get_prob_dropout_split(unlabeled_dataloader, device, unlabeled_features, examples).numpy()\n",
    "    # TODO: print for recording\n",
    "    print('Got probability!')\n",
    "    # deepAL+: sigma_c = np.std(probs, axis=0)\n",
    "    sigma_c = np.std(probs, axis=0)\n",
    "    # deepAL+: uncertainties = torch.from_numpy(np.mean(sigma_c, axis=-1))\n",
    "    uncertainties = torch.from_numpy(np.mean(sigma_c, axis=-1)) # use tensor.sort() will sort the data and produce sorted indexes\n",
    "    # deepAL+: return unlabeled_idxs[uncertainties.sort(descending=True)[1][:n]] # [1]: to get sorted data's indexes\n",
    "    return unlabeled_idxs[uncertainties.sort(descending=True)[1][:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n, rd):\n",
    "    # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "      unlabeled_data,\n",
    "      shuffle=True,\n",
    "      collate_fn=default_data_collator,\n",
    "      batch_size=8,\n",
    "    )\n",
    "    # deepAL+: probs = self.predict_prob_dropout_split(unlabeled_data, n_drop=self.n_drop)\n",
    "    probs = get_prob_dropout_split(unlabeled_dataloader, device, unlabeled_features, examples)\n",
    "    # deepAL+: pb = probs.mean(0)\n",
    "    probs_mean = probs.mean(0)\n",
    "    # deepAL+: entropy1 = (-pb*torch.log(pb)).sum(1)\n",
    "    entropy1 = (-probs_mean*torch.log(probs_mean)).sum(1)\n",
    "    # deepAL+: entropy2 = (-probs*torch.log(probs)).sum(2).mean(0)\n",
    "    entropy2 = (-probs*torch.log(probs)).sum(2).mean(0)\n",
    "    # deepAL+: uncertainties = entropy2 - entropy1\n",
    "    uncertainties = entropy2 - entropy1\n",
    "    # later on, we can use batch\n",
    "    # deepAL+: return unlabeled_idxs[uncertainties.sort()[1][:n]]\n",
    "    return unlabeled_idxs[uncertainties.sort()[1][:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n, rd):\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "      unlabeled_data,\n",
    "      shuffle=True,\n",
    "      collate_fn=default_data_collator,\n",
    "      batch_size=8,\n",
    "    )\n",
    "    # TODO: print for recording\n",
    "    print('BALD querying starts!')\n",
    "    probs = get_prob_dropout_split(unlabeled_dataloader, device, unlabeled_features, examples)\n",
    "    # TODO: print for recording\n",
    "    print('Got probability!')\n",
    "    probs_mean = probs.mean(0)\n",
    "    entropy1 = (-probs_mean*torch.log(probs_mean)).sum(1)\n",
    "    entropy2 = (-probs*torch.log(probs)).sum(2).mean(0)\n",
    "    uncertainties = entropy2 - entropy1\n",
    "    # later on, we can use batch\n",
    "    return unlabeled_idxs[uncertainties.sort()[1][:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n, rd):\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "  \t\tunlabeled_data,\n",
    "      shuffle=True,\n",
    "      collate_fn=default_data_collator,\n",
    "      batch_size=8,\n",
    "    )\n",
    "    # TODO: print for recording\n",
    "    print('Mean STD querying starts!')\n",
    "    probs = get_prob_dropout_split(unlabeled_dataloader, device, unlabeled_features, examples).numpy()\n",
    "    # TODO: print for recording\n",
    "    print('Got probability!')\n",
    "    sigma_c = np.std(probs, axis=0)\n",
    "    uncertainties = torch.from_numpy(np.mean(sigma_c, axis=-1)) # use tensor.sort() will sort the data and produce sorted indexes\n",
    "    return unlabeled_idxs[uncertainties.sort(descending=True)[1][:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n, rd):\n",
    "    # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "      unlabeled_data,\n",
    "      shuffle=True,\n",
    "      collate_fn=default_data_collator,\n",
    "      batch_size=8,\n",
    "    )\n",
    "    # deepAL+: embeddings = get_embeddings(unlabeled_data)\n",
    "    embeddings = get_embeddings(unlabeled_dataloader, device, unlabeled_features, examples, rd=1)\n",
    "    # deepAL+: embeddings = embeddings.numpy()\n",
    "    embeddings = embeddings.numpy()\n",
    "    # print(embeddings.shape)\n",
    "    # deepAL+: cluster_learner = KMeans(n_clusters=n)\n",
    "    cluster_learner = KMeans(n_clusters=n)\n",
    "    # deepAL+: cluster_learner.fit(embeddings)\n",
    "    cluster_learner.fit(embeddings)\n",
    "    # deepAL+: cluster_idxs = cluster_learner.predict(embeddings)\n",
    "    cluster_idxs = cluster_learner.predict(embeddings)\n",
    "    # deepAL+: centers = cluster_learner.cluster_centers_[cluster_idxs]\n",
    "    centers = cluster_learner.cluster_centers_[cluster_idxs]\n",
    "    # deepAL+: dis = (embeddings - centers)**2\n",
    "    dis = (embeddings - centers)**2\n",
    "    # deepAL+: dis = dis.sum(axis=1)\n",
    "    dis = dis.sum(axis=1)\n",
    "    # deepAL+: q_idxs = np.array([np.arange(embeddings.shape[0])[cluster_idxs==i][dis[cluster_idxs==i].argmin()] for i in range(n)])\n",
    "    q_idxs = np.array([np.arange(embeddings.shape[0])[cluster_idxs==i][dis[cluster_idxs==i].argmin()] for i in range(n)])\n",
    "\n",
    "    # deepAL+: return unlabeled_idxs[q_idxs]\n",
    "    return unlabeled_idxs[q_idxs]\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 4666\n",
    "# os.environ['TORCH_HOME']='./basicmodel'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(3)\n",
    "\n",
    "# fix random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.enabled  = True\n",
    "# torch.backends.cudnn.benchmark= True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = args_input_iteration\n",
    "model_batch = args_input_model_batch\n",
    "num_train_epochs = 3\n",
    "\n",
    "all_acc = []\n",
    "acq_time = []\n",
    "\n",
    "# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\t\n",
    "fp16_training = False\n",
    "\n",
    "if fp16_training:\n",
    "    !pip install accelerate==0.2.0\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator(fp16=True)\n",
    "    device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD\n",
      "KMeansSampling\n",
      "Round 0\n",
      "testing accuracy 13.8252\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# repeate # iteration trials\n",
    "while (iteration > 0): \n",
    "\titeration = iteration - 1\n",
    "\n",
    "\tstart = datetime.datetime.now()\n",
    "\n",
    "\t## generate initial labeled pool\n",
    "\tn_pool = len(train_dataset)\n",
    "\tlabeled_idxs = np.zeros(n_pool, dtype=bool)\n",
    "\n",
    "\ttmp_idxs = np.arange(n_pool)\n",
    "\tnp.random.shuffle(tmp_idxs)\n",
    "\tlabeled_idxs[tmp_idxs[:NUM_INIT_LB]] = True\n",
    "\n",
    "\trun_0_labeled_idxs = np.arange(n_pool)[labeled_idxs]\n",
    "\n",
    "\t## record acc performance \n",
    "\tacc = np.zeros(NUM_ROUND + 1) # quota/batch runs + run_0\n",
    "\n",
    "\t## data\n",
    "\t# eval_dataloader = DataLoader(\n",
    "\t# \tval_dataset, \n",
    "\t# \tcollate_fn=default_data_collator, \n",
    "\t# \tbatch_size=8\n",
    "\t# )\n",
    "\n",
    "\t## print info\n",
    "\tprint(DATA_NAME)\n",
    "\tprint(STRATEGY_NAME)\n",
    "\t\n",
    "\t## round 0 accuracy\n",
    "\t# acc[0] = get_pred(eval_dataloader, device, val_features, squad['validation'], rd_0=True)['f1']\n",
    "\t# acc[0] = 77.96450701 # init=4000\n",
    "\tacc[0] = 13.8252 # init=100\n",
    "\n",
    "\tprint('Round 0\\ntesting accuracy {}'.format(acc[0]))\n",
    "\tprint('\\n')\n",
    "\t\n",
    "\t# ## round 1 to rd\n",
    "\t# for rd in range(1, NUM_ROUND+1):\n",
    "\t# \tprint('Round {}'.format(rd))\n",
    "\n",
    "\t\t# ## query\n",
    "\t\t# if STRATEGY_NAME == 'RandomSampling':\n",
    "\t\t# \tq_idxs = random_sampling_query(labeled_idxs, NUM_QUERY, rd)\n",
    "\t\t# elif STRATEGY_NAME == 'MarginSampling':\n",
    "\t\t# \tq_idxs = margin_sampling_query(n_pool, labeled_idxs, train_dataset, train_features, squad['train'], device, NUM_QUERY, rd)\n",
    "\t\t# elif STRATEGY_NAME == 'LeastConfidence':\n",
    "\t\t# \tq_idxs = least_confidence_query(n_pool, labeled_idxs, train_dataset, train_features, squad['train'], device, NUM_QUERY, rd)\n",
    "\t\t# elif STRATEGY_NAME == 'EntropySampling':\n",
    "\t\t# \tq_idxs = entropy_query(n_pool, labeled_idxs, train_dataset, train_features, squad['train'], device, NUM_QUERY, rd)\n",
    "\t\t# elif STRATEGY_NAME == 'MarginSamplingDropout':\n",
    "\t\t# \tq_idxs = margin_sampling_dropout_query(n_pool, labeled_idxs, train_dataset, train_features, squad['train'], device, NUM_QUERY, rd)\n",
    "\t\t# elif STRATEGY_NAME == 'LeastConfidenceDropout':\n",
    "\t\t# \tq_idxs = least_confidence_dropout_query(n_pool, labeled_idxs, train_dataset, train_features, squad['train'], device, NUM_QUERY, rd)\n",
    "\t\t# elif STRATEGY_NAME == 'EntropySamplingDropout':\n",
    "\t\t# \tq_idxs = entropy_dropout_query(n_pool, labeled_idxs, train_dataset, train_features, squad['train'], device, NUM_QUERY, rd)\n",
    "\t\t# elif STRATEGY_NAME == 'VarRatio':\n",
    "\t\t# \tq_idxs = var_ratio_query(n_pool, labeled_idxs, train_dataset, train_features, squad['train'], device, NUM_QUERY, rd)\n",
    "\t\t# elif STRATEGY_NAME == 'KMeansSampling':\n",
    "\t\t# \tq_idxs = KMeans_query(n_pool, labeled_idxs, train_dataset, train_features, squad['train'], device, NUM_QUERY, rd)\n",
    "\t\t# elif STRATEGY_NAME == 'KCenterGreedy':\n",
    "\t\t# \tq_idxs = kcenter_query()\n",
    "\t\t# elif STRATEGY_NAME == 'KCenterGreedyPCA': # not sure\n",
    "\t\t# \tq_idxs = \n",
    "\t\t# elif STRATEGY_NAME == 'BALDDropout':\n",
    "\t\t# \tq_idxs = bayesian_query(n_pool, labeled_idxs, train_dataset, train_features, squad['train'], device, NUM_QUERY, rd)\n",
    "\t\t# elif STRATEGY_NAME == 'MeanSTD':\n",
    "\t\t# \tq_idxs = mean_std_query(n_pool, labeled_idxs, train_dataset, train_features, squad['train'], device, NUM_QUERY, rd)\n",
    "\t\t# elif STRATEGY_NAME == 'BadgeSampling':\n",
    "\t\t# \tq_idxs = badge_query()\n",
    "\t\t# elif STRATEGY_NAME == 'LossPredictionLoss':\n",
    "\t\t# \t# different net!\n",
    "\t\t# \tq_idxs = loss_prediction_query()\n",
    "\t\t# elif STRATEGY_NAME == 'CEALSampling':\n",
    "\t\t# \t# why use 'CEALSampling' in STRATEGY_NAME\n",
    "\t\t# \tq_idxs = ceal_query()\n",
    "\t\t# else:\n",
    "\t\t# \traise NotImplementedError\n",
    "\n",
    "\t# \t## update\n",
    "\t# \tlabeled_idxs[q_idxs] = True\n",
    "\t# \trun_rd_labeled_idxs = np.arange(n_pool)[labeled_idxs]\n",
    "\n",
    "\t# \ttrain_dataloader_rd = DataLoader(\n",
    "\t# \t\ttrain_dataset.select(indices=run_rd_labeled_idxs),\n",
    "\t# \t\tshuffle=True,\n",
    "\t# \t\tcollate_fn=default_data_collator,\n",
    "\t# \t\tbatch_size=8,\n",
    "\t# \t)\n",
    "\n",
    "\t# \tnum_update_steps_per_epoch_rd = len(train_dataloader_rd)\n",
    "\t# \tnum_training_steps_rd = num_train_epochs * num_update_steps_per_epoch_rd\n",
    "\n",
    "\t# \tlr_scheduler_rd = get_scheduler(\n",
    "\t# \t\t\"linear\",\n",
    "\t# \t\toptimizer=optimizer,\n",
    "\t# \t\tnum_warmup_steps=0,\n",
    "\t# \t\tnum_training_steps=num_training_steps_rd,\n",
    "\t# \t)\n",
    "\n",
    "\t# \tmodel_rd = AutoModelForQuestionAnswering.from_pretrained(model_dir).to(device)\n",
    "\t# \toptimizer_rd = AdamW(model_rd.parameters(), lr=1e-4)\n",
    "\n",
    "\t# \t## train\n",
    "\t# \tto_train(num_train_epochs, train_dataloader_rd, device, model_rd, optimizer_rd, lr_scheduler_rd)\n",
    "\n",
    "\t# \t## round rd accuracy\n",
    "\t# \tacc[rd] = get_pred(eval_dataloader, device, val_features, squad['validation'])['f1']\n",
    "\t# \tprint('testing accuracy {}'.format(acc[rd]))\n",
    "\t# \tprint('\\n')\n",
    "\n",
    "\t# \ttorch.cuda.empty_cache()\n",
    "\t\n",
    "\t# ## print results\n",
    "\t# print('SEED {}'.format(SEED))\n",
    "\t# print(STRATEGY_NAME)\n",
    "\t# print(acc)\n",
    "\t# all_acc.append(acc)\n",
    "\t\n",
    "\t# ## save model and record acq time\n",
    "\t# timestamp = re.sub('\\.[0-9]*','_',str(datetime.datetime.now())).replace(\" \", \"_\").replace(\"-\", \"\").replace(\":\",\"\")\n",
    "\t# final_model_dir = model_dir + '/' + timestamp + DATA_NAME+ '_'  + STRATEGY_NAME + '_' + str(NUM_QUERY) + '_' + str(NUM_INIT_LB) +  '_' + str(args_input.quota)\n",
    "\t# os.makedirs(final_model_dir, exist_ok=True)\n",
    "\t# end = datetime.datetime.now()\n",
    "\t# acq_time.append(round(float((end-start).seconds), 3))\n",
    "\n",
    "\t# final_model = AutoModelForQuestionAnswering.from_pretrained(model_dir).to(device)\n",
    "\t# model_to_save = final_model.module if hasattr(final_model, 'module') else final_model \n",
    "\t# model_to_save.save_pretrained(final_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get unlable data\n",
    "# unlabeled_idxs = np.arange(n_pool)[~labeled_idxs]\n",
    "# unlabeled_data = train_dataset.select(indices=unlabeled_idxs)\n",
    "# len(unlabeled_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unlable data\n",
    "unlabeled_idxs = np.arange(n_pool)[~labeled_idxs]\n",
    "# smaller data\n",
    "unlabeled_idxs_20 = unlabeled_idxs[20:40]\n",
    "unlabeled_data_20 = train_dataset.select(unlabeled_idxs_20)\n",
    "len(unlabeled_data_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_features_20 = train_features.select(unlabeled_idxs_20)\n",
    "unlabeled_dataloader_20 = DataLoader(\n",
    "\t\tunlabeled_data_20,\n",
    "\t\tshuffle=False,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=8,\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prob(unlabeled_dataloader_20, device, unlabeled_features_20, squad['train'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test: query 5 data from 20 unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# def kmeans_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n, rd):\n",
    "#     # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "#     unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "#     unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "#     unlabeled_dataloader = DataLoader(\n",
    "#       unlabeled_data,\n",
    "#       shuffle=True,\n",
    "#       collate_fn=default_data_collator,\n",
    "#       batch_size=8,\n",
    "#     )\n",
    "n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# smaller data\n",
    "unlabeled_idxs_20 = unlabeled_idxs[20:120]\n",
    "unlabeled_data_20 = train_dataset.select(unlabeled_idxs_20)\n",
    "unlabeled_feature_20 = train_features.select(unlabeled_idxs_20)\n",
    "len(unlabeled_data_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "       39, 40, 41])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_idxs_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data_20,\n",
    "\t\tshuffle=True,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=8,\n",
    "\t)\n",
    "len(unlabeled_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating_prob: 100%|██████████| 13/13 [00:02<00:00,  5.57it/s]\n"
     ]
    }
   ],
   "source": [
    "    # # deepAL+: embeddings = get_embeddings(unlabeled_data)\n",
    "    # embeddings = get_embeddings(unlabeled_dataloader, device, unlabeled_features, examples, rd=1)\n",
    "embeddings = get_embeddings(unlabeled_dataloader, device, unlabeled_feature_20, squad['train'], rd=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5449,  1.3946,  0.7511,  ..., -1.4325,  0.7666, -1.1297],\n",
      "        [ 0.5449,  1.3946,  0.7511,  ..., -1.4325,  0.7666, -1.1297],\n",
      "        [-0.0527,  1.7811, -0.2930,  ..., -1.8407,  0.1054,  0.4297],\n",
      "        ...,\n",
      "        [ 1.0195,  0.2562,  0.5435,  ..., -1.5053,  0.6708, -0.0900],\n",
      "        [-0.5486,  0.9826,  0.5415,  ..., -0.6682,  0.4827, -1.0402],\n",
      "        [ 0.5449,  1.3946,  0.7511,  ..., -1.4325,  0.7666, -1.1297]])\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/arbeitsdaten31/studenten1/linku/.venv/lib64/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=5)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # deepAL+: embeddings = embeddings.numpy()\n",
    "# embeddings = embeddings.numpy()\n",
    "print(embeddings.shape)\n",
    "    # deepAL+: cluster_learner = KMeans(n_clusters=n)\n",
    "cluster_learner = KMeans(n_clusters=n)\n",
    "    # deepAL+: cluster_learner.fit(embeddings)\n",
    "cluster_learner.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54492968,  1.39455354,  0.75112677, ..., -1.43251991,\n",
       "         0.76659399, -1.12965727],\n",
       "       [ 0.54492968,  1.39455354,  0.75112677, ..., -1.43251991,\n",
       "         0.76659399, -1.12965727],\n",
       "       [-0.05274361,  1.78112769, -0.29300603, ..., -1.84073281,\n",
       "         0.1053937 ,  0.42967919],\n",
       "       ...,\n",
       "       [ 0.31954438,  0.77420248,  0.4836542 , ..., -0.88744513,\n",
       "         0.31552405,  0.0243858 ],\n",
       "       [ 0.31954438,  0.77420248,  0.4836542 , ..., -0.88744513,\n",
       "         0.31552405,  0.0243858 ],\n",
       "       [ 0.54492968,  1.39455354,  0.75112677, ..., -1.43251991,\n",
       "         0.76659399, -1.12965727]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # deepAL+: cluster_idxs = cluster_learner.predict(embeddings)\n",
    "cluster_idxs = cluster_learner.predict(embeddings)\n",
    "    # deepAL+: centers = cluster_learner.cluster_centers_[cluster_idxs]\n",
    "centers = cluster_learner.cluster_centers_[cluster_idxs]\n",
    "centers # len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 2, 4, 4, 4, 4, 2, 3, 2, 2, 3, 2, 4, 3, 0, 2, 2, 0, 4, 2,\n",
       "       4, 4, 2, 4, 0, 3, 1, 4, 2, 2, 2, 4, 3, 3, 1, 3, 0, 2, 4, 0, 1, 1,\n",
       "       0, 4, 4, 2, 4, 2, 3, 4, 0, 2, 4, 2, 1, 2, 1, 1, 0, 1, 2, 4, 4, 2,\n",
       "       3, 2, 0, 2, 4, 4, 2, 3, 1, 4, 4, 2, 4, 1, 2, 3, 4, 2, 4, 1, 4, 2,\n",
       "       1, 0, 4, 0, 1, 1, 2, 0, 4, 4, 4, 2], dtype=int32)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2326e-32, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.9304e-32,\n",
       "         4.4373e-31],\n",
       "        [1.2326e-32, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.9304e-32,\n",
       "         4.4373e-31],\n",
       "        [1.2326e-32, 4.9304e-32, 3.0815e-33,  ..., 0.0000e+00, 6.9333e-33,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [4.8989e-01, 2.6835e-01, 3.5842e-03,  ..., 3.8171e-01, 1.2622e-01,\n",
       "         1.3087e-02],\n",
       "        [7.5372e-01, 4.3423e-02, 3.3410e-03,  ..., 4.8066e-02, 2.7931e-02,\n",
       "         1.1333e+00],\n",
       "        [1.2326e-32, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.9304e-32,\n",
       "         4.4373e-31]], dtype=torch.float64)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # deepAL+: dis = (embeddings - centers)**2\n",
    "dis = (embeddings - centers)**2\n",
    "dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17 30  0  2 24]\n"
     ]
    }
   ],
   "source": [
    "    # deepAL+: dis = dis.sum(axis=1)\n",
    "dis = dis.sum(axis=1)\n",
    "    # deepAL+: q_idxs = np.array([np.arange(embeddings.shape[0])[cluster_idxs==i][dis[cluster_idxs==i].argmin()] for i in range(n)])\n",
    "q_idxs = np.array([np.arange(embeddings.shape[0])[cluster_idxs==i][dis[cluster_idxs==i].argmin()] for i in range(n)])\n",
    "\n",
    "    # deepAL+: return unlabeled_idxs[q_idxs]\n",
    "print(unlabeled_idxs[q_idxs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alqa",
   "language": "python",
   "name": "alqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6317ac8a4c65d05b4cf6bac76f72bfaae40b2e9380067c26c20c9afff1d8528e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
