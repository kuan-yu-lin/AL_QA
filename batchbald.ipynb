{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Model \n",
    "## no trainer\n",
    "\n",
    "- dataset\n",
    "- torch\n",
    "- transformers\n",
    "- transformers[torch]\n",
    "- evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, disable_caching\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    AutoModelForQuestionAnswering\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch.cuda import amp\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# for batch_bald_query\n",
    "import evaluate\n",
    "from scipy import stats\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pdb\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arguments.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRATEGY_NAME = 'BatchBALDDropout'\n",
    "DATA_NAME = 'BioASQ'\n",
    "EXPE_ROUND = 3\n",
    "MODEL_BATCH = 8\n",
    "MAX_LENGTH = None\n",
    "LEARNING_RATE = 3e-5\n",
    "MODEL_NAME = 'RoBERTa'\n",
    "LOW_RES = True\n",
    "NUM_TRAIN_EPOCH = 3\n",
    "UNIQ_CONTEXT = False\n",
    "if LOW_RES:\n",
    "    args_input_quota = 200\n",
    "    NUM_QUERY = 50\n",
    "else:\n",
    "    NUM_INIT_LB = 500 # 1000\n",
    "    args_input_quota = 2000 # 200\n",
    "    NUM_QUERY = 500 # 50\n",
    "ITERATION = int(args_input_quota / NUM_QUERY)\n",
    "\n",
    "stride = 128\n",
    "\n",
    "model_dir = '/mount/arbeitsdaten31/studenten1/linku/dev_models'\n",
    "CACHE_DIR = '/mount/arbeitsdaten31/studenten1/linku/linku/.cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples, tokenizer):\n",
    "    # no ['offset_mapping'], for .train() and .eval()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def preprocess_training_features(examples, tokenizer):\n",
    "    # keep [\"offset_mapping\"] and [\"example_id\"], for compute_metrics()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "    contexts = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # added for used in unlabel data predict\n",
    "        example_ids.append(examples[\"id\"][sample_idx]) \n",
    "        # added for unique context filter\n",
    "        contexts.append(examples[\"context\"][sample_idx])\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"context\"] = contexts\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "def preprocess_validation_examples(examples, tokenizer):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs\n",
    "\n",
    "def preprocess_training_examples_lowRes(examples, tokenizer):\n",
    "    # no ['offset_mapping'], for .train() and .eval()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    detected_answers = examples[\"detected_answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = detected_answers[sample_idx]\n",
    "        start_char = answer[\"char_spans\"][0][\"start\"][0]\n",
    "        end_char = answer[\"char_spans\"][0][\"end\"][0]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "def preprocess_training_features_lowRes(examples, tokenizer):\n",
    "    # keep [\"offset_mapping\"] and [\"example_id\"], for compute_metrics()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"detected_answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "    contexts = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"char_spans\"][0][\"start\"][0]\n",
    "        end_char = answer[\"char_spans\"][0][\"end\"][0]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # added for used in unlabel data predict\n",
    "        example_ids.append(examples[\"qid\"][sample_idx])\n",
    "        # added for unique context filter\n",
    "        contexts.append(examples['context'][sample_idx])\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"context\"] = contexts\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "def preprocess_validation_examples_lowRes(examples, tokenizer):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"qid\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aubc(quota, bsize, resseq):\n",
    "\t# it is equal to use np.trapz for calculation\n",
    "\tressum = 0.0\n",
    "\tif quota % bsize == 0:\n",
    "\t\tfor i in range(len(resseq)-1):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\n",
    "\telse:\n",
    "\t\tfor i in range(len(resseq)-2):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\t\tk = quota % bsize\n",
    "\t\tressum = ressum + ((resseq[-1] + resseq[-2]) * k / 2)\n",
    "\tressum = round(ressum / quota,3)\n",
    "\t\n",
    "\treturn ressum\n",
    "\n",
    "\n",
    "def get_mean_stddev(datax):\n",
    "\treturn round(np.mean(datax),4),round(np.std(datax),4)\n",
    "\n",
    "\n",
    "def get_unlabel_data(n_pool, labeled_idxs, train_dataset):\n",
    "    unlabeled_idxs = np.arange(n_pool)[~labeled_idxs]\n",
    "    unlabeled_data = train_dataset.select(indices=unlabeled_idxs)\n",
    "    return unlabeled_idxs, unlabeled_data\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "\n",
    "def get_model(m):\n",
    "\tif m.lower() == 'bert':\n",
    "\t\treturn 'bert-base-uncased'\n",
    "\telif m.lower() == 'bertlarge':\n",
    "\t\treturn 'bert-large-uncased'\n",
    "\telif m.lower() == 'roberta':\n",
    "\t\treturn 'roberta-base'\n",
    "\telif m.lower() == 'robertalarge':\n",
    "\t\treturn 'roberta-large'\n",
    "\n",
    "\n",
    "def get_context_id(data):\n",
    "    context_id = {}\n",
    "    for i, c in enumerate(set(data['context'])):\n",
    "        context_id[c] = i+1\n",
    "    return context_id\n",
    "\n",
    "\n",
    "def preprocess_data(train_data, val_data):\n",
    "\ttokenizer = AutoTokenizer.from_pretrained(get_model(MODEL_NAME))\n",
    "\n",
    "\tif LOW_RES:\n",
    "\t\ttrain_dataset = train_data.map(\n",
    "\t\t\tpreprocess_training_examples_lowRes,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=train_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\ttrain_features = train_data.map(\n",
    "\t\t\tpreprocess_training_features_lowRes,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=train_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\tval_dataset = val_data.map(\n",
    "\t\t\tpreprocess_validation_examples_lowRes,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=val_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\tval_features = val_data.map(\n",
    "\t\t\tpreprocess_validation_examples_lowRes,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=val_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\ttrain_dataset = train_data.map(\n",
    "\t\t\tpreprocess_training_examples,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=train_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\ttrain_features = train_data.map(\n",
    "\t\t\tpreprocess_training_features,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=train_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\tval_dataset = val_data.map(\n",
    "\t\t\tpreprocess_validation_examples,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=val_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\tval_features = val_data.map(\n",
    "\t\t\tpreprocess_validation_examples,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=val_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\n",
    "\ttrain_dataset.set_format(\"torch\")\n",
    "\ttrain_features.set_format(\"torch\")\n",
    "\tval_dataset = val_dataset.remove_columns([\"offset_mapping\"])\n",
    "\tval_dataset.set_format(\"torch\")\n",
    "\tval_features.set_format(\"torch\")\n",
    "\n",
    "\treturn train_dataset, train_features, val_dataset, val_features\n",
    "\n",
    "\n",
    "def load_dataset_mrqa(d):\n",
    "\t'''\n",
    "\treturn train_set, val_set\n",
    "\t'''\n",
    "\tdata = load_dataset(\"mrqa\", cache_dir=CACHE_DIR)\n",
    "\tif d == 'squad':\n",
    "\t\t# the first to 86588th in train set\n",
    "\t\t# the first to 10507th in val set\n",
    "\t\tsquad_train = data['train'].select(range(86588))\n",
    "\t\tsquad_val = data['validation'].select(range(10507))\n",
    "\t\tfor t in squad_train: assert t['subset'] == 'SQuAD', 'Please select corrrect train data for SQuAD.'\n",
    "\t\tfor v in squad_val: assert v['subset'] == 'SQuAD', 'Please select corrrect validation data for SQuAD.'\n",
    "\t\treturn squad_train, squad_val\n",
    "\telif d == 'newsqa':\n",
    "\t\t# the 86589th to 160748th in train set\n",
    "\t\t# the 10508th to 14719th in val set\n",
    "\t\tdata_set = data['train'].select(range(86588, 160748))\n",
    "\t\tnewsqa_train = data_set.shuffle(1127).select(range(10000))\n",
    "\t\tnewsqa_val = data['validation'].select(range(10507, 14719))\n",
    "\t\tfor t in newsqa_train: assert t['subset'] == 'NewsQA', 'Please select corrrect train data for NewQA.'\n",
    "\t\tfor v in newsqa_val: assert v['subset'] == 'NewsQA', 'Please select corrrect validation data for NewQA.'\n",
    "\t\treturn newsqa_train, newsqa_val\n",
    "\telif d == 'searchqa':\n",
    "\t\t# the 222437th to 339820th in train set\n",
    "\t\t# the 22505th to 39484th in val set\n",
    "\t\tdata_set = data['train'].select(range(222436, 339820))\n",
    "\t\tsearchqa_train = data_set.shuffle(1127).select(range(10000))\n",
    "\t\tsearchqa_val = data['validation'].select(range(22504, 39484))\t\n",
    "\t\tfor t in searchqa_train: assert t['subset'] == 'SearchQA', 'Please select corrrect train data for SearchQA.'\n",
    "\t\tfor v in searchqa_val: assert v['subset'] == 'SearchQA', 'Please select corrrect validation data for SearchQA.'\n",
    "\t\treturn searchqa_train, searchqa_val\n",
    "\telif d == 'bioasq':\n",
    "\t\t# the first to the 1504th in the test set\n",
    "\t\tsub = data['test'].select(range(1504))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\tbioasq_train = sub.select(range(len_sub_val, len(sub)))\n",
    "\t\tbioasq_val = sub.select(range(len_sub_val))\n",
    "\t\tfor t in bioasq_train: assert t['subset'] == 'BioASQ', 'Please select corrrect train data for BioASQ.'\n",
    "\t\tfor v in bioasq_val: assert v['subset'] == 'BioASQ', 'Please select corrrect validation data for BioASQ.'\n",
    "\t\treturn bioasq_train, bioasq_val\n",
    "\telif d == 'textbookqa':\n",
    "\t\t# the 8131st to 9633rd\n",
    "\t\tsub = data['test'].select(range(8130, 9633))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\ttextbookqa_train = sub.select(range(len_sub_val, len(sub)))\n",
    "\t\ttextbookqa_val = sub.select(range(len_sub_val)) \n",
    "\t\tfor t in textbookqa_train: assert t['subset'] == 'TextbookQA', 'Please select corrrect train data for TextbookQA.'\n",
    "\t\tfor v in textbookqa_val: assert v['subset'] == 'TextbookQA', 'Please select corrrect validation data for TextbookQA.'\n",
    "\t\treturn textbookqa_train, textbookqa_val\n",
    "\telif d == 'drop': # Discrete Reasoning Over Paragraphs\n",
    "\t\t# the 1505th to 3007th in test set\n",
    "\t\tsub = data['test'].select(range(1504, 3007))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\tdrop_train = sub.select(range(len_sub_val, len(sub)))\n",
    "\t\tdrop_val = sub.select(range(len_sub_val))\n",
    "\t\tfor t in drop_train: assert t['subset'] == 'DROP', 'Please select corrrect train data for DROP.'\n",
    "\t\tfor v in drop_val: assert v['subset'] == 'DROP', 'Please select corrrect validation data for DROP.'\n",
    "\t\treturn drop_train, drop_val\n",
    "\t\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluation(theoretical_answers, predicted_answers, skip_no_answer=False):\n",
    "    '''\n",
    "\ttheoretical_answers, datatype=dict\n",
    "\t{strings of id: list of ground truth answers}\n",
    "\tpredicted_answers, datatype=dict\n",
    "\t{strings of id: strings of prediction text}\n",
    "\t'''\n",
    "    f1 = exact_match = total = 0\n",
    "    for qid, ground_truths in theoretical_answers.items():\n",
    "        if qid not in predicted_answers:\n",
    "            if not skip_no_answer:\n",
    "                message = 'Unanswered question %s will receive score 0.' % qid\n",
    "                print(message)\n",
    "                total += 1\n",
    "            continue\n",
    "        total += 1\n",
    "        prediction = predicted_answers[qid]\n",
    "        exact_match += metric_max_over_ground_truths(\n",
    "            exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(\n",
    "            f1_score, prediction, ground_truths)\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}\n",
    "\n",
    "\n",
    "def save_model(device, pretrain_dir, strategy_dir):\n",
    "    '''\n",
    "    Copy and save model from pretrain_models to current trained models.\n",
    "    '''\n",
    "    pretrain_model = AutoModelForQuestionAnswering.from_pretrained(pretrain_dir).to(device)\n",
    "    model_to_save = pretrain_model.module if hasattr(pretrain_model, 'module') else pretrain_model \n",
    "    model_to_save.save_pretrained(strategy_dir)\n",
    "\n",
    "\n",
    "def init_centers(X, K):\n",
    "    ind = np.argmax([np.linalg.norm(s, 2) for s in X])\n",
    "    mu = [X[ind]]\n",
    "    indsAll = [ind]\n",
    "    centInds = [0.] * len(X)\n",
    "    cent = 0\n",
    "    print('#Samps\\tTotal Distance')\n",
    "    while len(mu) < K:\n",
    "        if len(mu) == 1:\n",
    "            D2 = pairwise_distances(X, mu).ravel().astype(float)\n",
    "        else:\n",
    "            newD = pairwise_distances(X, [mu[-1]]).ravel().astype(float)\n",
    "            for i in range(len(X)):\n",
    "                if D2[i] >  newD[i]:\n",
    "                    centInds[i] = cent\n",
    "                    D2[i] = newD[i]\n",
    "        print(str(len(mu)) + '\\t' + str(sum(D2)), flush=True)\n",
    "        if sum(D2) == 0.0: pdb.set_trace()\n",
    "        D2 = D2.ravel().astype(float)\n",
    "        Ddist = (D2 ** 2)/ sum(D2 ** 2)\n",
    "        customDist = stats.rv_discrete(name='custm', values=(np.arange(len(D2)), Ddist))\n",
    "        ind = customDist.rvs(size=1)[0]\n",
    "        while ind in indsAll: ind = customDist.rvs(size=1)[0]\n",
    "        mu.append(X[ind])\n",
    "        indsAll.append(ind)\n",
    "        cent += 1\n",
    "    return indsAll\n",
    "\n",
    "def get_unique_sample(labeled_idxs, q_idxs, n_pool, train_features, iteration=0):\n",
    "\tif LOW_RES:\n",
    "\t\tnum_query_i = NUM_QUERY * iteration\n",
    "\t\tprint('num_query_i in get_unique_sample in LOW_RES:', num_query_i)\n",
    "\telse:\n",
    "\t\tnum_query_i = NUM_QUERY * iteration + NUM_INIT_LB\n",
    "\t\tprint('num_query_i in get_unique_sample:', num_query_i)\n",
    "\n",
    "\tdifference_i = 0\n",
    "\tnum_set_ex_id_i = 0\n",
    "\n",
    "\tn = 0\n",
    "\n",
    "\twhile num_set_ex_id_i < num_query_i:\n",
    "\t\tlabeled_idxs[q_idxs[:NUM_QUERY + difference_i]] = True\t# get first num_query, e.g. 50\n",
    "\t\titer_i_labeled_idxs = np.arange(n_pool)[labeled_idxs]\n",
    "\t\tprint('len(iter_i_labeled_idxs):', len(iter_i_labeled_idxs))\n",
    "\n",
    "\t\titer_i_samples = train_features.select(indices=iter_i_labeled_idxs)\n",
    "\t\tnum_set_ex_id_i = len(set(iter_i_samples['example_id']))\n",
    "\t\tprint('number of unique example id:', num_set_ex_id_i)\n",
    "\n",
    "\t\tassert num_set_ex_id_i <= num_query_i, 'Select too many examples!'\n",
    "\t\tassert num_set_ex_id_i > 0, \"Did not select examples!\"\n",
    "\n",
    "\t\tdifference_i = num_query_i - num_set_ex_id_i\n",
    "\t\tprint('difference_i', difference_i)\n",
    "\n",
    "\t\tn += 1\n",
    "\t\tif n == 3: break\n",
    "\t\n",
    "\treturn iter_i_labeled_idxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mrqa (/mount/arbeitsdaten31/studenten1/linku/linku/.cache/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3ae5cd370d4de49237b6a216cc5abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "disable_caching()\n",
    "if LOW_RES:\n",
    "\t## set dir\n",
    "\tpretrain_model_dir = '/mount/arbeitsdaten31/studenten1/linku/pretrain_models' + '/' + MODEL_NAME + '_SQuAD_full_dataset_lr_3e-5'\n",
    "\tstrategy_model_dir = model_dir + '/lowRes_' + str(args_input_quota) + '_' + STRATEGY_NAME + '_' + MODEL_NAME +  '_' + DATA_NAME\n",
    "\t## load data\n",
    "\ttrain_data, val_data = load_dataset_mrqa(DATA_NAME.lower())\n",
    "else:\n",
    "\t## set dir\n",
    "\tstrategy_model_dir = model_dir + '/' + str(NUM_INIT_LB) + '_' + str(args_input_quota) + '_' + STRATEGY_NAME + '_' + MODEL_NAME +  '_' + DATA_NAME\n",
    "\t## load data\n",
    "\tsquad = load_dataset(DATA_NAME.lower(), cache_dir=CACHE_DIR)\n",
    "\ttrain_data = squad[\"train\"]\n",
    "\tval_data = squad[\"validation\"]\n",
    "\tprint('Use full training data and full testing data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train(num_train_epochs, train_dataloader, device, model, optimizer, lr_scheduler, record_loss=False):\n",
    "\tif LOW_RES:\n",
    "\t\tprint('Training was performed using {} query data, i.e. {} data.'.format(NUM_QUERY, len(train_dataloader.dataset)))\n",
    "\telse:\n",
    "\t\tprint('Training was performed using the sum of {} initial data and {} query data, i.e. {} data.'.format(NUM_INIT_LB, NUM_QUERY, len(train_dataloader.dataset)))\n",
    "\t\n",
    "\tfor epoch in range(num_train_epochs):\n",
    "\t\tmodel.train()\n",
    "\t\tfor step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "\t\t\tbatch = {key: value.to(device) for key, value in batch.items()}\n",
    "\t\t\toutputs = model(**batch)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tlr_scheduler.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tif record_loss:\n",
    "\t\t\tprint('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "\tmodel_to_save = model.module if hasattr(model, 'module') else model \n",
    "\tmodel_to_save.save_pretrained(strategy_model_dir)\n",
    "\tprint('TRAIN done!')\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "\n",
    "def compute_metrics_lowRes(start_logits, end_logits, features, examples):\n",
    "    example_to_features = defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = dict()\n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        example_id = example[\"qid\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers[example_id] = best_answer[\"text\"]\n",
    "        else:\n",
    "            predicted_answers[example_id] = \"\"\n",
    "\n",
    "    theoretical_answers = dict()\n",
    "    for ex in examples: theoretical_answers[ex[\"qid\"]] = ex[\"answers\"]\n",
    "    return evaluation(theoretical_answers, predicted_answers)\n",
    "\n",
    "def get_pred(dataloader, device, features, examples):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating_pred\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    if LOW_RES:\n",
    "        return compute_metrics_lowRes(start_logits, end_logits, features, examples)\n",
    "    else:\n",
    "        return compute_metrics(start_logits, end_logits, features, examples)\n",
    "\n",
    "def get_prob_dropout_split(dataloader, device, features, examples, n_drop=10):\n",
    "    ## use tensor to save the answers\n",
    "\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    probs = torch.zeros([n_drop, len(dataloader.dataset), 200])\n",
    "    \n",
    "    for i in range(n_drop):\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating_prob_dropout\"):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "            end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "        start_logits = np.concatenate(start_logits)\n",
    "        end_logits = np.concatenate(end_logits)\n",
    "        start_logits = start_logits[: len(features)]\n",
    "        end_logits = end_logits[: len(features)]\n",
    "\n",
    "        example_to_features = defaultdict(list)\n",
    "        max_answer_length = 30\n",
    "        n_best = 20\n",
    "            \n",
    "        for idx, feature in enumerate(features):\n",
    "            example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "        for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "            if LOW_RES:\n",
    "                example_id = example[\"qid\"]\n",
    "            else:\n",
    "                example_id = example[\"id\"]\n",
    "            answers = []\n",
    "\n",
    "            # Loop through all features associated with that example\n",
    "            for feature_index in example_to_features[example_id]:\n",
    "                start_logit = start_logits[feature_index]\n",
    "                end_logit = end_logits[feature_index]\n",
    "                offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # Skip answers that are not fully in the context\n",
    "                        if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                            continue\n",
    "                        # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                        if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                        ):\n",
    "                            continue\n",
    "\n",
    "                        answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "\n",
    "            \n",
    "                if 1 < len(answers) < 200: # pad to same numbers of possible answers\n",
    "                    zero_list = [0] * (200 - len(answers))\n",
    "                    answers.extend(zero_list)\n",
    "                elif len(answers) >= 200:\n",
    "                    answers = answers[:200]\n",
    "\n",
    "                probs[i][feature_index] += torch.tensor(softmax(answers))\n",
    "\n",
    "    return probs\n",
    "\n",
    "def get_batch_prob_dropout_split(dataloader, device, features, examples, n_drop=10):\n",
    "    ## use tensor to save the answers\n",
    "    c = 10\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    probs = torch.zeros([n_drop, len(dataloader.dataset), c])\n",
    "    \n",
    "    for i in range(n_drop):\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating_prob_dropout\"):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "            end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "        start_logits = np.concatenate(start_logits)\n",
    "        end_logits = np.concatenate(end_logits)\n",
    "        start_logits = start_logits[: len(features)]\n",
    "        end_logits = end_logits[: len(features)]\n",
    "\n",
    "        example_to_features = defaultdict(list)\n",
    "        max_answer_length = 30\n",
    "        n_best = 20\n",
    "            \n",
    "        for idx, feature in enumerate(features):\n",
    "            example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "        n = 0\n",
    "        for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "            if LOW_RES:\n",
    "                example_id = example[\"qid\"]\n",
    "            else:\n",
    "                example_id = example[\"id\"]\n",
    "            answers = []\n",
    "\n",
    "            # Loop through all features associated with that example\n",
    "            for feature_index in example_to_features[example_id]:\n",
    "                start_logit = start_logits[feature_index]\n",
    "                end_logit = end_logits[feature_index]\n",
    "                offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # Skip answers that are not fully in the context\n",
    "                        if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                            continue\n",
    "                        # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                        if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                        ):\n",
    "                            continue\n",
    "\n",
    "                        answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "\n",
    "            \n",
    "                if 1 < len(answers) < c: # pad to same numbers of possible answers\n",
    "                    zero_list = [0] * (c - len(answers))\n",
    "                    answers.extend(zero_list)\n",
    "                elif len(answers) >= c:\n",
    "                    answers = answers[:c]\n",
    "\n",
    "                probs[i][feature_index] += torch.tensor(softmax(answers))\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def class_combinations(c, n, m=np.inf):\n",
    "    \"\"\" Generates an array of n-element combinations where each element is one of\n",
    "    the c classes (an integer). If m is provided and m < n^c, then instead of all\n",
    "    n^c combinations, m combinations aren randomly sampled.\n",
    "\n",
    "    Arguments:\n",
    "        c {int} -- the number of classes\n",
    "        n {int} -- the number of elements in each combination\n",
    "\n",
    "    Keyword Arguments:\n",
    "        m {int} -- the number of desired combinations (default: {np.inf})\n",
    "\n",
    "    Returns:\n",
    "        np.ndarry -- An [m x n] or [n^c x n] array of integers in [0, c)\n",
    "    \"\"\"\n",
    "\n",
    "    if m < c**n:\n",
    "        # randomly sample combinations\n",
    "        return np.random.randint(c, size=(int(m), n))\n",
    "    else:\n",
    "        p_c = combinations_with_replacement(np.arange(c), n)\n",
    "        return np.array(list(iter(p_c)), dtype=int)\n",
    "\n",
    "def H(x):\n",
    "    \"\"\" Compute the element-wise entropy of x\n",
    "\n",
    "    Arguments:\n",
    "        x {torch.Tensor} -- array of probabilities in (0,1)\n",
    "\n",
    "    Keyword Arguments:\n",
    "        eps {float} -- prevent failure on x == 0\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor -- H(x)\n",
    "    \"\"\"\n",
    "    return -(x)*torch.log(x)\n",
    "\n",
    "def bald_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n):\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "        unlabeled_data,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=MODEL_BATCH,\n",
    "    )\n",
    "    print('BALD querying starts!')\n",
    "    print('Query {} data from {} unlabeled training data.'.format(n, len(unlabeled_data)))\n",
    "    # to fasten development, turn dropout from 10 to 2\n",
    "    probs = get_prob_dropout_split(unlabeled_dataloader, device, unlabeled_features, examples, n_drop=3)\n",
    "    print('Got probability!')\n",
    "    probs_mean = probs.mean(0)\n",
    "    entropy1 = (-probs_mean*torch.log(probs_mean)).sum(1)\n",
    "    entropy2 = (-probs*torch.log(probs)).sum(2).mean(0)\n",
    "    uncertainties = entropy2 - entropy1\n",
    "    # later on, we can use batch\n",
    "    # uncertainties.sort()[1][:n] # select the best n # tensor([ 948, 1188, 1187, 1308, 1415,  534,  942,  918,  917, 1410,  913, 1372, ...\n",
    "    return unlabeled_idxs[uncertainties.sort()[1][:n]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hide some steps for better view in development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will preprocess the dataset (training and evaluation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8058b700e1a7409baedff37fc484e6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1354 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4a82aa081a496cb084b623c0a3e810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1354 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0908f1f05ea84af2a26d3a1e35456f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba345a0b85db4f5b9f4b618f8c65990f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset, train_features, val_dataset, val_features = preprocess_data(train_data, val_data)\n",
    "# get the number of extra data after preprocessing\n",
    "extra = len(train_dataset) - len(train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1127\n",
    "# os.environ['TORCH_HOME']='./basicmodel'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(2)\n",
    "\n",
    "# fix random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "all_acc = []\n",
    "acq_time = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_pool: 1442\n",
      "in low res setting\n",
      "BioASQ\n",
      "BatchBALDDropout\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058acf1e93284ef0a775070c5758f9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2189019aed8d461eb2294f444df00130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "testing accuracy 57.91499048455571\n",
      "testing accuracy em 35.333333333333336\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## record acc performance \n",
    "acc = np.zeros(ITERATION + 1) # quota/batch runs + iter_0\n",
    "acc_em = np.zeros(ITERATION + 1)\n",
    "acc = np.zeros(4) # quota/batch runs + iter_0\n",
    "acc_em = np.zeros(4)\n",
    "\n",
    "## generate initial labeled pool\n",
    "n_pool = len(train_dataset)\n",
    "print('n_pool:', n_pool)\n",
    "labeled_idxs = np.zeros(n_pool, dtype=bool)\n",
    "\n",
    "if LOW_RES:\n",
    "    print('in low res setting')\n",
    "    save_model(device, pretrain_model_dir, strategy_model_dir)\n",
    "    ## print info\n",
    "    print(DATA_NAME)\n",
    "    print(STRATEGY_NAME)\n",
    "else:\n",
    "    print('not in low res setting')\n",
    "    tmp_idxs = np.arange(n_pool)\n",
    "    print('len(tmp_idxs):', len(tmp_idxs))\n",
    "    np.random.shuffle(tmp_idxs)\n",
    "    print('len(tmp_idxs):', len(tmp_idxs))\n",
    "    \n",
    "    if UNIQ_CONTEXT:\n",
    "        print('in uc setting')\n",
    "        tmp_idxs = tmp_idxs[:NUM_INIT_LB+extra]\n",
    "        uc_tmp_idxs = get_unique_context(tmp_idxs, train_features, context_dict) # len() = almost num_query + extra\n",
    "        print('len(uc_tmp_idxs):', len(uc_tmp_idxs))\n",
    "        iter_0_labeled_idxs = get_unique_sample(labeled_idxs, uc_tmp_idxs, n_pool, train_features)\n",
    "        c_id = get_final_c_id(iter_0_labeled_idxs, train_features) # len() = num_query\n",
    "    else:\n",
    "        print('not in uc setting')\n",
    "        iter_0_labeled_idxs = get_unique_sample(labeled_idxs, tmp_idxs, n_pool, train_features)\n",
    "\n",
    "    ## load the selected train data to DataLoader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset.select(indices=iter_0_labeled_idxs),\n",
    "        shuffle=True,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=MODEL_BATCH,\n",
    "    )\n",
    "\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = NUM_TRAIN_EPOCH * num_update_steps_per_epoch\n",
    "\n",
    "    ## network\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(get_model(MODEL_NAME)).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "    ## print info\n",
    "    print(DATA_NAME)\n",
    "    print(STRATEGY_NAME)\n",
    "\n",
    "    ## iteration 0 accuracy\n",
    "    to_train(NUM_TRAIN_EPOCH, train_dataloader, device, model, optimizer, lr_scheduler)\n",
    "\n",
    "## load the selected validation data to DataLoader\n",
    "eval_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=MODEL_BATCH\n",
    ")\n",
    "\n",
    "acc_scores_0 = get_pred(eval_dataloader, device, val_features, val_data) # add i=1 to use model from models_dir\n",
    "acc[0] = acc_scores_0['f1']\n",
    "acc_em[0] = acc_scores_0['exact_match']\n",
    "\n",
    "print('Round 0\\ntesting accuracy {}'.format(acc[0]))\n",
    "print('testing accuracy em {}'.format(acc_em[0]))\n",
    "# time = datetime.datetime.now()\n",
    "# print('Time spent for init training:', (time - start))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end of better view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "total_query = NUM_QUERY + extra\n",
    "\t\t\n",
    "## query\n",
    "q_idxs = bald_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 873, 1408,  489, 1376,  925,  276,  789, 1312,  342,  931,  932,\n",
       "         48,  352,  672, 1421,  593, 1401,  464, 1415, 1429, 1287, 1201,\n",
       "       1200,  125,  861,  114,  594,  407,  533,  130, 1186,  540, 1438,\n",
       "       1439,  939, 1431,  580,  589,  739,  738,  171,  609, 1106, 1116,\n",
       "        886,  597,  465,  279, 1081, 1218,  290,  291,  595,  611,   29,\n",
       "        539, 1083,  347,  630,  912,  785,  519,  813,  492, 1325,  966,\n",
       "       1061, 1060,  990,  216,  217,  405,  467,  622,  962,  664, 1226,\n",
       "        144, 1232,  365,  530, 1379, 1380, 1398, 1247,  913,  689,  339,\n",
       "        353,   40,   55,  749,  967, 1123, 1434,  438,  559,  902,  262,\n",
       "        627,  866,  100, 1114, 1113,  776,  422, 1172,  753,  303, 1251,\n",
       "        750,   41,  450,   15,  596,  471,  314,   25,  356,  300,  321,\n",
       "        127,  598, 1111,  416,  415, 1107, 1131, 1416,  147, 1290, 1427,\n",
       "        196, 1313, 1222, 1067, 1068, 1066])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_bald_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n):\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    # unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    print('BALD querying starts!')\n",
    "    print('Query {} data from {} unlabeled training data.'.format(n, len(unlabeled_data)))\n",
    "    m = 1e4  # number of MC samples for label combinations\n",
    "    # if LOW_RES:\n",
    "    #     num_sub_pool = len(unlabeled_data)\n",
    "    # else:\n",
    "    #     num_sub_pool = 500  # number of datapoints in the subpool from which we acquire\n",
    "    num_sub_pool = 500  # number of datapoints in the subpool from which we acquire\n",
    "\n",
    "    c = 10\n",
    "    k = 10\n",
    "    processing_batch_size = 8 # Model_batch=8 or 128\n",
    "\n",
    "    # performing BatchBALD on the whole pool is very expensive, so we take a random subset of the pool.\n",
    "    num_extra = len(unlabeled_data) - num_sub_pool\n",
    "\n",
    "    if num_extra > 0:\n",
    "        sub_pool_data, _ = torch.utils.data.random_split(unlabeled_data, [num_sub_pool, num_extra])\n",
    "        sub_pool_idxs = np.array(sub_pool_data.indices)\n",
    "    else:\n",
    "        # even if we don't have enough data left to split, we still need to\n",
    "        # call random_splot to avoid messing up the indexing later on\n",
    "        sub_pool_data, _ = torch.utils.data.random_split(unlabeled_data, [len(unlabeled_data), 0])\n",
    "        sub_pool_idxs = np.array(sub_pool_data.indices)\n",
    "\n",
    "    # forward pass on the pool once to get class probabilities for each x\n",
    "    pool_loader = torch.utils.data.DataLoader(sub_pool_data,\n",
    "        batch_size=processing_batch_size, pin_memory=True, shuffle=False, collate_fn=default_data_collator)\n",
    "    pool_features = train_features.select(sub_pool_idxs)\n",
    "\n",
    "    pool_p_y = get_batch_prob_dropout_split(pool_loader, device, pool_features, examples, n_drop=k).permute(1, 0, 2) \n",
    "    # pool_p_y.shape = n * k * c = 500 * 3 * 200\n",
    "    # this only need to be calculated once so we pull it out of the loop\n",
    "    H2 = (H(pool_p_y).sum(axis=(1,2))/k)\n",
    "\n",
    "    # get all class combinations\n",
    "    c_1_to_n = class_combinations(k, n, m) # TODO: change from 'n' to 'total_query' # TODO: test1- change c to k\n",
    "    # print('c_1_to_n.shape:', c_1_to_n.shape) # (10000, 138) ? # tensor of size [m * n]\n",
    "\n",
    "    # tensor of size [m * k]\n",
    "    p_y_1_to_n_minus_1 = None\n",
    "\n",
    "    # store the indices of the chosen datapoints in the subpool\n",
    "    best_sub_local_indices = []\n",
    "\n",
    "    # create a mask to keep track of which indices we've chosen\n",
    "    remaining_indices = torch.ones(len(sub_pool_data), dtype=bool)\n",
    "    # remaining_indices = torch.ones(total_query, dtype=bool)\n",
    "    for batch_n in range(n): # TODO: change from 'n' to 'total_query'\n",
    "        # tensor of size [N * m * l] # [500, 10000, 3]\n",
    "        p_y_n = pool_p_y[:, c_1_to_n[:, batch_n], :] # wierd here # TODO: test1\n",
    "        # p_y_n = pool_p_y[:, :, c_1_to_n[:, batch_n]] # try with this\n",
    "        # tensor of size [N * m * k]   \n",
    "        if p_y_1_to_n_minus_1 == None:\n",
    "            p_y_1_to_n = p_y_n\n",
    "        # elif torch.tensor(0) in p_y_1_to_n_minus_1:\n",
    "        #     p_y_1_to_n = p_y_n\n",
    "        else:\n",
    "            p_y_1_to_n = torch.einsum('mk,pmk->pmk', p_y_1_to_n_minus_1, p_y_n)\n",
    "\n",
    "        # and compute the left entropy term\n",
    "        H1 = H(p_y_1_to_n.mean(axis=2)).sum(axis=1)\n",
    "        # if H1[0] == torch.tensor(0): break\n",
    "        # scores is a vector of scores for each element in the pool.\n",
    "        # mask by the remaining indices and find the highest scoring element\n",
    "        scores = H1 - H2\n",
    "        # uncertainties_ = H2 - H1\n",
    "        \n",
    "        # print(len(uncertainties_))\n",
    "        best_local_index = torch.argmax(scores - 1*(~remaining_indices)).item()\n",
    "\n",
    "        # if best_local_index == 0:\n",
    "        #     print(f'Best idx {best_local_index} in {batch_n}')\n",
    "\n",
    "        # print(f'Best idx {best_local_index}')\n",
    "        # print(f'Best idxs {scores[remaining_indices].sort(descending=True)[1][0]}')\n",
    "        \n",
    "        best_sub_local_indices.append(best_local_index)\n",
    "        # print(best_sub_local_indices)\n",
    "        # save the computation for the next batch\n",
    "        p_y_1_to_n_minus_1 = p_y_1_to_n[best_local_index]\n",
    "        # print(p_y_1_to_n_minus_1[0][0])\n",
    "        # remove the chosen element from the remaining indices mask\n",
    "        remaining_indices[best_local_index] = False\n",
    "        \n",
    "\n",
    "    # we've subset-ed our dataset twice, so we need to go back through\n",
    "    # subset indices twice to recover the global indices of the chosen data\n",
    "    best_local_indices = np.array(sub_pool_idxs)[best_sub_local_indices]\n",
    "    best_global_indices = np.array(unlabeled_idxs)[best_local_indices]\n",
    "    assert len(set(best_global_indices)) == n, f'there are only {len(set(best_global_indices))} unique idx in q_idx'\n",
    "    return best_global_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BALD querying starts!\n",
      "Query 138 data from 1442 unlabeled training data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c654ebb56643c1a247db0378a8503f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe1bf715c3e40bfa9008266f5143394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1306a67c3ace497bbfa8cc9218d3bdb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a1e912fe414895bb90bca6f98fda9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a58228b07d347f3aa584c41987208d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c9cdb06c2244b4a2dd63001be6398c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb Cell 28\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstrauss.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bstrauss.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m total_query \u001b[39m=\u001b[39m NUM_QUERY \u001b[39m+\u001b[39m extra\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bstrauss.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m q_idxs_bb \u001b[39m=\u001b[39m batch_bald_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n",
      "\u001b[1;32m/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb Cell 28\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstrauss.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m p_y_n \u001b[39m=\u001b[39m pool_p_y[:, c_1_to_n[:, batch_n], :] \u001b[39m# wierd here\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstrauss.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# p_y_n = pool_p_y[:, :, c_1_to_n[:, batch_n]] # try with this\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstrauss.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# tensor of size [N * m * k]   \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstrauss.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mif\u001b[39;00m p_y_1_to_n_minus_1 \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstrauss.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     p_y_1_to_n \u001b[39m=\u001b[39m p_y_n\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstrauss.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# elif torch.tensor(0) in p_y_1_to_n_minus_1:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstrauss.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m#     p_y_1_to_n = p_y_n\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstrauss.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "total_query = NUM_QUERY + extra\n",
    "\t\t\n",
    "q_idxs_bb = batch_bald_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 173,  626,  194,  140, 1075,  176,  118, 1084,  918,  917,  137,\n",
       "       1097,  232, 1168,  575, 1151,  496,  215,  786,  802,  619, 1331,\n",
       "       1336, 1085,   92,  226,  828,  833, 1070,  187,  205,  959,  410,\n",
       "       1155, 1401,  724,   60,  725,  682, 1228, 1234, 1110,   12,   87,\n",
       "        434,  558,  663,  744,  941, 1235,  294,  384,  771,  789, 1159,\n",
       "       1419,  453,   79,   88,   42,  871,  150,   77,   95,  699,  500,\n",
       "        383, 1291, 1154, 1258,  938, 1345,  494,  411,  366, 1392,  210,\n",
       "        333,  536,  456, 1082,  798,  250, 1022,  817,  632,  656, 1090,\n",
       "       1093,  106,  681,  654,  914,  878,  671,  816,  385,   90,    4,\n",
       "        700, 1409, 1284,  168,  653,  441,  449,  711, 1170,  664,   97,\n",
       "       1043,  560,  354,  183,  259,  723,  722, 1079, 1096, 1014,  277,\n",
       "        691,  762, 1176, 1058, 1037,  546,  315,  198, 1220,  238, 1039,\n",
       "        462,  610,   85,  791,  690,  823])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_idxs_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BALD querying starts!\n",
      "Query 138 data from 1442 unlabeled training data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dbefd3649848d0b40b8540669962a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cd534165b44bde92fbf7a5fc14d967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d835416e0d40999f50f1928424da30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9f3f4923374e56ae11744f17a5ef78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95bb017d98e49e390e61afda5756a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04bd58f8f24441d858578305bae7bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b6f9c3ab334fa49539e8388479b53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90994df2b4d24de3a3aa1a5b7d64cf4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8a26821fb246ea8d6372bb6bf94e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d901fcb900d4e6c979f023dd51b8ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35daecc8470144cdb2db307abd62a82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c43cc068de8496084c87cb71eb73925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd65a8f91ed4b3d961bb50ce6c110b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9faf03092859442bb4ee3348be8052a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67393043903241c9aefafcc3f960e8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b050c5572fce4108ad9b05efe3bc0634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8595352639d451b86d6aa9e38748bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a915740af8d47fc8fd277a1bbea4cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54ae533c18c4ced9085b02c4ebca2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c30419e1c24d0f9998d082862c0c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c09a749b054751b53a46a610bc4da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b8e45f51ee4331923dea63a44bbf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0a7fcbd6924c019bba04af6b1e827c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ce995cb53a40b696a25a87b2e76a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345bc54173304a388862e3ceecb9c725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aac276c1aba430697c9fe7f6c1561d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54c5a6ddc14420687914b4358bd1e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5df64b0c8a4417885ad49afdadde78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfddbef0a9d411d869ec007c36668a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70751ee90c4e4d1eab8e053ecf773af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faab8b8e375344e5a7d094f9cf1e9fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae08bdf294046bb8652b96e1ec91004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bbe742ba5640ca9a5d7ca3948aa1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437250966906445b939b96ebb51d996f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc63fdd0cb8548ed92773a389ebf4e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec836c08d2e47be9f264495f5a3633d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42d7cb4937247c891db018bb92d8170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8e3fcb54ca4c5bbbe10e47aeddeac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c83e182ffd04ebdb4cfa5827722d449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3303dfe77a344602b13f213d1c5620ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "# unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "print('BALD querying starts!')\n",
    "print('Query {} data from {} unlabeled training data.'.format(total_query, len(unlabeled_data)))\n",
    "# TODO: change from 'n' to 'total_query'\n",
    "m = 1e4  # number of MC samples for label combinations\n",
    "# if LOW_RES:\n",
    "#     num_sub_pool = len(unlabeled_data)\n",
    "# else:\n",
    "#     num_sub_pool = 500  # number of datapoints in the subpool from which we acquire\n",
    "num_sub_pool = 500  # number of datapoints in the subpool from which we acquire\n",
    "\n",
    "c = 10\n",
    "k = 10\n",
    "processing_batch_size = 128 # Model_batch=8 or 128\n",
    "\n",
    "# performing BatchBALD on the whole pool is very expensive, so we take a random subset of the pool.\n",
    "num_extra = len(unlabeled_data) - num_sub_pool\n",
    "\n",
    "if num_extra > 0:\n",
    "    sub_pool_data, _ = torch.utils.data.random_split(unlabeled_data, [num_sub_pool, num_extra])\n",
    "    sub_pool_idxs = np.array(sub_pool_data.indices)\n",
    "else:\n",
    "    # even if we don't have enough data left to split, we still need to\n",
    "    # call random_splot to avoid messing up the indexing later on\n",
    "    sub_pool_data, _ = torch.utils.data.random_split(unlabeled_data, [len(unlabeled_data), 0])\n",
    "    sub_pool_idxs = np.array(sub_pool_data.indices)\n",
    "\n",
    "# forward pass on the pool once to get class probabilities for each x\n",
    "pool_loader = torch.utils.data.DataLoader(sub_pool_data,\n",
    "    batch_size=processing_batch_size, pin_memory=True, shuffle=False, collate_fn=default_data_collator)\n",
    "pool_features = train_features.select(sub_pool_idxs)\n",
    "\n",
    "pool_p_y = get_prob_dropout_split(pool_loader, device, pool_features, train_data, n_drop=k).permute(1, 0, 2) \n",
    "# pool_p_y.shape = n * k * c = 500 * 3 * 200\n",
    "# TODO: change from 'examples' to 'train_data'\n",
    "# this only need to be calculated once so we pull it out of the loop\n",
    "H2 = (H(pool_p_y).sum(axis=(1,2))/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 20, 10])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_p_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_1_to_n.shape: [5 9 9 9 3 4 5 4 8 4 0 7 2 4 4 8 8 3 3 4 5 5 9 1 7 7 5 2 3 6 5 0 5 3 5 6 8\n",
      " 0 6 0 9 1 0 4 9 4 9 5 2 9 0 8 7 7 4 3 3 8 0 6 9 8 8 8 1 8 8 2 8 0 4 1 8 9\n",
      " 1 3 5 5 4 6 0 1 9 1 2 0 8 0 8 4 8 2 8 3 1 2 5 3 8 4 9 3 5 5 0 1 6 9 8 7 1\n",
      " 1 4 4 8 5 6 5 2 5 5 7 8 8 4 8 0 6 7 9 0 9 2 6 4 0 8 0]\n"
     ]
    }
   ],
   "source": [
    "c_1_to_n = class_combinations(c, total_query, m)\n",
    "print('c_1_to_n.shape:', c_1_to_n[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all class combinations\n",
    "c_1_to_n = class_combinations(c, total_query, m) # TODO: change from 'n' to 'total_query' # TODO: test1- change c to k\n",
    "# print('c_1_to_n.shape:', c_1_to_n.shape) # (10000, 138) ? # tensor of size [m * n]\n",
    "\n",
    "# tensor of size [m * k]\n",
    "p_y_1_to_n_minus_1 = None\n",
    "\n",
    "# store the indices of the chosen datapoints in the subpool\n",
    "best_sub_local_indices = []\n",
    "\n",
    "# create a mask to keep track of which indices we've chosen\n",
    "remaining_indices = torch.ones(len(sub_pool_data), dtype=bool)\n",
    "# remaining_indices = torch.ones(total_query, dtype=bool)\n",
    "for batch_n in range(total_query): # TODO: change from 'n' to 'total_query'\n",
    "    # tensor of size [N * m * l] # [500, 10000, 3]\n",
    "    p_y_n = pool_p_y[:, c_1_to_n[:, batch_n], :] # wierd here, bc k was < c # TODO: test1\n",
    "    # p_y_n = pool_p_y[:, :, c_1_to_n[:, batch_n]] # try with this\n",
    "    # tensor of size [N * m * k]   \n",
    "    if p_y_1_to_n_minus_1 == None:\n",
    "        p_y_1_to_n = p_y_n\n",
    "        # print(1)\n",
    "    elif torch.tensor(0) in p_y_1_to_n_minus_1:\n",
    "        p_y_1_to_n = p_y_n\n",
    "        # print(2)\n",
    "    else:\n",
    "        p_y_1_to_n = torch.einsum('mk,pmk->pmk', p_y_1_to_n_minus_1, p_y_n)\n",
    "        # print(3)\n",
    "\n",
    "    # and compute the left entropy term\n",
    "    H1 = H(p_y_1_to_n.mean(axis=2)).sum(axis=1)\n",
    "\n",
    "    # if H1[0] == torch.tensor(0): break\n",
    "    # scores is a vector of scores for each element in the pool.\n",
    "    # mask by the remaining indices and find the highest scoring element\n",
    "    scores = H1 - H2\n",
    "\n",
    "    # uncertainties_ = H2 - H1\n",
    "\n",
    "    remaining_scores = scores[remaining_indices]\n",
    "    \n",
    "    best_local_index = torch.argmax(scores - max(scores)*(~remaining_indices)).item()\n",
    "    # best_local_index_ = remaining_scores.sort(descending=True)[1][0]\n",
    "\n",
    "    # if remaining_indices[best_local_index] == False: break\n",
    "    # if best_local_index == 0:\n",
    "    #     print(f'Best idx {best_local_index} in {batch_n}')\n",
    "\n",
    "    # print(f'Best idx {best_local_index}')\n",
    "    # print(f'Best idxs {best_local_index_}')\n",
    "    # print(f'2Best idxs {remaining_scores.sort(descending=True)[1][1]}')\n",
    "    # print(f'3Best idxs {remaining_scores.sort(descending=True)[1][2]}')\n",
    "    # print(remaining_scores[best_local_index_])\n",
    "    # print(remaining_scores[remaining_scores.sort(descending=True)[1][1]])\n",
    "    # print(remaining_scores[remaining_scores.sort(descending=True)[1][2]])\n",
    "    # print(scores[best_local_index_])\n",
    "    # print(scores[remaining_scores.sort(descending=True)[1][1]])\n",
    "    # print(scores[remaining_scores.sort(descending=True)[1][2]])\n",
    "    \n",
    "    # best_sub_local_indices.append(best_local_index_)\n",
    "    # # print(best_sub_local_indices)\n",
    "    # # save the computation for the next batch\n",
    "    # p_y_1_to_n_minus_1 = p_y_1_to_n[best_local_index_]\n",
    "    # # print(p_y_1_to_n_minus_1[0][0])\n",
    "    # # i = 1\n",
    "    # # if remaining_indices[best_local_index_] == False:\n",
    "    #     # print('H1:', H1[:5])\n",
    "\n",
    "    #     # print(scores[:5])\n",
    "    #     # print(len(remaining_scores))\n",
    "    #     # print(remaining_scores.sort(descending=True)[0][:5])\n",
    "    #     # print(remaining_scores.sort(descending=True)[1][:5])\n",
    "\n",
    "    # # remove the chosen element from the remaining indices mask\n",
    "    # remaining_indices[best_local_index_] = False\n",
    "\n",
    "    best_sub_local_indices.append(best_local_index)\n",
    "    # save the computation for the next batch\n",
    "    p_y_1_to_n_minus_1 = p_y_1_to_n[best_local_index]\n",
    "    # remove the chosen element from the remaining indices mask\n",
    "    remaining_indices[best_local_index] = False\n",
    "    \n",
    "\n",
    "# we've subset-ed our dataset twice, so we need to go back through\n",
    "# subset indices twice to recover the global indices of the chosen data\n",
    "best_local_indices = np.array(sub_pool_idxs)[best_sub_local_indices]\n",
    "best_global_indices = np.array(unlabeled_idxs)[best_local_indices]\n",
    "assert len(set(best_global_indices)) == total_query, f'there are only {len(set(best_global_indices))} unique idx in q_idx'\n",
    "# return best_global_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bstrauss.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m scores\u001b[39m.\u001b[39;49mindex\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "scores.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1315.2615)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(246.2740)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_scores[311]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "for r in remaining_indices:\n",
    "    if r == True:\n",
    "        n += 1\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1078,  173,  118,  508, 1084,  211,  709,   85,  795,  796,  197,\n",
       "        107, 1188, 1132,  218,  233,  417, 1097,  439,  137,  581,  141,\n",
       "        199,  201,  549, 1034,  408, 1168, 1145, 1318,  800,  744, 1186,\n",
       "       1092,  315,  271, 1044,  192,  457, 1030,  725,   95,  702, 1012,\n",
       "       1009, 1185, 1054,  193, 1227,    4,  894,  893, 1205, 1155, 1013,\n",
       "        426,  789,  154,  543, 1032,  971, 1058, 1064, 1314,  388,  852,\n",
       "        939,  513,  985,  723, 1277,  722, 1062, 1344,  435,   64, 1035,\n",
       "        706,  441,  724,  257,  812, 1079, 1373, 1275,  698, 1069, 1041,\n",
       "        122,   79, 1025, 1419,  455, 1137,  526,  632, 1001,  144,  696,\n",
       "        854,  873,  161,  663,  139, 1106, 1401, 1110,  682,  494,  628,\n",
       "        354,  557,   77,  618,   63, 1171,  601, 1285, 1071,   44,  743,\n",
       "       1015,  990, 1128, 1167,  701,  771,  649,  379,   54, 1376,   62,\n",
       "        892, 1257,  382,  788,   56, 1424])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_global_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[321,\n",
       " 77,\n",
       " 434,\n",
       " 378,\n",
       " 146,\n",
       " 47,\n",
       " 289,\n",
       " 53,\n",
       " 153,\n",
       " 307,\n",
       " 494,\n",
       " 91,\n",
       " 331,\n",
       " 185,\n",
       " 282,\n",
       " 456,\n",
       " 388,\n",
       " 43,\n",
       " 368,\n",
       " 60,\n",
       " 243,\n",
       " 391,\n",
       " 181,\n",
       " 475,\n",
       " 24,\n",
       " 387,\n",
       " 472,\n",
       " 353,\n",
       " 112,\n",
       " 418,\n",
       " 403,\n",
       " 367,\n",
       " 354,\n",
       " 57,\n",
       " 54,\n",
       " 208,\n",
       " 186,\n",
       " 80,\n",
       " 267,\n",
       " 467,\n",
       " 55,\n",
       " 180,\n",
       " 199,\n",
       " 263,\n",
       " 269,\n",
       " 428,\n",
       " 215,\n",
       " 462,\n",
       " 258,\n",
       " 424,\n",
       " 9,\n",
       " 140,\n",
       " 265,\n",
       " 29,\n",
       " 187,\n",
       " 249,\n",
       " 122,\n",
       " 189,\n",
       " 228,\n",
       " 393,\n",
       " 392,\n",
       " 370,\n",
       " 155,\n",
       " 435,\n",
       " 276,\n",
       " 90,\n",
       " 319,\n",
       " 309,\n",
       " 312,\n",
       " 222,\n",
       " 492,\n",
       " 236,\n",
       " 426,\n",
       " 247,\n",
       " 72,\n",
       " 16,\n",
       " 464,\n",
       " 330,\n",
       " 463,\n",
       " 279,\n",
       " 96,\n",
       " 419,\n",
       " 232,\n",
       " 219,\n",
       " 364,\n",
       " 375,\n",
       " 133,\n",
       " 266,\n",
       " 87,\n",
       " 495,\n",
       " 158,\n",
       " 344,\n",
       " 8,\n",
       " 455,\n",
       " 485,\n",
       " 323,\n",
       " 410,\n",
       " 217,\n",
       " 152,\n",
       " 313,\n",
       " 334,\n",
       " 79,\n",
       " 245,\n",
       " 56,\n",
       " 459,\n",
       " 246,\n",
       " 422,\n",
       " 135,\n",
       " 115,\n",
       " 221,\n",
       " 99,\n",
       " 188,\n",
       " 211,\n",
       " 179,\n",
       " 318,\n",
       " 62,\n",
       " 343,\n",
       " 175,\n",
       " 49,\n",
       " 465,\n",
       " 498,\n",
       " 397,\n",
       " 411,\n",
       " 200,\n",
       " 229,\n",
       " 61,\n",
       " 166,\n",
       " 117,\n",
       " 310,\n",
       " 70,\n",
       " 288,\n",
       " 105,\n",
       " 399,\n",
       " 453,\n",
       " 385,\n",
       " 93,\n",
       " 425,\n",
       " 438]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sub_local_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores[remaining_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "there are only 138 unique idx in q_idx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bstrauss.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/batchbald.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m total_query, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mthere are only \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(best_global_indices))\u001b[39m}\u001b[39;00m\u001b[39m unique idx in q_idx\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: there are only 138 unique idx in q_idx"
     ]
    }
   ],
   "source": [
    "assert 1 == total_query, f'there are only {len(set(best_global_indices))} unique idx in q_idx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BALD querying starts!\n",
      "Query 138 data from 1442 unlabeled training data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d362ba3c2dc4e84811d6b593626228e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4da0ea031b43eeaff9c85a0dbf7a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140ea8b3e47e4d71907867183484c46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58a2924d16e4b31bae3a5250b8efee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a918a2138143aba5a55c0d68a46ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob_dropout:   0%|          | 0/181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf6d3b27f7a41a786240154af339680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 638, 1185, 1184,  225,  224, 1201, 1200,  746, 1202, 1402,  538,\n",
       "        483, 1341, 1216,  412,  867, 1088, 1087,  357, 1010, 1009, 1008,\n",
       "       1109, 1436, 1437,  648,  647,  527,  542,  707,  997,  998,    5,\n",
       "        905, 1239, 1190,  180,  359,  358,  600, 1208,  924, 1271, 1129,\n",
       "       1406,  980, 1192,    0, 1322, 1189,  629,  451,  293, 1246,  380,\n",
       "        849, 1165,  156,  157,  376,  485,  484, 1181, 1432,  904, 1327,\n",
       "        343, 1323, 1233, 1198, 1182,  987,  988, 1106, 1389,  132,  947,\n",
       "       1111,  309, 1390, 1422,  890,  889,   37,   66, 1417,   32,  373,\n",
       "        284,   40,  951,  922,  139,  645,  646,  573,  642,  862,  921,\n",
       "       1211, 1397,  920,  166, 1433, 1197, 1196,  639,  289,  891,  954,\n",
       "       1328,  124,  487,  223, 1410, 1317, 1126,  272, 1384,  949, 1241,\n",
       "        946,   47,  935,    9, 1108,  636, 1019,  814,  304,  126,  590,\n",
       "        452,   33, 1304, 1240,  530,  160])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "total_query = NUM_QUERY + extra\n",
    "\n",
    "unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "unlabeled_dataloader = DataLoader(\n",
    "    unlabeled_data,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=MODEL_BATCH,\n",
    ")\n",
    "print('BALD querying starts!')\n",
    "print('Query {} data from {} unlabeled training data.'.format(total_query, len(unlabeled_data)))\n",
    "# to fasten development, turn dropout from 10 to 2\n",
    "probs = get_prob_dropout_split(unlabeled_dataloader, device, unlabeled_features, train_data, n_drop=3)\n",
    "\n",
    "c = 3\n",
    "k = 200\n",
    "# print(probs.shape) # k n c\n",
    "Y = probs.permute(1, 0, 2)\n",
    "# print(torch_stack_probs.shape) # n k c\n",
    "entropy1 = H(Y.mean(1)).sum(axis=1)\n",
    "entropy2 = H(Y).sum(axis=(1,2))/k\n",
    "\n",
    "uncertainties = entropy2 - entropy1 ### for getting the descending order\n",
    "### same as -> return H1 - H2 from score(model, x, k=100)\n",
    "\n",
    "unlabeled_idxs[uncertainties.sort()[1][:total_query]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.9013, -4.3057, -1.9604,  ..., -3.8115, -4.4537, -1.6220])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertainties"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alqa",
   "language": "python",
   "name": "alqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
