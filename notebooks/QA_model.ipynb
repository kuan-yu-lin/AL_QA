{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Model \n",
    "## no trainer\n",
    "\n",
    "- dataset\n",
    "- torch\n",
    "- transformers\n",
    "- transformers[torch]\n",
    "- evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/arbeitsdaten31/studenten1/linku/.venv/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DefaultDataCollator,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/mount/arbeitsdaten31/studenten1/linku/cache\n",
      "env: HF_MODULES_CACHE=/mount/arbeitsdaten31/studenten1/linku/cache\n",
      "env: HF_DATASETS_CACHE=/mount/arbeitsdaten31/studenten1/linku/cache\n"
     ]
    }
   ],
   "source": [
    "model_dir = '/mount/arbeitsdaten31/studenten1/linku/models'\n",
    "CACHE_DIR='/mount/arbeitsdaten31/studenten1/linku/cache'\n",
    "%set_env TRANSFORMERS_CACHE $CACHE_DIR\n",
    "%set_env HF_MODULES_CACHE $CACHE_DIR\n",
    "%set_env HF_DATASETS_CACHE $CACHE_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arguments.py\n",
    "\n",
    "args_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_input_ALstrategy = 'MarginSampling'\n",
    "args_input_initseed = 100 # 1000\n",
    "args_input_quota = 100 # 1000\n",
    "args_input_batch = 35 # 128\n",
    "args_input_dataset_name = 'SQuAD'\n",
    "args_input_iteration = 1\n",
    "args_input_model_batch = 8 # already add in arguments.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/users1/linku/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00, 77.96it/s]\n"
     ]
    }
   ],
   "source": [
    "squad = load_dataset(args_input_dataset_name.lower())\n",
    "# squad[\"train\"] = squad[\"train\"].shuffle(42).select(range(2000))\n",
    "squad[\"train\"] = squad[\"train\"].select(range(3000))\n",
    "squad[\"validation\"] = squad[\"validation\"].select(range(1000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will preprocess the dataset (training and evaluation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_features(examples):\n",
    "    # keep [\"offset_mapping\"], for compute_metrics()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        example_ids.append(examples[\"id\"][sample_idx]) # newly added for used in unlabel data predict\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples):\n",
    "    # no ['offset_mapping'], for .train() and .eval()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        example_ids.append(examples[\"id\"][sample_idx]) # newly added for used in unlabel data predict\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/users1/linku/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-d10ddabde563b597.arrow\n",
      "Loading cached processed dataset at /home/users1/linku/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-773e51dc4558cf67.arrow\n",
      "Loading cached processed dataset at /home/users1/linku/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-0f087a92e258c27b.arrow\n",
      "Loading cached processed dataset at /home/users1/linku/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-0f087a92e258c27b.arrow\n"
     ]
    }
   ],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "# load tokenizer for dataset preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# preprocess data\n",
    "train_dataset = squad[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"train\"].column_names,\n",
    ")\n",
    "train_features = squad[\"train\"].map(\n",
    "    preprocess_training_features,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"train\"].column_names,\n",
    ")\n",
    "val_dataset = squad[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"validation\"].column_names,\n",
    ")\n",
    "val_features = squad[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "train_features.set_format(\"torch\")\n",
    "val_dataset = val_dataset.remove_columns([\"offset_mapping\"])\n",
    "val_dataset.set_format(\"torch\")\n",
    "val_features.set_format(\"torch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    \n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(model, eval_dataloader, device, features, examples):\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    # accelerator.print(\"Evaluation!\")\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    return compute_metrics(start_logits, end_logits, features, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(model, eval_dataloader, device, features, examples):\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    # accelerator.print(\"Evaluation!\")\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    prob_dict = {}\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20 # TODO: if set n_best as 5, will it effect the time??\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        # context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "        \n",
    "        if len(answers) > 1:\n",
    "            if example_to_features[example_id][0] not in prob_dict:\n",
    "                prob_dict[example_to_features[example_id][0]] = softmax(answers)\n",
    "            else:\n",
    "                prob_dict[example_to_features[example_id][0]] += softmax(answers)\n",
    "        elif example_to_features[example_id] != []:\n",
    "            if example_to_features[example_id][0] not in prob_dict:\n",
    "                prob_dict[example_to_features[example_id][0]] = np.array([0])\n",
    "    \n",
    "    return prob_dict\n",
    "# move to evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_dropout(model, eval_dataloader, device, features, examples, n_drop=10):\n",
    "    model.train()\n",
    "    \n",
    "    prob_dict = {}\n",
    "    for_check = []\n",
    "    \n",
    "    for i in range(n_drop):\n",
    "        \n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        # accelerator.print(\"Evaluation!\")\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "            end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "        start_logits = np.concatenate(start_logits)\n",
    "        end_logits = np.concatenate(end_logits)\n",
    "        start_logits = start_logits[: len(features)]\n",
    "        end_logits = end_logits[: len(features)]\n",
    "\n",
    "        example_to_features = collections.defaultdict(list)\n",
    "        max_answer_length = 30\n",
    "        n_best = 20\n",
    "            \n",
    "        for idx, feature in enumerate(features):\n",
    "            example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "        n = 0\n",
    "        for example in tqdm(examples):\n",
    "            example_id = example[\"id\"]\n",
    "            # context = example[\"context\"]\n",
    "            answers = []\n",
    "\n",
    "            # Loop through all features associated with that example\n",
    "            for feature_index in example_to_features[example_id]:\n",
    "                start_logit = start_logits[feature_index]\n",
    "                end_logit = end_logits[feature_index]\n",
    "                offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # Skip answers that are not fully in the context\n",
    "                        if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                            continue\n",
    "                        # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                        if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                        ):\n",
    "                            continue\n",
    "\n",
    "                        answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "\n",
    "            \n",
    "            if 1 < len(answers) < 150:\n",
    "                zero_list = [0] * (150 - len(answers))\n",
    "                answers.extend(zero_list)\n",
    "            else:\n",
    "                answers[:150]\n",
    "\n",
    "            if len(answers) > 1:\n",
    "                if example_to_features[example_id][0] not in prob_dict:\n",
    "                    prob_dict[example_to_features[example_id][0]] = softmax(answers)\n",
    "                else:\n",
    "                    prob_dict[example_to_features[example_id][0]] += softmax(answers)\n",
    "            elif example_to_features[example_id] != []:\n",
    "                if example_to_features[example_id][0] not in prob_dict:\n",
    "                    prob_dict[example_to_features[example_id][0]] = np.array([0])\n",
    "            # if n == 0 and len(softmax(answers)) > 1:\n",
    "            #     for_check.append(answers[:5])\n",
    "            #     n += 1      \n",
    "\n",
    "    for key in prob_dict.keys():\n",
    "        prob_dict[key] /= n_drop\n",
    "    # return prob_dict, for_check\n",
    "    return prob_dict\n",
    "# move to evaluation.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unlabel_data(n_pool, labeled_idxs, train_dataset, train_feature):\n",
    "    unlabeled_idxs = np.arange(n_pool)[~labeled_idxs]\n",
    "    unlabeled_data = train_dataset.select(indices=unlabeled_idxs)\n",
    "    return unlabeled_idxs, unlabeled_data\n",
    "\n",
    "# move to utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "# move to utils.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling_query(labeled_idxs, n):\n",
    "    return np.random.choice(np.where(labeled_idxs==0)[0], n, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DataLoader\n",
    "# import default_data_collator\n",
    "import itertools\n",
    "def margin_sampling_query(n_pool, labeled_idxs, train_dataset, train_features, examples, model, device, n):\n",
    "    # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    \n",
    "    unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data,\n",
    "\t\tshuffle=True,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=8,\n",
    "\t)\n",
    "\n",
    "    # deepAL+: probs = self.predict_prob(unlabeled_data)\n",
    "    prob_dict = get_prob(model, unlabeled_dataloader, device, unlabeled_features, examples)\n",
    "    \n",
    "    # deepAL+: probs_sorted, _ = probs.sort(descending=True)\n",
    "    # deepAL+: uncertainties = probs_sorted[:, 0] - probs_sorted[:,1]\n",
    "    uncertainties_dict = {}\n",
    "    for idx, probs in prob_dict.items():\n",
    "        if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "            sort_probs = np.sort(probs)[::-1] # This method returns a copy of the array, leaving the original array unchanged.\n",
    "            uncertainties = sort_probs[0] - sort_probs[1]\n",
    "            uncertainties_dict[idx] = uncertainties\n",
    "        elif idx:\n",
    "            uncertainties_dict[idx] = np.array([0])\n",
    "\n",
    "    # deepAL+: return unlabeled_idxs[uncertainties.sort()[1][:n]] \n",
    "    sorted_uncertainties_dict = sorted(uncertainties_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return unlabeled_idxs[[idx for (idx, uncertainties) in sorted_uncertainties_list[:n]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_confidence_query(n_pool, labeled_idxs, train_dataset, trainer_qs, example, n):\n",
    "    # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    \n",
    "    # deepAL+: probs = self.predict_prob(unlabeled_data)\n",
    "    prob_list_dict = get_prob(trainer_qs, unlabeled_data, example)\n",
    "    \n",
    "    # deepAL+: uncertainties = probs.max(1)[0]\n",
    "    confidence_list_dict = []\n",
    "    for d in prob_list_dict:\n",
    "        if len(d['probs']) > 1: # if prob_dict['probs'] is not 0\n",
    "            confidence = max(d['probs'])\n",
    "            confidence_list_dict.append(\n",
    "                {'idx': d['idx'], \n",
    "                    'confidence': confidence}\n",
    "                    )\n",
    "        elif d['idx']:\n",
    "            confidence_list_dict.append(\n",
    "                {'idx': d['idx'], \n",
    "                    'confidence': np.array([0])}\n",
    "                    )\n",
    "    # deepAL+: return unlabeled_idxs[uncertainties.sort()[1][:n]]\n",
    "    sorted_confidence_dict = sorted(confidence_list_dict, key=lambda d: d['confidence'])   \n",
    "    return unlabeled_idxs[[confidence_dict['idx'][0] for confidence_dict in sorted_confidence_dict[:n]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_ratio_query(n_pool, labeled_idxs, train_dataset, trainer_qs, example, n):\n",
    "    # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    \n",
    "    # deepAL+: probs = self.predict_prob(unlabeled_data)\n",
    "    prob_list_dict = get_prob(trainer_qs, unlabeled_data, example)\n",
    "    \n",
    "    # deepAL+: preds = torch.max(probs, 1)[0]\n",
    "    # deepAL+: uncertainties = 1.0 - preds\n",
    "    confidence_list_dict = []\n",
    "    for d in prob_list_dict:\n",
    "        if len(d['probs']) > 1: # if prob_dict['probs'] is not 0\n",
    "            confidence = max(d['probs'])\n",
    "            confidence_list_dict.append(\n",
    "                {'idx': d['idx'], \n",
    "                    'confidence': 1 - confidence}\n",
    "                    )\n",
    "        elif d['idx']:\n",
    "            confidence_list_dict.append(\n",
    "                {'idx': d['idx'], \n",
    "                    'confidence': np.array([0])}\n",
    "                    )\n",
    "\n",
    "    # deepAL+: return unlabeled_idxs[uncertainties.sort(descending=True)[1][:n]]\n",
    "    sorted_confidence_dict = sorted(confidence_list_dict, key=lambda d: d['confidence'], reverse=True)\n",
    "    return unlabeled_idxs[[confidence_dict['idx'][0] for confidence_dict in sorted_confidence_dict[:n]]]\n",
    "# comment for the same query as LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_query(n_pool, labeled_idxs, train_dataset, trainer_qs, example, n):\n",
    "    # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    \n",
    "    # deepAL+: probs = self.predict_prob(unlabeled_data)\n",
    "    prob_list_dict = get_prob(trainer_qs, unlabeled_data, example)\n",
    "    \n",
    "    # deepAL+: log_probs = torch.log(probs)\n",
    "    # deepAL+: uncertainties = (probs*log_probs).sum(1)\n",
    "    entropy_list_dict = []\n",
    "    for d in prob_list_dict:\n",
    "        if len(d['probs']) > 1: # if prob_dict['probs'] is not 0\n",
    "            log_probs = np.log(d['probs'])\n",
    "            entropy_list_dict.append(\n",
    "                {'idx': d['idx'], \n",
    "                'entropy': (d['probs']*log_probs).sum()}\n",
    "                )\n",
    "        elif d['idx']:\n",
    "            entropy_list_dict.append(\n",
    "                {'idx': d['idx'], \n",
    "                'entropy': np.array([0])}\n",
    "                )\n",
    "    # deepAL+: return unlabeled_idxs[uncertainties.sort()[1][:n]] # use smallest Entropy\n",
    "    sorted_entropy_dict = sorted(entropy_list_dict, key=lambda d: d['entropy'], reverse=True) # use largest Entropy, different from deepAL+\n",
    "    return unlabeled_idxs[[entropy_dict['idx'][0] for entropy_dict in sorted_entropy_dict[:n]]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_QUERY = args_input_batch\n",
    "NUM_INIT_LB = args_input_initseed\n",
    "NUM_ROUND = int(args_input_quota / args_input_batch)\n",
    "DATA_NAME = args_input_dataset_name\n",
    "STRATEGY_NAME = args_input_ALstrategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 4666\n",
    "# os.environ['TORCH_HOME']='./basicmodel'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(3)\n",
    "\n",
    "# fix random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.enabled  = True\n",
    "# torch.backends.cudnn.benchmark= True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = args_input_iteration\n",
    "model_batch = args_input_model_batch\n",
    "num_train_epochs = 3\n",
    "\n",
    "all_acc = []\n",
    "acq_time = []\n",
    "\n",
    "# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\t\n",
    "fp16_training = False\n",
    "\n",
    "if fp16_training:\n",
    "    !pip install accelerate==0.2.0\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator(fp16=True)\n",
    "    device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(num_training_steps, num_train_epochs, train_dataloader, device, model, optimizer, lr_scheduler):\n",
    "\tif fp16_training:\n",
    "\t\tmodel, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n",
    "\t\n",
    "\tprogress_bar = tqdm(range(num_training_steps))\n",
    "\tfor epoch in range(num_train_epochs):\n",
    "\t\t# Training\n",
    "\t\tmodel.train()\n",
    "\t\tfor step, batch in enumerate(train_dataloader):\n",
    "\t\t\tbatch = {key: value.to(device) for key, value in batch.items()}\n",
    "\t\t\toutputs = model(**batch)\n",
    "\t\t\tloss = outputs.loss\n",
    "\n",
    "\t\t\tif fp16_training:\n",
    "\t\t\t\taccelerator.backward(loss)\n",
    "\t\t\telse:\n",
    "\t\t\t\tloss.backward()\n",
    "\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tlr_scheduler.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tprogress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD\n",
      "MarginSampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:19<00:00,  2.01it/s]\n",
      "100%|██████████| 128/128 [00:23<00:00,  5.34it/s]\n",
      "100%|██████████| 1000/1000 [00:09<00:00, 102.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# repeate # iteration trials\n",
    "while (iteration > 0): \n",
    "\titeration = iteration - 1\n",
    "\n",
    "\t## data, network, strategy\n",
    "\tmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\toptimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "\tstart = datetime.datetime.now()\n",
    "\n",
    "\t## generate initial labeled pool\n",
    "\tn_pool = len(train_dataset)\n",
    "\tlabeled_idxs = np.zeros(n_pool, dtype=bool)\n",
    "\n",
    "\ttmp_idxs = np.arange(n_pool)\n",
    "\tnp.random.shuffle(tmp_idxs)\n",
    "\tlabeled_idxs[tmp_idxs[:args_input_initseed]] = True\n",
    "\n",
    "\trun_0_labeled_idxs = np.arange(n_pool)[labeled_idxs] \n",
    "\t\n",
    "\t## record acc performance \n",
    "\tacc = np.zeros(NUM_ROUND + 1) # build 3 runs + run_0 # origin 10 runs + run_0\n",
    "\t\n",
    "\t## load the selected train data to DataLoader\n",
    "\ttrain_dataloader = DataLoader(\n",
    "\t\ttrain_dataset.select(indices=run_0_labeled_idxs),\n",
    "\t\tshuffle=True,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=8,\n",
    "\t)\n",
    "\n",
    "\teval_dataloader = DataLoader(\n",
    "\t\tval_dataset, \n",
    "\t\tcollate_fn=default_data_collator, \n",
    "\t\tbatch_size=8\n",
    "\t)\n",
    "\n",
    "\tnum_update_steps_per_epoch = len(train_dataloader)\n",
    "\tnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "\tlr_scheduler = get_scheduler(\n",
    "\t\t\"linear\",\n",
    "\t\toptimizer=optimizer,\n",
    "\t\tnum_warmup_steps=0,\n",
    "\t\tnum_training_steps=num_training_steps,\n",
    "\t)\n",
    "\n",
    "\n",
    "\t## print info\n",
    "\tprint(DATA_NAME)\n",
    "\t# print('RANDOM SEED {}'.format(SEED))\n",
    "\tprint(STRATEGY_NAME) # print(type(strategy).__name__)\n",
    "\t\n",
    "\t## round 0 accuracy \n",
    "\tmodel_train(num_training_steps, num_train_epochs, train_dataloader, device, model, optimizer, lr_scheduler)\n",
    "\n",
    "\tacc[0] = get_pred(model, eval_dataloader, device, val_features, squad['validation'])['f1']\n",
    "\n",
    "\tprint('Round 0\\ntesting accuracy {}'.format(acc[0]))\n",
    "\tprint('\\n')\n",
    "\t\n",
    "\t## round 1 to rd\n",
    "\tfor rd in range(1, NUM_ROUND+1):\n",
    "\t\tprint('Round {}'.format(rd))\n",
    "\n",
    "\t\t## query\n",
    "\t\tif STRATEGY_NAME == 'RandomSampling':\n",
    "\t\t\tq_idxs = random_sampling_query(labeled_idxs, NUM_QUERY)\n",
    "\t\telif STRATEGY_NAME == 'MarginSampling':\n",
    "\t\t\tq_idxs = margin_sampling_query(n_pool, labeled_idxs, train_dataset, trainer_qs, squad['train'], NUM_QUERY)\n",
    "\t\telif STRATEGY_NAME == 'LeastConfidence':\n",
    "\t\t\tq_idxs = least_confidence_query(n_pool, labeled_idxs, train_dataset, trainer_qs, squad['train'], NUM_QUERY)\n",
    "\t\telif STRATEGY_NAME == 'EntropySampling':\n",
    "\t\t\tq_idxs = entropy_query(n_pool, labeled_idxs, train_dataset, trainer_qs, squad['train'], NUM_QUERY)\n",
    "\t\telif STRATEGY_NAME == 'MarginSamplingDropout':\n",
    "\t\t\tq_idxs = margin_sampling_dropout_query(n_pool, labeled_idxs, train_dataset, trainer_qs, squad['train'], NUM_QUERY)\n",
    "\t\telif STRATEGY_NAME == 'LeastConfidenceDropout':\n",
    "\t\t\tq_idxs = least_confidence_dropout_query(n_pool, labeled_idxs, train_dataset, trainer_qs, squad['train'], NUM_QUERY)\n",
    "\t\telif STRATEGY_NAME == 'EntropySamplingDropout':\n",
    "\t\t\tq_idxs = entropy_dropout_query(n_pool, labeled_idxs, train_dataset, trainer_qs, squad['train'], NUM_QUERY)\n",
    "\t\telif STRATEGY_NAME == 'VarRatio':\n",
    "\t\t\tq_idxs = var_ratio_query(n_pool, labeled_idxs, train_dataset, trainer_qs, squad['train'], NUM_QUERY)\n",
    "\t\t\n",
    "\t\t## update\n",
    "\t\tlabeled_idxs[q_idxs] = True\n",
    "\t\trun_rd_labeled_idxs = np.arange(n_pool)[labeled_idxs]\n",
    "\t\t\n",
    "\t\t## load the selected train data to DataLoader\n",
    "\t\ttrain_dataloader_rd = DataLoader(\n",
    "\t\t\ttrain_dataset.select(indices=run_rd_labeled_idxs),\n",
    "\t\t\tshuffle=True,\n",
    "\t\t\tcollate_fn=default_data_collator,\n",
    "\t\t\tbatch_size=8,\n",
    "\t\t)\n",
    "\n",
    "\t\t## train\n",
    "\t\tmodel_train(num_training_steps, num_train_epochs, train_dataloader, device, model, optimizer, lr_scheduler)\n",
    "\n",
    "\t\t## round rd accuracy\n",
    "\t\tacc[rd] = get_pred(model, eval_dataloader, device, val_features, squad['validation'])['f1']\n",
    "\t\tprint('testing accuracy {}'.format(acc[rd]))\n",
    "\t\tprint('\\n')\n",
    "\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\t\n",
    "\t## print results\n",
    "\tprint('SEED {}'.format(SEED))\n",
    "\tprint(STRATEGY_NAME)\n",
    "\tprint(acc)\n",
    "\tall_acc.append(acc)\n",
    "\t\n",
    "\t## record acq time\n",
    "\ttimestamp = re.sub('\\.[0-9]*','_',str(datetime.datetime.now())).replace(\" \", \"_\").replace(\"-\", \"\").replace(\":\",\"\")\n",
    "\tmodel_saved_dir = model_dir + '/' + timestamp + '/train_bert_squad_' + str(rd)\n",
    "\tend = datetime.datetime.now()\n",
    "\tacq_time.append(round(float((end-start).seconds), 3))\n",
    "\ttorch.save(model, model_saved_dir)\n",
    "\n",
    "# cal mean & standard deviation\n",
    "acc_m = []\n",
    "file_name_res_tot = DATA_NAME+ '_'  + STRATEGY_NAME + '_' + str(NUM_QUERY) + '_' + str(NUM_INIT_LB) +  '_' + str(args_input.quota) + '_normal_res_tot.txt'\n",
    "file_res_tot =  open(os.path.join(os.path.abspath('') + '/results', '%s' % file_name_res_tot),'w')\n",
    "\n",
    "file_res_tot.writelines('dataset: {}'.format(DATA_NAME) + '\\n')\n",
    "file_res_tot.writelines('AL strategy: {}'.format(STRATEGY_NAME) + '\\n')\n",
    "file_res_tot.writelines('number of labeled pool: {}'.format(NUM_INIT_LB) + '\\n')\n",
    "file_res_tot.writelines('number of unlabeled pool: {}'.format(len(train_dataset) - NUM_INIT_LB) + '\\n')\n",
    "file_res_tot.writelines('number of testing pool: {}'.format(len(val_dataset)) + '\\n')\n",
    "file_res_tot.writelines('batch size: {}'.format(NUM_QUERY) + '\\n')\n",
    "file_res_tot.writelines('quota: {}'.format(NUM_ROUND*NUM_QUERY)+ '\\n')\n",
    "file_res_tot.writelines('time of repeat experiments: {}'.format(args_input.iteration)+ '\\n')\n",
    "\n",
    "# result\n",
    "for i in range(len(all_acc)):\n",
    "\tacc_m.append(get_aubc(args_input.quota, NUM_QUERY, all_acc[i]))\n",
    "\tprint(str(i)+': '+str(acc_m[i]))\n",
    "\tfile_res_tot.writelines(str(i)+': '+str(acc_m[i])+'\\n')\n",
    "mean_acc, stddev_acc = get_mean_stddev(acc_m)\n",
    "mean_time, stddev_time = get_mean_stddev(acq_time)\n",
    "\n",
    "print('mean AUBC(acc): '+str(mean_acc)+'. std dev AUBC(acc): '+str(stddev_acc))\n",
    "print('mean time: '+str(mean_time)+'. std dev time: '+str(stddev_time))\n",
    "\n",
    "file_res_tot.writelines('mean acc: '+str(mean_acc)+'. std dev acc: '+str(stddev_acc)+'\\n')\n",
    "file_res_tot.writelines('mean time: '+str(mean_time)+'. std dev acc: '+str(stddev_time)+'\\n')\n",
    "\n",
    "# save result\n",
    "file_name_res = DATA_NAME+ '_'  + STRATEGY_NAME + '_' + str(NUM_QUERY) + '_' + str(NUM_INIT_LB) +  '_' + str(args_input.quota) + '_normal_res.txt'\n",
    "file_res =  open(os.path.join(os.path.abspath('') + '/results', '%s' % file_name_res),'w')\n",
    "\n",
    "\n",
    "file_res.writelines('dataset: {}'.format(DATA_NAME) + '\\n')\n",
    "file_res.writelines('AL strategy: {}'.format(STRATEGY_NAME) + '\\n')\n",
    "file_res.writelines('number of labeled pool: {}'.format(NUM_INIT_LB) + '\\n')\n",
    "# file_res.writelines('number of unlabeled pool: {}'.format(dataset.n_pool - NUM_INIT_LB) + '\\n')\n",
    "file_res.writelines('number of unlabeled pool: {}'.format(len(train_dataset) - NUM_INIT_LB) + '\\n')\n",
    "# file_res.writelines('number of testing pool: {}'.format(dataset.n_test) + '\\n')\n",
    "file_res.writelines('number of testing pool: {}'.format(len(val_dataset)) + '\\n')\n",
    "file_res.writelines('batch size: {}'.format(NUM_QUERY) + '\\n')\n",
    "file_res.writelines('quota: {}'.format(NUM_ROUND*NUM_QUERY)+ '\\n')\n",
    "file_res.writelines('time of repeat experiments: {}'.format(args_input.iteration)+ '\\n')\n",
    "avg_acc = np.mean(np.array(all_acc),axis=0)\n",
    "for i in range(len(avg_acc)):\n",
    "\ttmp = 'Size of training set is ' + str(NUM_INIT_LB + i*NUM_QUERY) + ', ' + 'accuracy is ' + str(round(avg_acc[i],4)) + '.' + '\\n'\n",
    "\tfile_res.writelines(tmp)\n",
    "\n",
    "file_res.close()\n",
    "file_res_tot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.34348008,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2974"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unlable data\n",
    "unlabeled_idxs = np.arange(n_pool)[~labeled_idxs]\n",
    "unlabeled_data = train_dataset.select(indices=unlabeled_idxs)\n",
    "len(unlabeled_idxs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test: query 5 data from 20 unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# smaller data\n",
    "unlabeled_idxs_20 = unlabeled_idxs[20:40]\n",
    "unlabeled_data_20 = train_dataset.select(unlabeled_idxs_20)\n",
    "unlabeled_feature_20 = train_features.select(unlabeled_idxs_20)\n",
    "len(unlabeled_data_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "       38, 39, 40])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_idxs_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data_20,\n",
    "\t\tshuffle=True,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=8,\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  6.23it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 3829.59it/s]\n"
     ]
    }
   ],
   "source": [
    "probs_list_dict_20 = get_prob(model, unlabeled_dataloader, device, unlabeled_feature_20, squad['train'])\n",
    "# len(probs_list_dict_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([0.07559638, 0.05358176, 0.04246078, 0.03847675, 0.02871645,\n",
       "        0.02713737, 0.04639477, 0.03605317, 0.03241921, 0.0239022 ,\n",
       "        0.02320026, 0.01954072, 0.01561525, 0.01165528, 0.01564988,\n",
       "        0.01211285, 0.01168001, 0.01103775, 0.05603519, 0.02389271,\n",
       "        0.01693485, 0.01342   , 0.01216083, 0.00907602, 0.00857694,\n",
       "        0.02903107, 0.01237849, 0.00877372, 0.00695272, 0.00630036,\n",
       "        0.00470216, 0.00410473, 0.00404521, 0.02572316, 0.01096804,\n",
       "        0.00777401, 0.00445864, 0.00363702, 0.00358428, 0.00549511,\n",
       "        0.00497951, 0.00385409, 0.00371637, 0.00351201, 0.01052027,\n",
       "        0.00541996, 0.00526079, 0.00470922, 0.01899265, 0.00809823,\n",
       "        0.00573992, 0.00329203, 0.00289546, 0.00268539, 0.00264645,\n",
       "        0.01777625, 0.00388845, 0.00308119, 0.00271002, 0.0025134 ,\n",
       "        0.00247696, 0.00647306, 0.00333487, 0.00323693, 0.00289756,\n",
       "        0.00447262, 0.00251217, 0.00227646, 0.001699  , 0.00160557,\n",
       "        0.00217191, 0.00168104, 0.00153183, 0.00420041, 0.00235928,\n",
       "        0.00213791, 0.00159559, 0.00150785, 0.00133585, 0.00099708,\n",
       "        0.00332062, 0.00258044, 0.00232035, 0.00171076, 0.00166052,\n",
       "        0.00148642, 0.00160865, 0.0013977 , 0.00110753, 0.00097412,\n",
       "        0.00299109, 0.00232436, 0.00209008, 0.00154098, 0.00149573,\n",
       "        0.00133891], dtype=float32),\n",
       " 1: array([0.07957832, 0.06291039, 0.05341204, 0.03785047, 0.01663861,\n",
       "        0.01610998, 0.00921169, 0.05250581, 0.02638493, 0.0090554 ,\n",
       "        0.00741554, 0.02542355, 0.00714535, 0.09643754, 0.00695926,\n",
       "        0.05216868, 0.04124179, 0.02481341, 0.0114553 , 0.01090767,\n",
       "        0.01056113, 0.00639936, 0.00603885, 0.04554957, 0.03600907,\n",
       "        0.03057234, 0.01536303, 0.00527265, 0.00431782, 0.02384724,\n",
       "        0.00566953, 0.00542851, 0.00427393, 0.03398684, 0.00593552,\n",
       "        0.00245261, 0.01431931, 0.00961095, 0.00482964, 0.00165755,\n",
       "        0.00135738, 0.01511441, 0.00263961, 0.00256329, 0.00109071,\n",
       "        0.00219091, 0.00212757, 0.00178839, 0.00171236, 0.00134816,\n",
       "        0.00296877, 0.00083438, 0.00479908, 0.0037939 , 0.00322109,\n",
       "        0.00161864, 0.00100341, 0.00055552, 0.00045492, 0.00469801,\n",
       "        0.003714  , 0.00315325, 0.00223455, 0.00098228, 0.00054382,\n",
       "        0.00044534, 0.0054538 , 0.00095246, 0.00092492, 0.00058609,\n",
       "        0.00039356, 0.00123908, 0.00229666, 0.0011541 , 0.00032436],\n",
       "       dtype=float32),\n",
       " 2: array([0.04477115, 0.03196248, 0.01718238, 0.01651896, 0.08371682,\n",
       "        0.046013  , 0.03636574, 0.02596179, 0.02462591, 0.01835912,\n",
       "        0.01395654, 0.01224261, 0.01218645, 0.08065247, 0.03503462,\n",
       "        0.02501149, 0.01344568, 0.01292652, 0.01179448, 0.01845455,\n",
       "        0.01787151, 0.01671967, 0.00655083, 0.00571581, 0.00508378,\n",
       "        0.01427485, 0.01382385, 0.00506716, 0.00442126, 0.01156125,\n",
       "        0.00426569, 0.02438264, 0.01059158, 0.00756141, 0.00717233,\n",
       "        0.00406486, 0.00390791, 0.00356568, 0.00914336, 0.00350302,\n",
       "        0.00891306, 0.00316388, 0.00276059, 0.02109974, 0.01159698,\n",
       "        0.00916552, 0.00654333, 0.00620664, 0.00351757, 0.00308559,\n",
       "        0.00307144, 0.01963218, 0.00852803, 0.00608823, 0.00327291,\n",
       "        0.00314654, 0.00816383, 0.00582822, 0.00301216, 0.00697609,\n",
       "        0.00675569, 0.00632028, 0.00247631, 0.00216066, 0.01609839,\n",
       "        0.0088481 , 0.00499234, 0.00473546, 0.00353038, 0.00268378,\n",
       "        0.0023542 , 0.0023434 , 0.00183193, 0.00202443, 0.00158878,\n",
       "        0.00138626, 0.01021782, 0.00443852, 0.00316869, 0.00300565,\n",
       "        0.00170343, 0.00163765, 0.00149424], dtype=float32),\n",
       " 3: array([0.03461884, 0.01923704, 0.01680327, 0.00959194, 0.0074167 ,\n",
       "        0.00677423, 0.12296313, 0.06665857, 0.03877525, 0.0245928 ,\n",
       "        0.02016761, 0.01169517, 0.00888463, 0.00816792, 0.00587908,\n",
       "        0.01145053, 0.00636285, 0.00317263, 0.00245315, 0.00224064,\n",
       "        0.04902287, 0.01545891, 0.00980465, 0.00510776, 0.00354212,\n",
       "        0.00234387, 0.04833075, 0.01524065, 0.00966622, 0.00503565,\n",
       "        0.00349211, 0.00321041, 0.00231078, 0.04651839, 0.01466914,\n",
       "        0.00930374, 0.00484681, 0.00336116, 0.00222413, 0.01374314,\n",
       "        0.00871644, 0.00454086, 0.00314899, 0.01089453, 0.00690974,\n",
       "        0.00359965, 0.03323245, 0.01801538, 0.01047954, 0.00664654,\n",
       "        0.00316078, 0.00240119, 0.00220749, 0.0015889 , 0.01556443,\n",
       "        0.00670459, 0.00470903, 0.00325427, 0.00273076, 0.00131196,\n",
       "        0.02857153, 0.00900977, 0.00571435, 0.00297691, 0.00206442,\n",
       "        0.00136605, 0.0023449 , 0.00436652, 0.00242639, 0.00149324,\n",
       "        0.00120984, 0.00099565, 0.00093548, 0.0031628 , 0.00175751,\n",
       "        0.00153516, 0.00087633, 0.00067759, 0.0006189 , 0.00263655,\n",
       "        0.00137352, 0.01061298, 0.0033467 , 0.00212261, 0.00110578,\n",
       "        0.00100941, 0.00076683, 0.00070497, 0.00050742, 0.0102141 ,\n",
       "        0.00553708, 0.00322092, 0.00204283, 0.00167525, 0.00097147,\n",
       "        0.00073801, 0.00067848, 0.00048835, 0.00958787, 0.0051976 ,\n",
       "        0.00302345, 0.00157254, 0.00108674, 0.00091191, 0.00069277,\n",
       "        0.00063688, 0.00045841, 0.00043812, 0.00292694, 0.00074729,\n",
       "        0.00062017, 0.00049827, 0.00292387, 0.00074651, 0.00061952,\n",
       "        0.00049775], dtype=float32),\n",
       " 4: array([0.21894261, 0.01358491, 0.02858025, 0.05455258, 0.02661796,\n",
       "        0.00905684, 0.04888514, 0.04601724, 0.03590285, 0.03363938,\n",
       "        0.01394102, 0.01053182, 0.00811593, 0.04060458, 0.03167986,\n",
       "        0.02968263, 0.01289108, 0.01230124, 0.01062952, 0.00929304,\n",
       "        0.00716131, 0.02554644, 0.02404773, 0.0175793 , 0.01246493,\n",
       "        0.00424123, 0.0624209 , 0.00736513, 0.00387308, 0.011998  ,\n",
       "        0.00546745, 0.00370081, 0.00254447, 0.00970574, 0.00913635,\n",
       "        0.00473575, 0.00161135, 0.02287051, 0.00269852, 0.00205793,\n",
       "        0.00141906, 0.00668041, 0.00521208, 0.00488349, 0.00212089,\n",
       "        0.00202384, 0.00174881, 0.00152892, 0.00250732, 0.00182608,\n",
       "        0.00182338, 0.00139053, 0.00123604, 0.00084983, 0.00072486,\n",
       "        0.00363776, 0.00342435, 0.00267169, 0.00250326, 0.00103741,\n",
       "        0.00060394, 0.00175312, 0.00328293, 0.00309034, 0.00225909,\n",
       "        0.00160185, 0.00093622, 0.00054503], dtype=float32),\n",
       " 5: array([0.07071762, 0.04019064, 0.03241076, 0.02572178, 0.02458021,\n",
       "        0.01951736, 0.076855  , 0.04581711, 0.04092685, 0.03012327,\n",
       "        0.02331215, 0.0203415 , 0.01542313, 0.01005681, 0.00961048,\n",
       "        0.00763098, 0.05548454, 0.0240067 , 0.01364363, 0.01100257,\n",
       "        0.00873184, 0.00834431, 0.00662561, 0.01286549, 0.03232608,\n",
       "        0.01398664, 0.00794897, 0.00641025, 0.00508729, 0.00465926,\n",
       "        0.00398521, 0.00386017, 0.01656772, 0.00649371, 0.00620984,\n",
       "        0.00438504, 0.00305643, 0.02202192, 0.00952831, 0.00541518,\n",
       "        0.0034967 , 0.00317409, 0.0027149 , 0.00474977, 0.00390255,\n",
       "        0.00309714, 0.00295968, 0.00235007, 0.01945256, 0.00478522,\n",
       "        0.00308873, 0.00280376, 0.00265049, 0.00239814, 0.00235524,\n",
       "        0.01677762, 0.00725923, 0.00412561, 0.00266399, 0.00241821,\n",
       "        0.00228601, 0.00206837, 0.00883343, 0.00526605, 0.00470398,\n",
       "        0.00346226, 0.00331091, 0.00233798, 0.00877983, 0.00344125,\n",
       "        0.00329082, 0.00232379, 0.00161971, 0.00803261, 0.00478864,\n",
       "        0.00427753, 0.00314838, 0.0024365 , 0.00212602, 0.00565906,\n",
       "        0.00337365, 0.00301357, 0.00221806, 0.0021211 , 0.0014978 ,\n",
       "        0.00187913, 0.0012253 , 0.00117092, 0.0033674 , 0.00154332,\n",
       "        0.00122481, 0.00117045, 0.00092937, 0.00334526, 0.00153318,\n",
       "        0.00121676, 0.00116275, 0.00092326, 0.00178471], dtype=float32),\n",
       " 6: array([0.05575795, 0.02782401, 0.00985837, 0.00849527, 0.07040148,\n",
       "        0.05554635, 0.05485442, 0.03853322, 0.01586197, 0.01286891,\n",
       "        0.00969861, 0.02559347, 0.00781425, 0.11064296, 0.05251043,\n",
       "        0.04143037, 0.0287408 , 0.0128248 , 0.01183098, 0.01090694,\n",
       "        0.00959855, 0.00723391, 0.03630471, 0.02864417, 0.02828736,\n",
       "        0.01411579, 0.00500139, 0.00430986, 0.05281204, 0.01101487,\n",
       "        0.0125129 , 0.005681  , 0.00344772, 0.00251331, 0.0202591 ,\n",
       "        0.00422539, 0.00416958, 0.00145478, 0.01129752, 0.00880264,\n",
       "        0.00439264, 0.00155636, 0.00134117, 0.00294555, 0.00089934,\n",
       "        0.0024597 , 0.00242721, 0.00206685, 0.00125434, 0.00091439,\n",
       "        0.00084686, 0.00450173, 0.00355184, 0.00350759, 0.00246396,\n",
       "        0.00101427, 0.00062016, 0.00053442, 0.0042072 , 0.00331945,\n",
       "        0.0032781 , 0.00163582, 0.00094791, 0.00057959, 0.00049945,\n",
       "        0.00653607, 0.00136321, 0.00134521, 0.00069518, 0.00046935,\n",
       "        0.0025088 , 0.00125193, 0.00038224], dtype=float32),\n",
       " 7: array([0.05596411, 0.02706637, 0.00861679, 0.00756097, 0.0270217 ,\n",
       "        0.07161313, 0.05790715, 0.05168574, 0.03275238, 0.01814681,\n",
       "        0.01177297, 0.00795805, 0.00698294, 0.09389538, 0.05663779,\n",
       "        0.04579793, 0.02590338, 0.01435205, 0.01410918, 0.01355747,\n",
       "        0.00931108, 0.00629391, 0.04462972, 0.03608807, 0.03221088,\n",
       "        0.01557841, 0.00495951, 0.00435181, 0.04679845, 0.00926277,\n",
       "        0.01624914, 0.00751551, 0.00350334, 0.00276272, 0.01263347,\n",
       "        0.00911803, 0.00440982, 0.0014039 , 0.00123188, 0.00342193,\n",
       "        0.00095591, 0.00285758, 0.00272735, 0.00259115, 0.00133206,\n",
       "        0.00105046, 0.0120417 , 0.00250869, 0.0023834 , 0.00393239,\n",
       "        0.00217878, 0.00214191, 0.00205816, 0.00141351, 0.00081322,\n",
       "        0.00491524, 0.00397452, 0.00354751, 0.00171571, 0.00124553,\n",
       "        0.00054621, 0.00047928, 0.00467784, 0.00378255, 0.00337616,\n",
       "        0.00213942, 0.00118537, 0.00051983, 0.00045613, 0.00128075,\n",
       "        0.00255445, 0.00123543, 0.00034512], dtype=float32),\n",
       " 8: array([0.06047536, 0.03346101, 0.02916761, 0.02586189, 0.0203026 ,\n",
       "        0.01823216, 0.06808797, 0.04442161, 0.03948521, 0.03764965,\n",
       "        0.02331047, 0.02175004, 0.01095594, 0.01001314, 0.00815631,\n",
       "        0.00786071, 0.00705908, 0.05821681, 0.02297683, 0.01271308,\n",
       "        0.01108186, 0.0098259 , 0.00771371, 0.00692707, 0.04210128,\n",
       "        0.00823822, 0.00718485, 0.00699726, 0.00682805, 0.00664311,\n",
       "        0.00585629, 0.02826862, 0.01115697, 0.00617315, 0.00538107,\n",
       "        0.00477121, 0.00458465, 0.00393217, 0.00336361, 0.02511673,\n",
       "        0.00991299, 0.00548486, 0.00417442, 0.00407347, 0.00349374,\n",
       "        0.01317754, 0.00764185, 0.00459983, 0.00451144, 0.00401167,\n",
       "        0.02081306, 0.00821443, 0.00454505, 0.00345914, 0.0033755 ,\n",
       "        0.00328407, 0.0028951 , 0.01073288, 0.00622415, 0.00374648,\n",
       "        0.00367449, 0.00326744, 0.00363443, 0.00322252, 0.00262494,\n",
       "        0.00252981, 0.00227182, 0.0077815 , 0.00507677, 0.00451261,\n",
       "        0.00430283, 0.00271626, 0.00266406, 0.00473806, 0.00309118,\n",
       "        0.00274767, 0.00261994, 0.00165389, 0.00162211, 0.00444045,\n",
       "        0.00289702, 0.00257508, 0.00245538, 0.00152023, 0.00141846,\n",
       "        0.00128979, 0.00105062, 0.00101254, 0.00296595, 0.0014305 ,\n",
       "        0.00126837, 0.00099572, 0.00089418, 0.00101824, 0.0028818 ,\n",
       "        0.00138991, 0.00123238, 0.00096747, 0.00086881], dtype=float32),\n",
       " 9: array([0.06947887, 0.05783695, 0.03539664, 0.02183002, 0.01737732,\n",
       "        0.03537413, 0.0294468 , 0.01111443, 0.0088474 , 0.00832708,\n",
       "        0.07062593, 0.03509871, 0.03315109, 0.03095631, 0.01918646,\n",
       "        0.00858387, 0.0079    , 0.03620516, 0.01799274, 0.01586921,\n",
       "        0.00596115, 0.00440037, 0.03410853, 0.01695079, 0.01495022,\n",
       "        0.00561594, 0.00414555, 0.01686457, 0.01487419, 0.00558738,\n",
       "        0.00412446, 0.00307215, 0.02612155, 0.01298153, 0.01144942,\n",
       "        0.00430089, 0.00317481, 0.01147516, 0.01080962, 0.00625615,\n",
       "        0.00584613, 0.00257596, 0.0097242 , 0.00809481, 0.00439751,\n",
       "        0.00350012, 0.00305531, 0.00243211, 0.00228908, 0.00177935,\n",
       "        0.00786193, 0.00693405, 0.00260473, 0.00143218, 0.00630699,\n",
       "        0.00236917, 0.00130266, 0.00227029, 0.00124829, 0.01260654,\n",
       "        0.00626502, 0.00591738, 0.00552562, 0.0015322 , 0.00141013,\n",
       "        0.00593667, 0.00494191, 0.00302449, 0.00186528, 0.00148482,\n",
       "        0.00528684, 0.00440098, 0.00239083, 0.00190294, 0.00166111,\n",
       "        0.00132229, 0.00124452, 0.0009674 , 0.00982689, 0.00488363,\n",
       "        0.00430726, 0.00161799, 0.00119436, 0.00187887, 0.00149545,\n",
       "        0.00097803, 0.00095489, 0.00094452, 0.00076024, 0.00345379,\n",
       "        0.00287507, 0.00188297, 0.00175957, 0.00338486, 0.00281769,\n",
       "        0.00172445, 0.00106351, 0.00084659, 0.00653659, 0.00324846,\n",
       "        0.00306821, 0.00177575, 0.00165937, 0.00079446, 0.00073116],\n",
       "       dtype=float32),\n",
       " 10: array([0.07373021, 0.03991822, 0.03371056, 0.02975663, 0.02440435,\n",
       "        0.02001812, 0.02007063, 0.04760325, 0.0177577 , 0.01750259,\n",
       "        0.0111677 , 0.01066783, 0.01666105, 0.01522655, 0.01470686,\n",
       "        0.00989372, 0.12270971, 0.03452912, 0.0186944 , 0.01578725,\n",
       "        0.01393554, 0.01142899, 0.00937483, 0.00836762, 0.02090812,\n",
       "        0.00779947, 0.00490504, 0.00468549, 0.04622097, 0.01300606,\n",
       "        0.0070416 , 0.00594657, 0.00524909, 0.00430495, 0.00333211,\n",
       "        0.00327639, 0.00315183, 0.03327974, 0.00936455, 0.00507006,\n",
       "        0.00333561, 0.00239916, 0.00235905, 0.00226936, 0.02866943,\n",
       "        0.00806725, 0.00436769, 0.00294363, 0.00287352, 0.0020668 ,\n",
       "        0.00203224, 0.00195498, 0.02662334, 0.00353474, 0.00305473,\n",
       "        0.00273355, 0.00266844, 0.0019193 , 0.0018872 , 0.00314644,\n",
       "        0.00287553, 0.00277739, 0.00227783, 0.00186843, 0.00693709,\n",
       "        0.00261507, 0.0025506 , 0.00225995, 0.00648493, 0.00244462,\n",
       "        0.00238436, 0.00211265, 0.00433823, 0.00198351, 0.00175086,\n",
       "        0.00143594, 0.00117785, 0.00159369, 0.00172514, 0.0015766 ,\n",
       "        0.00102443, 0.00323786, 0.0014804 , 0.00130676, 0.00107172,\n",
       "        0.00087909, 0.00137577, 0.00134185, 0.00118894, 0.00106393,\n",
       "        0.00103859, 0.00352295, 0.00131419, 0.00129531, 0.00082648,\n",
       "        0.00078949], dtype=float32),\n",
       " 11: array([0.09031783, 0.08678196, 0.05149749, 0.05005974, 0.02335066,\n",
       "        0.04476821, 0.03085563, 0.02749844, 0.02023231, 0.01967892,\n",
       "        0.01475122, 0.0424626 , 0.01142553, 0.01110485, 0.00899895,\n",
       "        0.04690935, 0.02659636, 0.02555513, 0.01516472, 0.01474134,\n",
       "        0.00687619, 0.02895702, 0.01641787, 0.01577512, 0.00936115,\n",
       "        0.0090998 , 0.00444273, 0.00424391, 0.0132527 , 0.00786431,\n",
       "        0.00356594, 0.00346586, 0.01990345, 0.01128473, 0.00625469,\n",
       "        0.00305369, 0.00291702, 0.00291601, 0.00953833, 0.0043107 ,\n",
       "        0.0041928 , 0.00364416, 0.00229336, 0.01565585, 0.00887646,\n",
       "        0.00491988, 0.002402  , 0.0022945 , 0.0022937 , 0.00194723,\n",
       "        0.0129493 , 0.00260243, 0.00198675, 0.00189783, 0.00189717,\n",
       "        0.00163777, 0.00161059, 0.00610593, 0.00275948, 0.00268401,\n",
       "        0.0023328 , 0.00146808, 0.00584507, 0.00561624, 0.00333274,\n",
       "        0.00151118, 0.00502857, 0.00483171, 0.0028672 , 0.00130008,\n",
       "        0.00385283, 0.00265549, 0.00236657, 0.00174123, 0.0016936 ,\n",
       "        0.00147199, 0.00357131, 0.00096094, 0.00093397, 0.0008994 ,\n",
       "        0.001375  , 0.00116239, 0.00084738, 0.00073152, 0.00071938,\n",
       "        0.00278992, 0.0019229 , 0.00171368, 0.00126086, 0.00122637,\n",
       "        0.0010659 ], dtype=float32),\n",
       " 12: array([0.21155253, 0.03974297, 0.0225664 , 0.01700682, 0.02392644,\n",
       "        0.01850155, 0.01215466, 0.01113404, 0.00867026, 0.06007932,\n",
       "        0.03999437, 0.01277161, 0.01249312, 0.01068855, 0.00788114,\n",
       "        0.00659534, 0.0590706 , 0.03932287, 0.0110972 , 0.00774881,\n",
       "        0.00648461, 0.04543748, 0.0302474 , 0.00853604, 0.00596044,\n",
       "        0.004988  , 0.00509022, 0.00412597, 0.00361144, 0.00310948,\n",
       "        0.00921824, 0.00468288, 0.00656348, 0.00333425, 0.00305428,\n",
       "        0.01676032, 0.00535216, 0.00523546, 0.00447922, 0.00330272,\n",
       "        0.00220556, 0.02513073, 0.01672934, 0.00534226, 0.00329662,\n",
       "        0.00275878, 0.00302947, 0.0028826 , 0.00233655, 0.00204517,\n",
       "        0.0017609 , 0.00452368, 0.00442504, 0.00378586, 0.00208979,\n",
       "        0.00186415, 0.01852288, 0.01233055, 0.00347977, 0.00203339,\n",
       "        0.01668361, 0.01110616, 0.00354659, 0.00218854, 0.00183148,\n",
       "        0.00325721, 0.00172382, 0.00153827, 0.00137218, 0.00252349,\n",
       "        0.00143286, 0.00107985, 0.00332114, 0.00256813, 0.00168714,\n",
       "        0.00154547, 0.00256109, 0.00250524, 0.00214337, 0.00118314,\n",
       "        0.00105539], dtype=float32),\n",
       " 13: array([0.11368471, 0.01701447, 0.01531817, 0.06780099, 0.06682295,\n",
       "        0.01479023, 0.01418986, 0.01231256, 0.02525213, 0.01695552,\n",
       "        0.0101856 , 0.00890552, 0.00740971, 0.00668547, 0.04292687,\n",
       "        0.04230769, 0.01877578, 0.01173263, 0.01152035, 0.00936415,\n",
       "        0.00898404, 0.00779546, 0.03878275, 0.0382233 , 0.00846014,\n",
       "        0.00704289, 0.03613492, 0.03561367, 0.01580504, 0.00788253,\n",
       "        0.00756257, 0.00656205, 0.00631675, 0.00500821, 0.0045089 ,\n",
       "        0.00424067, 0.01661134, 0.0081725 , 0.00802463, 0.00391011,\n",
       "        0.01026585, 0.00362039, 0.00271787, 0.00862963, 0.00348081,\n",
       "        0.00304336, 0.00228468, 0.01632504, 0.01608956, 0.00714041,\n",
       "        0.00356117, 0.00341662, 0.00296461, 0.01458758, 0.00647384,\n",
       "        0.00404538, 0.00397219, 0.00309767, 0.00268785, 0.00646338,\n",
       "        0.00403884, 0.00396577, 0.00193237, 0.00215901, 0.00260784,\n",
       "        0.00206762, 0.00186148, 0.00175074, 0.00279591, 0.00274532,\n",
       "        0.00133769, 0.00886905, 0.00874112, 0.00193471, 0.0036097 ,\n",
       "        0.00242373, 0.00145599, 0.00127301, 0.00095566, 0.00355034,\n",
       "        0.00221854, 0.0021784 , 0.00106145], dtype=float32),\n",
       " 14: array([0.10659172, 0.05705122, 0.0334532 , 0.03202035, 0.01464465,\n",
       "        0.04622867, 0.02474303, 0.01388717, 0.00836664, 0.00635136,\n",
       "        0.01004078, 0.00917916, 0.00800545, 0.00728616, 0.00663186,\n",
       "        0.06433627, 0.03416266, 0.03016626, 0.01931364, 0.01530679,\n",
       "        0.00731405, 0.00671996, 0.03545509, 0.01662433, 0.01064356,\n",
       "        0.00556579, 0.0040307 , 0.03154091, 0.01478903, 0.00946853,\n",
       "        0.00495134, 0.00358572, 0.01884268, 0.01524625, 0.00683118,\n",
       "        0.00591367, 0.00299901, 0.01308109, 0.00837504, 0.00437952,\n",
       "        0.00317162, 0.0021293 , 0.02138939, 0.01002915, 0.00642106,\n",
       "        0.00335774, 0.00243165, 0.01319053, 0.00705998, 0.00396246,\n",
       "        0.00291809, 0.00266768, 0.00238727, 0.00181225, 0.00805592,\n",
       "        0.00515772, 0.00269711, 0.00131132, 0.01527415, 0.00811059,\n",
       "        0.0071618 , 0.00458527, 0.00173644, 0.00159539, 0.0022367 ,\n",
       "        0.00108747, 0.00852283, 0.00456169, 0.00267484, 0.00256027,\n",
       "        0.00117095, 0.00711322, 0.00380722, 0.00223244, 0.00213683,\n",
       "        0.00097729, 0.01015203, 0.00476012, 0.00304762, 0.00159368,\n",
       "        0.00115413, 0.00278681, 0.00145729, 0.00070853, 0.00543774,\n",
       "        0.00291045, 0.00163351, 0.00120297, 0.00109974, 0.00098414,\n",
       "        0.00074709, 0.00110302, 0.00100836, 0.00087943, 0.00080041,\n",
       "        0.00072853, 0.00096741, 0.0008844 , 0.00079143, 0.00070201,\n",
       "        0.00063897], dtype=float32),\n",
       " 15: array([0.05051869, 0.04148417, 0.01989158, 0.01782671, 0.07042429,\n",
       "        0.03478666, 0.02856559, 0.01369714, 0.0122753 , 0.01186786,\n",
       "        0.06726357, 0.03624359, 0.0332254 , 0.02728354, 0.02182275,\n",
       "        0.01452501, 0.0130824 , 0.01133522, 0.02461849, 0.02394044,\n",
       "        0.0204131 , 0.00740471, 0.00535613, 0.01943223, 0.01889702,\n",
       "        0.00584479, 0.00422778, 0.01390137, 0.00490543, 0.00303163,\n",
       "        0.02260806, 0.01116744, 0.00917031, 0.00733488, 0.00439715,\n",
       "        0.0039407 , 0.0038099 , 0.01033764, 0.01176974, 0.00354008,\n",
       "        0.00256069, 0.00932012, 0.00765336, 0.00328882, 0.00230911,\n",
       "        0.01788289, 0.00963582, 0.0088334 , 0.00725368, 0.00580186,\n",
       "        0.00347812, 0.00301361, 0.01604189, 0.00792403, 0.00650693,\n",
       "        0.00312006, 0.00279618, 0.0158685 , 0.00855042, 0.0064366 ,\n",
       "        0.00514832, 0.00342667, 0.00308634, 0.00267415, 0.00202568,\n",
       "        0.00896462, 0.00871772, 0.00743326, 0.00269636, 0.00195039,\n",
       "        0.00211247, 0.00152804, 0.00950915, 0.00469712, 0.00385711,\n",
       "        0.00308511, 0.00184948, 0.00165749, 0.00160248, 0.00500065,\n",
       "        0.00200406, 0.00134384, 0.00118471], dtype=float32),\n",
       " 16: array([0.09187537, 0.01422764, 0.01357935, 0.0814696 , 0.07494681,\n",
       "        0.04791311, 0.03167827, 0.03099898, 0.01049453, 0.02972093,\n",
       "        0.01825065, 0.01028804, 0.00947619, 0.00942745, 0.00823185,\n",
       "        0.00759724, 0.00809322, 0.0067908 , 0.00648137, 0.00646378,\n",
       "        0.00638138, 0.02578057, 0.01648138, 0.01089684, 0.01066318,\n",
       "        0.00360996, 0.00340496, 0.02640364, 0.02428966, 0.01026667,\n",
       "        0.01004652, 0.00340119, 0.01209022, 0.00385483, 0.00334865,\n",
       "        0.00309049, 0.02600119, 0.02391942, 0.00989339, 0.00334935,\n",
       "        0.02302872, 0.02118494, 0.00296645, 0.01207051, 0.00798054,\n",
       "        0.00780942, 0.00280863, 0.0024937 , 0.01895093, 0.01743364,\n",
       "        0.00244117, 0.00834102, 0.00288728, 0.00265944, 0.00231022,\n",
       "        0.00213212, 0.00261407, 0.00227081, 0.00257465, 0.00216032,\n",
       "        0.00206188, 0.00205629, 0.00203007, 0.01152253, 0.01059998,\n",
       "        0.00438429, 0.00148428, 0.00595833, 0.00393941, 0.00385494,\n",
       "        0.00138641, 0.00123096, 0.00401848, 0.00246762, 0.00139101,\n",
       "        0.00128125, 0.001113  , 0.0010272 , 0.00848277, 0.0078036 ,\n",
       "        0.00749666, 0.00479258, 0.00316866, 0.00310072, 0.00104973],\n",
       "       dtype=float32),\n",
       " 17: array([0.10519054, 0.01883976, 0.01469859, 0.05830234, 0.04250211,\n",
       "        0.02849044, 0.02499559, 0.02256063, 0.0183842 , 0.01476781,\n",
       "        0.01409623, 0.01349787, 0.03225469, 0.01576182, 0.01382835,\n",
       "        0.01248126, 0.00817002, 0.00746745, 0.02633275, 0.0101897 ,\n",
       "        0.00667   , 0.00609643, 0.03743589, 0.02992457, 0.00637395,\n",
       "        0.03687763, 0.02947834, 0.01177072, 0.00794646, 0.0065654 ,\n",
       "        0.00627891, 0.00660417, 0.00648916, 0.0062328 , 0.00486277,\n",
       "        0.02541124, 0.02031262, 0.00811084, 0.0043266 , 0.02328653,\n",
       "        0.01861422, 0.00396484, 0.00303193, 0.00277121, 0.01544256,\n",
       "        0.00616622, 0.00416284, 0.00343936, 0.00328927, 0.00584005,\n",
       "        0.00394264, 0.00325743, 0.00286071, 0.00345051, 0.00339042,\n",
       "        0.00325647, 0.00259963, 0.00254067, 0.00734595, 0.00535516,\n",
       "        0.00358973, 0.00314938, 0.00284258, 0.00186071, 0.00177609,\n",
       "        0.0017007 , 0.01199086, 0.00958496, 0.00382728, 0.0020416 ,\n",
       "        0.00620155, 0.0045209 , 0.00303049, 0.00265875, 0.00239975,\n",
       "        0.00157083, 0.00143575, 0.00558482, 0.00272912, 0.0021611 ,\n",
       "        0.00141462, 0.00129297, 0.00293192, 0.00197935, 0.00163535,\n",
       "        0.00143618, 0.00892735, 0.00713613], dtype=float32),\n",
       " 18: array([0.06598029, 0.05646468, 0.03575381, 0.02051426, 0.01681645,\n",
       "        0.07413441, 0.03982518, 0.03833558, 0.034045  , 0.02079226,\n",
       "        0.00941428, 0.00892882, 0.00612852, 0.02257524, 0.01931945,\n",
       "        0.00701898, 0.00575377, 0.03692253, 0.01909298, 0.01695607,\n",
       "        0.00650308, 0.00468877, 0.0030523 , 0.03366547, 0.01740873,\n",
       "        0.01546031, 0.00592942, 0.00427516, 0.01694999, 0.01505292,\n",
       "        0.00577317, 0.0041625 , 0.00310387, 0.02632247, 0.01361159,\n",
       "        0.01208816, 0.00463611, 0.00334268, 0.00217602, 0.01196966,\n",
       "        0.01100303, 0.00624922, 0.00596239, 0.0026836 , 0.0085549 ,\n",
       "        0.00732112, 0.00445166, 0.0035824 , 0.00265985, 0.0021804 ,\n",
       "        0.00183222, 0.00830577, 0.00737617, 0.00282895, 0.00152095,\n",
       "        0.00724817, 0.00277985, 0.00149455, 0.00245598, 0.00132043,\n",
       "        0.01374776, 0.00738533, 0.00710909, 0.00631343, 0.00174582,\n",
       "        0.00165579, 0.0011365 , 0.0058618 , 0.00501641, 0.00317643,\n",
       "        0.00182252, 0.001494  , 0.00485129, 0.00415164, 0.00252444,\n",
       "        0.0020315 , 0.00150834, 0.00123645, 0.00103901, 0.00979213,\n",
       "        0.0050636 , 0.00449688, 0.00172466, 0.0012435 , 0.00080949,\n",
       "        0.00210551, 0.00169438, 0.00104265, 0.00095775, 0.00086659,\n",
       "        0.0036653 , 0.00313669, 0.00208172, 0.00198617, 0.00327889,\n",
       "        0.00280601, 0.00177679, 0.00101946, 0.00083569, 0.0064969 ,\n",
       "        0.00349015, 0.00335961, 0.00182217, 0.00173853, 0.00082504,\n",
       "        0.00078249, 0.00053708], dtype=float32),\n",
       " 19: array([0.06334993, 0.04044314, 0.0235226 , 0.01852757, 0.00947383,\n",
       "        0.09331273, 0.04126878, 0.0339897 , 0.01592376, 0.01553596,\n",
       "        0.00713703, 0.00638296, 0.0059717 , 0.02604815, 0.01662936,\n",
       "        0.00761814, 0.00513756, 0.00389544, 0.00609249, 0.00574798,\n",
       "        0.00427471, 0.0041192 , 0.06495027, 0.02365851, 0.01081379,\n",
       "        0.0058381 , 0.00444285, 0.06183264, 0.02252289, 0.01029473,\n",
       "        0.00555787, 0.00472927, 0.00422959, 0.04832438, 0.01760243,\n",
       "        0.00804569, 0.00434367, 0.00369609, 0.00330558, 0.01099505,\n",
       "        0.0050256 , 0.0027132 , 0.00206477, 0.02870068, 0.01045439,\n",
       "        0.00477848, 0.00257978, 0.00219517, 0.00196324, 0.01218627,\n",
       "        0.00916259, 0.00470213, 0.00340218, 0.00176339, 0.00406471,\n",
       "        0.00219444, 0.00788335, 0.00360331, 0.00194534, 0.02151009,\n",
       "        0.00951312, 0.00783517, 0.00358129, 0.0016452 , 0.00147137,\n",
       "        0.00137657, 0.00659966, 0.00421328, 0.00193016, 0.00181989,\n",
       "        0.00171699, 0.00130167, 0.00098696, 0.00446819, 0.00285253,\n",
       "        0.00165909, 0.00130678, 0.00066821, 0.00119801, 0.00392113,\n",
       "        0.00250329, 0.00145596, 0.00114679, 0.0005864 , 0.00883842,\n",
       "        0.00390891, 0.00321945, 0.00150827, 0.00147154, 0.00067601,\n",
       "        0.00060458, 0.00056563, 0.00281768, 0.00179883, 0.00082407,\n",
       "        0.00077699, 0.00073306, 0.00055574, 0.00042138, 0.00075375,\n",
       "        0.00071113, 0.00053912, 0.00052886, 0.00050962], dtype=float32)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_list_dict_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort_probs:\n",
      " [0.21894264 0.06242085 0.05455256 0.04888514 0.04601725 0.04060458\n",
      " 0.03590286 0.03363937 0.03167988 0.02968265 0.02858026 0.02661794\n",
      " 0.02554646 0.02404774 0.02287048 0.01757931 0.01394101 0.01358491\n",
      " 0.01289108 0.01246493 0.01230124 0.01199799 0.01062952 0.01053182\n",
      " 0.00970575 0.00929304 0.00913635 0.00905683 0.00811593 0.00736513\n",
      " 0.00716131 0.00668042 0.00546745 0.00521209 0.0048835  0.00473575\n",
      " 0.00424123 0.00387308 0.00370081 0.00363776 0.00342435 0.00328293\n",
      " 0.00309034 0.00269852 0.00267169 0.00254447 0.00250732 0.00250326\n",
      " 0.00225909 0.00212089 0.00205793 0.00202384 0.00182608 0.00182338\n",
      " 0.00175312 0.00174881 0.00161135 0.00160185 0.00152893 0.00141906\n",
      " 0.00139053 0.00123604 0.00103741 0.00093622 0.00084983 0.00072486\n",
      " 0.00060394 0.00054503]\n",
      "uncertainties_dict:\n",
      " {0: 0.019561194, 1: 0.0061373785, 2: 0.003160715, 3: 0.045883723, 4: 0.028362542, 5: 0.04024148, 6: 0.016859218, 7: 0.0030643567, 8: 0.046888124, 9: 0.008154124, 10: 0.042255454, 11: 0.02228225, 12: 0.0035358667, 13: 0.05630456, 14: 0.0489795, 15: 0.0011470616, 16: 0.010405771, 17: 0.007612668, 18: 0.1514731, 19: 0.1565218}\n",
      "sorted_uncertainties_list:\n",
      " [(19, 0.1565218), (18, 0.1514731), (13, 0.05630456), (14, 0.0489795), (8, 0.046888124), (3, 0.045883723), (10, 0.042255454), (5, 0.04024148), (4, 0.028362542), (11, 0.02228225), (0, 0.019561194), (6, 0.016859218), (16, 0.010405771), (9, 0.008154124), (17, 0.007612668), (1, 0.0061373785), (12, 0.0035358667), (2, 0.003160715), (7, 0.0030643567), (15, 0.0011470616)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([19, 18, 13, 14,  8])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertainties_dict = {}\n",
    "for idx, probs in probs_list_dict_20.items():\n",
    "    if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "        sort_probs = np.sort(probs)[::-1] # This method returns a copy of the array, leaving the original array unchanged.\n",
    "        uncertainties = sort_probs[0] - sort_probs[1]\n",
    "        uncertainties_dict[idx] = uncertainties\n",
    "    elif idx:\n",
    "        uncertainties_dict[idx] = np.array([0])\n",
    "print('sort_probs:\\n', sort_probs)\n",
    "print('uncertainties_dict:\\n', uncertainties_dict)\n",
    "# deepAL+: return unlabeled_idxs[uncertainties.sort()[1][:n]] \n",
    "sorted_uncertainties_list = sorted(uncertainties_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "unlabeled_idxs[[idx for (idx, uncertainties) in sorted_uncertainties_list[:5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DataLoader\n",
    "# import default_data_collator\n",
    "import itertools\n",
    "def margin_sampling_query(n_pool, labeled_idxs, train_dataset, train_features, examples, model, device, n):\n",
    "    # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    \n",
    "    unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data,\n",
    "\t\tshuffle=True,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=8,\n",
    "\t)\n",
    "\n",
    "    # deepAL+: probs = self.predict_prob(unlabeled_data)\n",
    "    prob_dict = get_prob(model, unlabeled_dataloader, device, unlabeled_features, examples)\n",
    "    \n",
    "    # deepAL+: probs_sorted, _ = probs.sort(descending=True)\n",
    "    # deepAL+: uncertainties = probs_sorted[:, 0] - probs_sorted[:,1]\n",
    "    uncertainties_dict = {}\n",
    "    for idx, probs in prob_dict.items():\n",
    "        if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "            sort_probs = np.sort(probs)[::-1] # This method returns a copy of the array, leaving the original array unchanged.\n",
    "            uncertainties = sort_probs[0] - sort_probs[1]\n",
    "            uncertainties_dict[idx] = uncertainties\n",
    "        elif idx:\n",
    "            uncertainties_dict[idx] = np.array([0])\n",
    "    print('sort_probs:\\n', sort_probs)\n",
    "    # deepAL+: return unlabeled_idxs[uncertainties.sort()[1][:n]] \n",
    "    sorted_uncertainties_dict = sorted(uncertainties_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_uncertainties_dict = dict(itertools.islice(sorted_uncertainties_dict.items(), n))\n",
    "    \n",
    "    return unlabeled_idxs[list(sorted_uncertainties_dict.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_dropout(model, eval_dataloader, device, features, examples, n_drop=10):\n",
    "    model.train()\n",
    "    \n",
    "    prob_dict = {}\n",
    "    for_check = []\n",
    "    \n",
    "    for i in range(n_drop):\n",
    "        \n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        # accelerator.print(\"Evaluation!\")\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "            end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "        start_logits = np.concatenate(start_logits)\n",
    "        end_logits = np.concatenate(end_logits)\n",
    "        start_logits = start_logits[: len(val_dataset)]\n",
    "        end_logits = end_logits[: len(val_dataset)]\n",
    "\n",
    "        example_to_features = collections.defaultdict(list)\n",
    "        max_answer_length = 30\n",
    "        n_best = 20\n",
    "            \n",
    "        for idx, feature in enumerate(features):\n",
    "            example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "        n = 0\n",
    "        for example in tqdm(examples):\n",
    "            example_id = example[\"id\"]\n",
    "            # context = example[\"context\"]\n",
    "            answers = []\n",
    "\n",
    "            # Loop through all features associated with that example\n",
    "            for feature_index in example_to_features[example_id]:\n",
    "                start_logit = start_logits[feature_index]\n",
    "                end_logit = end_logits[feature_index]\n",
    "                offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # Skip answers that are not fully in the context\n",
    "                        if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                            continue\n",
    "                        # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                        if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                        ):\n",
    "                            continue\n",
    "\n",
    "                        answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "\n",
    "            \n",
    "            if 1 < len(answers) < 150:\n",
    "                zero_list = [0] * (150 - len(answers))\n",
    "                answers.extend(zero_list)\n",
    "            else:\n",
    "                answers[:150]\n",
    "\n",
    "            if len(answers) > 1:\n",
    "                if example_to_features[example_id][0] not in prob_dict:\n",
    "                    prob_dict[example_to_features[example_id][0]] = softmax(answers)\n",
    "                else:\n",
    "                    prob_dict[example_to_features[example_id][0]] += softmax(answers)\n",
    "            elif example_to_features[example_id] != []:\n",
    "                if example_to_features[example_id][0] not in prob_dict:\n",
    "                    prob_dict[example_to_features[example_id][0]] = np.array([0])\n",
    "            # if n == 0 and len(softmax(answers)) > 1:\n",
    "            #     for_check.append(answers[:5])\n",
    "            #     n += 1      \n",
    "\n",
    "    for key in prob_dict.keys():\n",
    "        prob_dict[key] /= n_drop\n",
    "    # return prob_dict, for_check\n",
    "    return prob_dict\n",
    "# move to evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_list_dict_20_dropout = get_prob_dropout(model, unlabeled_dataloader, device, unlabeled_data_20, squad['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_check\n",
    "# the prediction are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_list_dict_20_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_list_dict_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_list_dict = []\n",
    "for d in probs_list_dict_20:\n",
    "    if len(d['probs']) > 1: # if prob_dict['probs'] is not 0\n",
    "        confidence = max(d['probs'])\n",
    "        confidence_list_dict.append(\n",
    "            {'idx': d['idx'], \n",
    "                'confidence': confidence}\n",
    "                )\n",
    "    elif d['idx']:\n",
    "        confidence_list_dict.append(\n",
    "            {'idx': d['idx'], \n",
    "                'confidence': np.array([0])}\n",
    "                )\n",
    "# deepAL+: return unlabeled_idxs[uncertainties.sort()[1][:n]]\n",
    "sorted_confidence_dict = sorted(confidence_list_dict, key=lambda d: d['confidence'])   \n",
    "unlabeled_idxs_20[[confidence_dict['idx'][0] for confidence_dict in sorted_confidence_dict[:5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(prob_list_dict, NUM_QUERY):\n",
    "    entropy_list_dict = []\n",
    "    for d in prob_list_dict:\n",
    "        if len(d['probs']) > 1: # if prob_dict['probs'] is not 0\n",
    "            log_probs = np.log(d['probs'])\n",
    "            entropy_list_dict.append(\n",
    "                {'idx': d['idx'], \n",
    "                'entropy': (d['probs']*log_probs).sum()}\n",
    "                )\n",
    "        elif d['idx']:\n",
    "            entropy_list_dict.append(\n",
    "                {'idx': d['idx'], \n",
    "                'entropy': np.array([0])}\n",
    "                )\n",
    "    # deepAL+: return unlabeled_idxs[uncertainties.sort()[1][:n]] # use smallest Entropy\n",
    "    sorted_entropy_dict = sorted(entropy_list_dict, key=lambda d: d['entropy'], reverse=True) # use largest Entropy, different from deepAL+\n",
    "    return unlabeled_idxs[[entropy_dict['idx'][0] for entropy_dict in sorted_entropy_dict[:NUM_QUERY]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_idxs_20 = get_entropy(probs_list_dict_20, 5)\n",
    "q_idxs_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, _, _ = trainer_qs.predict(unlabeled_data_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits, end_logits = preds\n",
    "start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with unlable data\n",
    "preds, _, _ = trainer_qs.predict(unlabeled_data_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits, end_logits = preds\n",
    "start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_idxs = get_entropy(probs_list_dict, NUM_QUERY)\n",
    "q_idxs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alqa",
   "language": "python",
   "name": "alqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6317ac8a4c65d05b4cf6bac76f72bfaae40b2e9380067c26c20c9afff1d8528e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
