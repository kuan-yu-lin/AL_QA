{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Model \n",
    "## no trainer\n",
    "\n",
    "- dataset\n",
    "- torch\n",
    "- transformers\n",
    "- transformers[torch]\n",
    "- evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, disable_caching\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    BertConfig\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/mount/arbeitsdaten31/studenten1/linku/.cache\n",
      "env: HF_MODULES_CACHE=/mount/arbeitsdaten31/studenten1/linku/.cache\n",
      "env: HF_DATASETS_CACHE=/mount/arbeitsdaten31/studenten1/linku/.cache\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR='/mount/arbeitsdaten31/studenten1/linku/.cache'\n",
    "%set_env TRANSFORMERS_CACHE $CACHE_DIR\n",
    "%set_env HF_MODULES_CACHE $CACHE_DIR\n",
    "%set_env HF_DATASETS_CACHE $CACHE_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arguments.py\n",
    "\n",
    "args_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_input_ALstrategy = 'RandomSampling'\n",
    "# args_input_initseed = 100 # 1000\n",
    "args_input_quota = 200 # 200\n",
    "args_input_batch = 50 # 50\n",
    "args_input_dataset_name = 'DROP'\n",
    "args_input_expe_round = 2\n",
    "args_input_model_batch = 8 # already add in arguments.py\n",
    "args_input_max_length = None\n",
    "args_input_learning_rate = 3e-5\n",
    "args_input_model = 'RoBERTa'\n",
    "\n",
    "stride = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = args_input_max_length\n",
    "NUM_QUERY = args_input_batch\n",
    "# NUM_INIT_LB = args_input_initseed\n",
    "ITERATION = int(args_input_quota / args_input_batch)\n",
    "DATA_NAME = args_input_dataset_name\n",
    "STRATEGY_NAME = args_input_ALstrategy\n",
    "\n",
    "MODEL_NAME = args_input_model\n",
    "LEARNING_RATE = args_input_learning_rate\n",
    "EXPE_ROUND = args_input_expe_round\n",
    "MODEL_BATCH = args_input_model_batch\n",
    "NUM_TRAIN_EPOCH = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/mount/arbeitsdaten31/studenten1/linku/models'\n",
    "pretrain_model_dir = '/mount/arbeitsdaten31/studenten1/linku/pretrain_models' + '/' + 'RoBERTa_SQuAD_full_dataset'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_mrqa(d):\n",
    "\t'''\n",
    "\treturn train_set, val_set\n",
    "\t'''\n",
    "\tdata = load_dataset(\"mrqa\", cache_dir=CACHE_DIR)\n",
    "\tif d == 'squad':\n",
    "\t\t# the first to 86588th in train set\n",
    "\t\t# the first to 10507th in val set\t\t\n",
    "\t\treturn data['train'].select(range(86588)), data['validation'].select(range(10507))\n",
    "\telif d == 'newsqa':\n",
    "\t\t# the 86589th to 160748th in train set\n",
    "\t\t# the 10508th to 14719th in val set\t\t\n",
    "\t\treturn data['train'].select(range(86588, 160748)), data['validation'].select(range(10507, 14719))\n",
    "\telif d == 'searchqa':\n",
    "\t\t# the 222437th to 339820th in train set\n",
    "\t\t# the 22505th to 39484th in val set\n",
    "\t\treturn data['train'].select(range(222436, 339820)), data['validation'].select(range(22504, 39484))\n",
    "\telif d == 'bioasq':\n",
    "\t\t# the first to the 1504th in the test set\n",
    "\t\tsub = data['test'].select(range(1504))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\treturn sub.select(range(len_sub_val, len(sub))), sub.select(range(len_sub_val))\n",
    "\telif d == 'textbookqa':\n",
    "\t\t# the 8131st to 9633rd\n",
    "\t\tsub = data['test'].select(range(8130, 9633))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\treturn sub.select(range(len_sub_val, len(sub))), sub.select(range(len_sub_val)) \n",
    "\telif d == 'drop': # Discrete Reasoning Over Paragraphs\n",
    "\t\t# the 1505th to 3007th in test set\n",
    "\t\tsub = data['test'].select(range(1504, 3007))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\treturn sub.select(range(len_sub_val, len(sub))), sub.select(range(len_sub_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mrqa (/mount/arbeitsdaten31/studenten1/linku/.cache/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5536ce0d196d4f3fb2e3f47fbf7b5e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: function different!!!\n",
    "\n",
    "# train_data, val_data = load_dataset_mrqa(DATA_NAME.lower())\n",
    "train_data, val_data = load_dataset_mrqa('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.select(range(1000))\n",
    "val_data = val_data.select(range(100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will preprocess the dataset (training and evaluation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples_lowRes(examples, tokenizer):\n",
    "    # no ['offset_mapping'], for .train() and .eval()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    detected_answers = examples[\"detected_answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = detected_answers[sample_idx]\n",
    "        start_char = answer[\"char_spans\"][0][\"start\"][0]\n",
    "        end_char = answer[\"char_spans\"][0][\"end\"][0]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        example_ids.append(examples[\"qid\"][sample_idx]) # newly added for used in unlabel data predict\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_features_lowRes(examples, tokenizer):\n",
    "    # keep [\"offset_mapping\"] and [\"example_id\"], for compute_metrics()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"detected_answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"char_spans\"][0][\"start\"][0]\n",
    "        end_char = answer[\"char_spans\"][0][\"end\"][0]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        example_ids.append(examples[\"qid\"][sample_idx]) # newly added for used in unlabel data predict\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples_lowRes(examples, tokenizer):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"qid\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer for dataset preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d437e3d8173640d49017fa2077792e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18809ace9b3f4e77b888f962e1101e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5b33e4886743b591e52473381f2cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67463703d374b4d80fc937302d8f1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: The data name different!!!\n",
    "\n",
    "# preprocess data\n",
    "train_dataset = train_data.map(\n",
    "    preprocess_training_examples_lowRes,\n",
    "    batched=True,\n",
    "    remove_columns=train_data.column_names,\n",
    "    fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "train_features = train_data.map(\n",
    "    preprocess_training_features_lowRes,\n",
    "    batched=True,\n",
    "    remove_columns=train_data.column_names,\n",
    "    fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "val_dataset = val_data.map(\n",
    "    preprocess_validation_examples_lowRes,\n",
    "    batched=True,\n",
    "    remove_columns=val_data.column_names,\n",
    "    fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "val_features = val_data.map(\n",
    "    preprocess_validation_examples_lowRes,\n",
    "    batched=True,\n",
    "    remove_columns=val_data.column_names,\n",
    "    fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "train_features.set_format(\"torch\")\n",
    "val_dataset = val_dataset.remove_columns([\"offset_mapping\"])\n",
    "val_dataset.set_format(\"torch\")\n",
    "val_features.set_format(\"torch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_lowRes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_model_dir = model_dir + '/lowRes_' + str(args_input_quota) + '_' + STRATEGY_NAME + '_' + 'RoBERTa' +  '_' + DATA_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train_lowRes(num_train_epochs, train_dataloader, device, model, optimizer, lr_scheduler, record_loss=False):\n",
    "\tprint('Training was performed using {} query data, i.e. {} data.'.format(NUM_QUERY, len(train_dataloader.dataset)))\n",
    "\tfor epoch in range(num_train_epochs):\n",
    "\t\tmodel.train()\n",
    "\t\tfor step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "\t\t\tbatch = {key: value.to(device) for key, value in batch.items()}\n",
    "\t\t\toutputs = model(**batch)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tlr_scheduler.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tif record_loss:\n",
    "\t\t\tprint('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "\tmodel_to_save = model.module if hasattr(model, 'module') else model \n",
    "\tmodel_to_save.save_pretrained(strategy_model_dir)\n",
    "\tprint('TRAIN done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(dataloader, device, features, examples):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating_pred\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    return compute_metrics(start_logits, end_logits, features, examples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unlabel_data(n_pool, labeled_idxs, train_dataset):\n",
    "    unlabeled_idxs = np.arange(n_pool)[~labeled_idxs]\n",
    "    unlabeled_data = train_dataset.select(indices=unlabeled_idxs)\n",
    "    return unlabeled_idxs, unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    softmax_num = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    return np.round(softmax_num, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aubc(quota, bsize, resseq):\n",
    "\t# it is equal to use np.trapz for calculation\n",
    "\tressum = 0.0\n",
    "\tif quota % bsize == 0:\n",
    "\t\tfor i in range(len(resseq)-1):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\n",
    "\telse:\n",
    "\t\tfor i in range(len(resseq)-2):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\t\tk = quota % bsize\n",
    "\t\tressum = ressum + ((resseq[-1] + resseq[-2]) * k / 2)\n",
    "\tressum = round(ressum / quota,3)\n",
    "\t\n",
    "\treturn ressum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_stddev(datax):\n",
    "\treturn round(np.mean(datax),4),round(np.std(datax),4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling_query(labeled_idxs, n):\n",
    "    print('Random querying starts!')\n",
    "    return np.random.choice(np.where(labeled_idxs==0)[0], n, replace=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of extra data after preprocessing\n",
    "extra = len(train_dataset) - len(train_data)\n",
    "\n",
    "SEED = 4666\n",
    "# os.environ['TORCH_HOME']='./basicmodel'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1)\n",
    "\n",
    "# fix random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acc = []\n",
    "acq_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate initial labeled pool\n",
    "n_pool = len(train_dataset)\n",
    "labeled_idxs = np.zeros(n_pool, dtype=bool)\n",
    "\n",
    "## record acc performance \n",
    "acc = np.zeros(ITERATION) # quota/batch runs\n",
    "acc_em = np.zeros(ITERATION)\n",
    "\n",
    "## load the selected train data to DataLoader\n",
    "eval_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=MODEL_BATCH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random querying starts!\n",
      "Use pretrain model in iteration  1\n",
      "Training was performed using 50 query data, i.e. 50 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a99e1952c524596b60800f3c019427a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd3de34234546279c958de93433a790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82503d02a3514e7bad8c91663c399bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "## use total_query (NUM_QUERY + extra) to query instead of just NUM_QUERY\n",
    "total_query = NUM_QUERY + extra\n",
    "\n",
    "## query\n",
    "q_idxs = random_sampling_query(labeled_idxs, total_query)\n",
    "\n",
    "# print('Time spent for querying:', (datetime.datetime.now() - time))\n",
    "# time = datetime.datetime.now()\n",
    "\n",
    "## update\n",
    "    \n",
    "## goal of total query data: NUM_QUERY * i\n",
    "num_set_query_i = NUM_QUERY * i\n",
    "\n",
    "difference_i = 0\n",
    "num_set_ex_id_i = 0\n",
    "\n",
    "while num_set_ex_id_i != num_set_query_i:        \n",
    "    labeled_idxs[q_idxs[:NUM_QUERY + difference_i]] = True\n",
    "    run_i_labeled_idxs = np.arange(n_pool)[labeled_idxs]\n",
    "\n",
    "    run_i_samples = train_features.select(indices=run_i_labeled_idxs)\n",
    "    num_set_ex_id_i = len(set(run_i_samples['example_id']))\n",
    "\n",
    "    difference_i = num_set_query_i - num_set_ex_id_i\n",
    "\n",
    "train_dataloader_i = DataLoader(\n",
    "    train_dataset.select(indices=run_i_labeled_idxs),\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=MODEL_BATCH,\n",
    ")\n",
    "\n",
    "num_update_steps_per_epoch_i = len(train_dataloader_i)\n",
    "num_training_steps_i = NUM_TRAIN_EPOCH * num_update_steps_per_epoch_i\n",
    "\n",
    "if i == 1:\n",
    "    print('Use pretrain model in iteration ', i)\n",
    "    model_i = AutoModelForQuestionAnswering.from_pretrained(pretrain_model_dir).to(device)\n",
    "else:\n",
    "    print('Use strategy model in iteration ', i)\n",
    "    model_i = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "\n",
    "optimizer_i = AdamW(model_i.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "lr_scheduler_i = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer_i,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps_i,\n",
    ")\n",
    "\n",
    "## train\n",
    "to_train_lowRes(NUM_TRAIN_EPOCH, train_dataloader_i, device, model_i, optimizer_i, lr_scheduler_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        example_id = example[\"qid\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"qid\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return predicted_answers, theoretical_answers\n",
    "    # return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ab68d5c4ad4c7cba90b1b514907c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24b32e1a4994b42844a80bfb4554441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# acc_scores_i = get_pred(eval_dataloader, device, val_features, val_data)\n",
    "# print(acc_scores_i)\n",
    "predicted_answers, theoretical_answers = get_pred(eval_dataloader, device, val_features, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'b0626b3af0764c80b1e6f22c114982c1', 'prediction_text': 'Denver Broncos'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'b0626b3af0764c80b1e6f22c114982c1',\n",
       " 'answers': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theoretical_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'b0626b3af0764c80b1e6f22c114982c1',\n",
       " 'answers': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_scores_i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(answers, predictions, skip_no_answer=False):\n",
    "    f1 = exact_match = total = 0\n",
    "    for qid, ground_truths in answers.items():\n",
    "        if qid not in predictions:\n",
    "            if not skip_no_answer:\n",
    "                message = 'Unanswered question %s will receive score 0.' % qid\n",
    "                print(message)\n",
    "                total += 1\n",
    "            continue\n",
    "        total += 1\n",
    "        prediction = predictions[qid]\n",
    "        exact_match += metric_max_over_ground_truths(\n",
    "            exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(\n",
    "            f1_score, prediction, ground_truths)\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with lowRes data\n",
    "\n",
    "# while (EXPE_ROUND > 0): \n",
    "# \tEXPE_ROUND = EXPE_ROUND - 1\n",
    "\n",
    "# \tstart = datetime.datetime.now()\n",
    "\t\n",
    "# \t## generate initial labeled pool\n",
    "# \tn_pool = len(train_dataset)\n",
    "# \tlabeled_idxs = np.zeros(n_pool, dtype=bool)\n",
    "\t\n",
    "# \t## record acc performance \n",
    "# \tacc = np.zeros(ITERATION) # quota/batch runs\n",
    "# \tacc_em = np.zeros(ITERATION)\n",
    "\n",
    "# \t## load the selected train data to DataLoader\n",
    "# \teval_dataloader = DataLoader(\n",
    "# \t\tval_dataset, \n",
    "# \t\tcollate_fn=default_data_collator, \n",
    "# \t\tbatch_size=MODEL_BATCH\n",
    "# \t)\n",
    "\n",
    "# \ttime = datetime.datetime.now()\n",
    "\t\n",
    "# \t## iteration 1 to i\n",
    "# \tfor i in range(1, ITERATION+1):\n",
    "# \t\tprint('Iteraion {} in experiment round {}'.format(i, 5 - EXPE_ROUND))\n",
    "\n",
    "# \t\t## use total_query (NUM_QUERY + extra) to query instead of just NUM_QUERY\n",
    "# \t\ttotal_query = NUM_QUERY + extra\n",
    "\n",
    "# \t\t## query\n",
    "# \t\tq_idxs = random_sampling_query(labeled_idxs, total_query)\n",
    "\n",
    "# \t\tprint('Time spent for querying:', (datetime.datetime.now() - time))\n",
    "# \t\ttime = datetime.datetime.now()\n",
    "\n",
    "# \t\t## update\n",
    "\t\t \n",
    "# \t\t## goal of total query data: NUM_QUERY * i\n",
    "# \t\tnum_set_query_i = NUM_QUERY * i\n",
    "\t\t\n",
    "# \t\tdifference_i = 0\n",
    "# \t\tnum_set_ex_id_i = 0\n",
    "\n",
    "# \t\twhile num_set_ex_id_i != num_set_query_i:        \n",
    "# \t\t\tlabeled_idxs[q_idxs[:NUM_QUERY + difference_i]] = True\n",
    "# \t\t\trun_i_labeled_idxs = np.arange(n_pool)[labeled_idxs]\n",
    "\n",
    "# \t\t\trun_i_samples = train_features.select(indices=run_i_labeled_idxs)\n",
    "# \t\t\tnum_set_ex_id_i = len(set(run_i_samples['example_id']))\n",
    "\n",
    "# \t\t\tdifference_i = num_set_query_i - num_set_ex_id_i\n",
    "\n",
    "# \t\ttrain_dataloader_i = DataLoader(\n",
    "# \t\t\ttrain_dataset.select(indices=run_i_labeled_idxs),\n",
    "# \t\t\tshuffle=True,\n",
    "# \t\t\tcollate_fn=default_data_collator,\n",
    "# \t\t\tbatch_size=MODEL_BATCH,\n",
    "# \t\t)\n",
    "\n",
    "# \t\tnum_update_steps_per_epoch_i = len(train_dataloader_i)\n",
    "# \t\tnum_training_steps_i = NUM_TRAIN_EPOCH * num_update_steps_per_epoch_i\n",
    "\n",
    "# \t\tif i == 1:\n",
    "# \t\t\tprint('Use pretrain model in iteration ', i)\n",
    "# \t\t\tmodel_i = AutoModelForQuestionAnswering.from_pretrained(pretrain_model_dir).to(device)\n",
    "# \t\telse:\n",
    "# \t\t\tprint('Use strategy model in iteration ', i)\n",
    "# \t\t\tmodel_i = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "\n",
    "# \t\toptimizer_i = AdamW(model_i.parameters(), lr=LEARNING_RATE)\n",
    "\t\t\n",
    "# \t\tlr_scheduler_i = get_scheduler(\n",
    "# \t\t\t\"linear\",\n",
    "# \t\t\toptimizer=optimizer_i,\n",
    "# \t\t\tnum_warmup_steps=0,\n",
    "# \t\t\tnum_training_steps=num_training_steps_i,\n",
    "# \t\t)\n",
    "\t\t\n",
    "# \t\t## train\n",
    "# \t\tto_train_lowRes(NUM_TRAIN_EPOCH, train_dataloader_i, device, model_i, optimizer_i, lr_scheduler_i)\n",
    "\n",
    "# \t\t## iteration i accuracy\n",
    "# \t\tprint('iter_{} get_pred!'.format(i))\n",
    "# \t\tacc_scores_i = get_pred_lowRes(eval_dataloader, device, val_features, val_data)\n",
    "# \t\tacc[i] = acc_scores_i['f1']\n",
    "# \t\tacc_em[i] = acc_scores_i['exact_match']\n",
    "# \t\tprint('testing accuracy {}'.format(acc[i]))\n",
    "# \t\tprint('testing accuracy em {}'.format(acc_em[i]))\n",
    "# \t\tprint('Time spent for training after querying:', (datetime.datetime.now() - time))\n",
    "# \t\ttime = datetime.datetime.now()\n",
    "# \t\tprint('\\n')\n",
    "\n",
    "# \t\ttorch.cuda.empty_cache()\n",
    "\t\n",
    "# \t## print results\n",
    "# \tprint('SEED {}'.format(SEED))\n",
    "# \tprint(STRATEGY_NAME)\n",
    "# \tprint(acc)\n",
    "# \tall_acc.append(acc)\n",
    "\t\n",
    "# \t## save model and record acq time\n",
    "# \ttimestamp = re.sub('\\.[0-9]*','_',str(datetime.datetime.now())).replace(\" \", \"_\").replace(\"-\", \"\").replace(\":\",\"\")\n",
    "# \tfinal_model_dir = model_dir + '/' + timestamp + DATA_NAME+ '_'  + STRATEGY_NAME + '_' + str(NUM_QUERY) + '_' + str(args_input_quota)\n",
    "# \tos.makedirs(final_model_dir, exist_ok=True)\n",
    "# \tend = datetime.datetime.now()\n",
    "# \tacq_time.append(round(float((end-start).seconds), 3))\n",
    "\n",
    "# \tfinal_model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "# \tmodel_to_save = final_model.module if hasattr(final_model, 'module') else final_model \n",
    "# \tmodel_to_save.save_pretrained(final_model_dir)\n",
    "\n",
    "# # cal mean & standard deviation\n",
    "# acc_m = []\n",
    "# file_name_res_tot = str(args_input_quota) + '_' + STRATEGY_NAME + '_' + MODEL_NAME + '_' + DATA_NAME + '_normal_res_tot.txt'\n",
    "# file_res_tot =  open(os.path.join(os.path.abspath('') + '/results_lowRes', '%s' % file_name_res_tot),'w')\n",
    "\n",
    "# file_res_tot.writelines('dataset: {}'.format(DATA_NAME) + '\\n')\n",
    "# file_res_tot.writelines('AL strategy: {}'.format(STRATEGY_NAME) + '\\n')\n",
    "# file_res_tot.writelines('number of unlabeled pool: {}'.format(len(train_dataset)) + '\\n')\n",
    "# file_res_tot.writelines('number of testing pool: {}'.format(len(val_dataset)) + '\\n')\n",
    "# file_res_tot.writelines('batch size: {}'.format(NUM_QUERY) + '\\n')\n",
    "# file_res_tot.writelines('quota: {}'.format(ITERATION*NUM_QUERY)+ '\\n')\n",
    "# file_res_tot.writelines('time of repeat experiments: {}'.format(args_input_expe_round)+ '\\n')\n",
    "\n",
    "# # result\n",
    "# for i in range(len(all_acc)):\n",
    "# \tacc_m.append(get_aubc(args_input_quota, NUM_QUERY, all_acc[i]))\n",
    "# \tprint(str(i)+': '+str(acc_m[i]))\n",
    "# \tfile_res_tot.writelines(str(i)+': '+str(acc_m[i])+'\\n')\n",
    "# mean_acc, stddev_acc = get_mean_stddev(acc_m)\n",
    "# mean_time, stddev_time = get_mean_stddev(acq_time)\n",
    "\n",
    "# print('mean AUBC(acc): '+str(mean_acc)+'. std dev AUBC(acc): '+str(stddev_acc))\n",
    "# print('mean time: '+str(mean_time)+'. std dev time: '+str(stddev_time))\n",
    "\n",
    "# file_res_tot.writelines('mean acc: '+str(mean_acc)+'. std dev acc: '+str(stddev_acc)+'\\n')\n",
    "# file_res_tot.writelines('mean time: '+str(mean_time)+'. std dev acc: '+str(stddev_time)+'\\n')\n",
    "\n",
    "# # save result\n",
    "# file_name_res = str(args_input_quota) + '_' + STRATEGY_NAME + '_' + MODEL_NAME + '_' + DATA_NAME + '_normal_res.txt'\n",
    "# file_res =  open(os.path.join(os.path.abspath('') + '/results_lowRes', '%s' % file_name_res),'w')\n",
    "\n",
    "\n",
    "# file_res.writelines('dataset: {}'.format(DATA_NAME) + '\\n')\n",
    "# file_res.writelines('AL strategy: {}'.format(STRATEGY_NAME) + '\\n')\n",
    "# file_res.writelines('number of unlabeled pool: {}'.format(len(train_dataset)) + '\\n')\n",
    "# file_res.writelines('number of testing pool: {}'.format(len(val_dataset)) + '\\n')\n",
    "# file_res.writelines('batch size: {}'.format(NUM_QUERY) + '\\n')\n",
    "# file_res.writelines('quota: {}'.format(ITERATION*NUM_QUERY)+ '\\n')\n",
    "# file_res.writelines('time of repeat experiments: {}'.format(args_input_expe_round)+ '\\n')\n",
    "# avg_acc = np.mean(np.array(all_acc),axis=0)\n",
    "# for i in range(len(avg_acc)):\n",
    "# \ttmp = 'Size of training set is ' + str(i*NUM_QUERY) + ', ' + 'accuracy is ' + str(round(avg_acc[i],4)) + '.' + '\\n'\n",
    "# \tfile_res.writelines(tmp)\n",
    "\n",
    "# file_res.close()\n",
    "# file_res_tot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alqa",
   "language": "python",
   "name": "alqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6317ac8a4c65d05b4cf6bac76f72bfaae40b2e9380067c26c20c9afff1d8528e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
