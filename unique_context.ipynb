{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Model \n",
    "## no trainer\n",
    "\n",
    "- dataset\n",
    "- torch\n",
    "- transformers\n",
    "- transformers[torch]\n",
    "- evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, disable_caching\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    AutoModelForQuestionAnswering\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch.cuda import amp\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# for badge_query\n",
    "from scipy import stats\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pdb\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arguments.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATEGY_NAME = 'RandomSampling'\n",
    "STRATEGY_NAME = 'LeastConfidence'\n",
    "# STRATEGY_NAME = 'MarginSampling'\n",
    "# STRATEGY_NAME = 'BALDDropout'\n",
    "DATA_NAME = 'BioASQ'\n",
    "# EXPE_ROUND = 5\n",
    "MODEL_BATCH = 8\n",
    "MAX_LENGTH = None\n",
    "LEARNING_RATE = 3e-5\n",
    "MODEL_NAME = 'RoBERTa'\n",
    "LOW_RES = True\n",
    "NUM_TRAIN_EPOCH = 3\n",
    "UNIQ_CONTEXT = True\n",
    "if LOW_RES:\n",
    "    args_input_quota = 200\n",
    "    NUM_QUERY = 50\n",
    "else:\n",
    "    NUM_INIT_LB = 500 # 1000\n",
    "    args_input_quota = 2000 # 200\n",
    "    NUM_QUERY = 500 # 50\n",
    "# ITERATION = int(args_input_quota / NUM_QUERY)\n",
    "\n",
    "stride = 128\n",
    "\n",
    "model_dir = '/mount/arbeitsdaten31/studenten1/linku/dev_models'\n",
    "CACHE_DIR = '/mount/arbeitsdaten31/studenten1/linku/.cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples, tokenizer):\n",
    "    # no ['offset_mapping'], for .train() and .eval()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def preprocess_training_features(examples, tokenizer):\n",
    "    # keep [\"offset_mapping\"] and [\"example_id\"], for compute_metrics()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "    contexts = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # added for used in unlabel data predict\n",
    "        example_ids.append(examples[\"id\"][sample_idx]) \n",
    "        # added for unique context filter\n",
    "        contexts.append(examples[\"context\"][sample_idx])\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"context\"] = contexts\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "def preprocess_validation_examples(examples, tokenizer):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs\n",
    "\n",
    "def preprocess_training_examples_lowRes(examples, tokenizer):\n",
    "    # no ['offset_mapping'], for .train() and .eval()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    detected_answers = examples[\"detected_answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = detected_answers[sample_idx]\n",
    "        start_char = answer[\"char_spans\"][0][\"start\"][0]\n",
    "        end_char = answer[\"char_spans\"][0][\"end\"][0]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "def preprocess_training_features_lowRes(examples, tokenizer):\n",
    "    # keep [\"offset_mapping\"] and [\"example_id\"], for compute_metrics()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"detected_answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "    contexts = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"char_spans\"][0][\"start\"][0]\n",
    "        end_char = answer[\"char_spans\"][0][\"end\"][0]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # added for used in unlabel data predict\n",
    "        example_ids.append(examples[\"qid\"][sample_idx])\n",
    "        # added for unique context filter\n",
    "        contexts.append(examples['context'][sample_idx])\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"context\"] = contexts\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "def preprocess_validation_examples_lowRes(examples, tokenizer):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"qid\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aubc(quota, bsize, resseq):\n",
    "\t# it is equal to use np.trapz for calculation\n",
    "\tressum = 0.0\n",
    "\tif quota % bsize == 0:\n",
    "\t\tfor i in range(len(resseq)-1):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\n",
    "\telse:\n",
    "\t\tfor i in range(len(resseq)-2):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\t\tk = quota % bsize\n",
    "\t\tressum = ressum + ((resseq[-1] + resseq[-2]) * k / 2)\n",
    "\tressum = round(ressum / quota,3)\n",
    "\t\n",
    "\treturn ressum\n",
    "\n",
    "\n",
    "def get_mean_stddev(datax):\n",
    "\treturn round(np.mean(datax),4),round(np.std(datax),4)\n",
    "\n",
    "\n",
    "def get_unlabel_data(n_pool, labeled_idxs, train_dataset):\n",
    "    unlabeled_idxs = np.arange(n_pool)[~labeled_idxs]\n",
    "    unlabeled_data = train_dataset.select(indices=unlabeled_idxs)\n",
    "    return unlabeled_idxs, unlabeled_data\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "\n",
    "def get_model(m):\n",
    "\tif m.lower() == 'bert':\n",
    "\t\treturn 'bert-base-uncased'\n",
    "\telif m.lower() == 'bertlarge':\n",
    "\t\treturn 'bert-large-uncased'\n",
    "\telif m.lower() == 'roberta':\n",
    "\t\treturn 'roberta-base'\n",
    "\telif m.lower() == 'robertalarge':\n",
    "\t\treturn 'roberta-large'\n",
    "\n",
    "\n",
    "def get_context_id(data):\n",
    "    context_id = {}\n",
    "    for i, c in enumerate(set(data['context'])):\n",
    "        context_id[c] = i+1\n",
    "    return context_id\n",
    "\n",
    "\n",
    "def preprocess_data(train_data, val_data):\n",
    "\ttokenizer = AutoTokenizer.from_pretrained(get_model(MODEL_NAME))\n",
    "\n",
    "\tif LOW_RES:\n",
    "\t\ttrain_dataset = train_data.map(\n",
    "\t\t\tpreprocess_training_examples_lowRes,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=train_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\ttrain_features = train_data.map(\n",
    "\t\t\tpreprocess_training_features_lowRes,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=train_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\tval_dataset = val_data.map(\n",
    "\t\t\tpreprocess_validation_examples_lowRes,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=val_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\tval_features = val_data.map(\n",
    "\t\t\tpreprocess_validation_examples_lowRes,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=val_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\ttrain_dataset = train_data.map(\n",
    "\t\t\tpreprocess_training_examples,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=train_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\ttrain_features = train_data.map(\n",
    "\t\t\tpreprocess_training_features,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=train_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\tval_dataset = val_data.map(\n",
    "\t\t\tpreprocess_validation_examples,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=val_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\tval_features = val_data.map(\n",
    "\t\t\tpreprocess_validation_examples,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=val_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\n",
    "\ttrain_dataset.set_format(\"torch\")\n",
    "\ttrain_features.set_format(\"torch\")\n",
    "\tval_dataset = val_dataset.remove_columns([\"offset_mapping\"])\n",
    "\tval_dataset.set_format(\"torch\")\n",
    "\tval_features.set_format(\"torch\")\n",
    "\n",
    "\treturn train_dataset, train_features, val_dataset, val_features\n",
    "\n",
    "\n",
    "def load_dataset_mrqa(d):\n",
    "\t'''\n",
    "\treturn train_set, val_set\n",
    "\t'''\n",
    "\tdata = load_dataset(\"mrqa\", cache_dir=CACHE_DIR)\n",
    "\tif d == 'squad':\n",
    "\t\t# the first to 86588th in train set\n",
    "\t\t# the first to 10507th in val set\n",
    "\t\tsquad_train = data['train'].select(range(86588))\n",
    "\t\tsquad_val = data['validation'].select(range(10507))\n",
    "\t\tfor t in squad_train: assert t['subset'] == 'SQuAD', 'Please select corrrect train data for SQuAD.'\n",
    "\t\tfor v in squad_val: assert v['subset'] == 'SQuAD', 'Please select corrrect validation data for SQuAD.'\n",
    "\t\treturn squad_train, squad_val\n",
    "\telif d == 'newsqa':\n",
    "\t\t# the 86589th to 160748th in train set\n",
    "\t\t# the 10508th to 14719th in val set\n",
    "\t\tdata_set = data['train'].select(range(86588, 160748))\n",
    "\t\tnewsqa_train = data_set.shuffle(1127).select(range(10000))\n",
    "\t\tnewsqa_val = data['validation'].select(range(10507, 14719))\n",
    "\t\tfor t in newsqa_train: assert t['subset'] == 'NewsQA', 'Please select corrrect train data for NewQA.'\n",
    "\t\tfor v in newsqa_val: assert v['subset'] == 'NewsQA', 'Please select corrrect validation data for NewQA.'\n",
    "\t\treturn newsqa_train, newsqa_val\n",
    "\telif d == 'searchqa':\n",
    "\t\t# the 222437th to 339820th in train set\n",
    "\t\t# the 22505th to 39484th in val set\n",
    "\t\tdata_set = data['train'].select(range(222436, 339820))\n",
    "\t\tsearchqa_train = data_set.shuffle(1127).select(range(10000))\n",
    "\t\tsearchqa_val = data['validation'].select(range(22504, 39484))\t\n",
    "\t\tfor t in searchqa_train: assert t['subset'] == 'SearchQA', 'Please select corrrect train data for SearchQA.'\n",
    "\t\tfor v in searchqa_val: assert v['subset'] == 'SearchQA', 'Please select corrrect validation data for SearchQA.'\n",
    "\t\treturn searchqa_train, searchqa_val\n",
    "\telif d == 'bioasq':\n",
    "\t\t# the first to the 1504th in the test set\n",
    "\t\tsub = data['test'].select(range(1504))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\tbioasq_train = sub.select(range(len_sub_val, len(sub)))\n",
    "\t\tbioasq_val = sub.select(range(len_sub_val))\n",
    "\t\tfor t in bioasq_train: assert t['subset'] == 'BioASQ', 'Please select corrrect train data for BioASQ.'\n",
    "\t\tfor v in bioasq_val: assert v['subset'] == 'BioASQ', 'Please select corrrect validation data for BioASQ.'\n",
    "\t\treturn bioasq_train, bioasq_val\n",
    "\telif d == 'textbookqa':\n",
    "\t\t# the 8131st to 9633rd\n",
    "\t\tsub = data['test'].select(range(8130, 9633))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\ttextbookqa_train = sub.select(range(len_sub_val, len(sub)))\n",
    "\t\ttextbookqa_val = sub.select(range(len_sub_val)) \n",
    "\t\tfor t in textbookqa_train: assert t['subset'] == 'TextbookQA', 'Please select corrrect train data for TextbookQA.'\n",
    "\t\tfor v in textbookqa_val: assert v['subset'] == 'TextbookQA', 'Please select corrrect validation data for TextbookQA.'\n",
    "\t\treturn textbookqa_train, textbookqa_val\n",
    "\telif d == 'drop': # Discrete Reasoning Over Paragraphs\n",
    "\t\t# the 1505th to 3007th in test set\n",
    "\t\tsub = data['test'].select(range(1504, 3007))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\tdrop_train = sub.select(range(len_sub_val, len(sub)))\n",
    "\t\tdrop_val = sub.select(range(len_sub_val))\n",
    "\t\tfor t in drop_train: assert t['subset'] == 'DROP', 'Please select corrrect train data for DROP.'\n",
    "\t\tfor v in drop_val: assert v['subset'] == 'DROP', 'Please select corrrect validation data for DROP.'\n",
    "\t\treturn drop_train, drop_val\n",
    "\t\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluation(theoretical_answers, predicted_answers, skip_no_answer=False):\n",
    "    '''\n",
    "\ttheoretical_answers, datatype=dict\n",
    "\t{strings of id: list of ground truth answers}\n",
    "\tpredicted_answers, datatype=dict\n",
    "\t{strings of id: strings of prediction text}\n",
    "\t'''\n",
    "    f1 = exact_match = total = 0\n",
    "    for qid, ground_truths in theoretical_answers.items():\n",
    "        if qid not in predicted_answers:\n",
    "            if not skip_no_answer:\n",
    "                message = 'Unanswered question %s will receive score 0.' % qid\n",
    "                print(message)\n",
    "                total += 1\n",
    "            continue\n",
    "        total += 1\n",
    "        prediction = predicted_answers[qid]\n",
    "        exact_match += metric_max_over_ground_truths(\n",
    "            exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(\n",
    "            f1_score, prediction, ground_truths)\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}\n",
    "\n",
    "\n",
    "def save_model(device, pretrain_dir, strategy_dir):\n",
    "    '''\n",
    "    Copy and save model from pretrain_models to current trained models.\n",
    "    '''\n",
    "    pretrain_model = AutoModelForQuestionAnswering.from_pretrained(pretrain_dir).to(device)\n",
    "    model_to_save = pretrain_model.module if hasattr(pretrain_model, 'module') else pretrain_model \n",
    "    model_to_save.save_pretrained(strategy_dir)\n",
    "\n",
    "\n",
    "def init_centers(X, K):\n",
    "    ind = np.argmax([np.linalg.norm(s, 2) for s in X])\n",
    "    mu = [X[ind]]\n",
    "    indsAll = [ind]\n",
    "    centInds = [0.] * len(X)\n",
    "    cent = 0\n",
    "    print('#Samps\\tTotal Distance')\n",
    "    while len(mu) < K:\n",
    "        if len(mu) == 1:\n",
    "            D2 = pairwise_distances(X, mu).ravel().astype(float)\n",
    "        else:\n",
    "            newD = pairwise_distances(X, [mu[-1]]).ravel().astype(float)\n",
    "            for i in range(len(X)):\n",
    "                if D2[i] >  newD[i]:\n",
    "                    centInds[i] = cent\n",
    "                    D2[i] = newD[i]\n",
    "        print(str(len(mu)) + '\\t' + str(sum(D2)), flush=True)\n",
    "        if sum(D2) == 0.0: pdb.set_trace()\n",
    "        D2 = D2.ravel().astype(float)\n",
    "        Ddist = (D2 ** 2)/ sum(D2 ** 2)\n",
    "        customDist = stats.rv_discrete(name='custm', values=(np.arange(len(D2)), Ddist))\n",
    "        ind = customDist.rvs(size=1)[0]\n",
    "        while ind in indsAll: ind = customDist.rvs(size=1)[0]\n",
    "        mu.append(X[ind])\n",
    "        indsAll.append(ind)\n",
    "        cent += 1\n",
    "    return indsAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_context(q_idxs, features, context_dict, exist_c_id=None):\t\n",
    "\t# create a new_q_idxs with unique context_id\n",
    "\tif exist_c_id:\n",
    "\t\tc_id_lst = exist_c_id\n",
    "\telse:\n",
    "\t\tc_id_lst = []\n",
    "\n",
    "\tnew_q_idxs = []\n",
    "\tfor q_i in tqdm(q_idxs, \"Creating unique context idxs\"):\n",
    "\t\tsample = features.select(indices=[q_i])\n",
    "\t\tc_id = context_dict[sample['context'][0]]\n",
    "\t\tif c_id not in c_id_lst:\n",
    "\t\t\tnew_q_idxs.append(q_i)\n",
    "\t\t\tc_id_lst.append(c_id)\n",
    "\treturn new_q_idxs\n",
    "\n",
    "def get_final_c_id(iter_labeled_idxs, features, context_dict):\n",
    "\tc_id_lst = []\n",
    "\tfor i in tqdm(iter_labeled_idxs, \"Creating final context id\"):\n",
    "\t\tsample = features.select(indices=[i])\n",
    "\t\tc_id_lst.append(context_dict[sample['context'][0]])\n",
    "\treturn c_id_lst\n",
    "\n",
    "def get_unique_sample(labeled_idxs, q_idxs, n_pool, train_features, iteration=0):\n",
    "\tif LOW_RES:\n",
    "\t\tnum_query_i = NUM_QUERY * iteration\n",
    "\t\tprint('num_query_i in get_unique_sample in LOW_RES:', num_query_i)\n",
    "\telse:\n",
    "\t\tnum_query_i = NUM_QUERY * iteration + NUM_INIT_LB\n",
    "\t\tprint('num_query_i in get_unique_sample:', num_query_i)\n",
    "\n",
    "\tdifference_i = 0\n",
    "\tnum_set_ex_id_i = 0\n",
    "\n",
    "\twhile num_set_ex_id_i < num_query_i:\n",
    "\t\tlabeled_idxs[q_idxs[:NUM_QUERY + difference_i]] = True\t# get first num_query, e.g. 50\n",
    "\t\titer_i_labeled_idxs = np.arange(n_pool)[labeled_idxs]\n",
    "\t\tprint('len(iter_i_labeled_idxs):', len(iter_i_labeled_idxs))\n",
    "\n",
    "\t\titer_i_samples = train_features.select(indices=iter_i_labeled_idxs)\n",
    "\t\tnum_set_ex_id_i = len(set(iter_i_samples['example_id']))\n",
    "\t\tprint('number of unique example id:', num_set_ex_id_i)\n",
    "\n",
    "\t\tassert num_set_ex_id_i <= num_query_i, 'Select too many examples!'\n",
    "\t\tassert num_set_ex_id_i > 0, \"Did not select examples!\"\n",
    "\n",
    "\t\tdifference_i = num_query_i - num_set_ex_id_i\n",
    "\t\tprint('difference_i', difference_i)\n",
    "\t\n",
    "\treturn iter_i_labeled_idxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mrqa (/mount/arbeitsdaten31/studenten1/linku/.cache/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa23eb88f8a4e42818f3bf49955b9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "disable_caching()\n",
    "if LOW_RES:\n",
    "\t## set dir\n",
    "\tpretrain_model_dir = '/mount/arbeitsdaten31/studenten1/linku/pretrain_models' + '/' + MODEL_NAME + '_SQuAD_full_dataset_lr_3e-5'\n",
    "\tstrategy_model_dir = model_dir + '/lowRes_' + str(args_input_quota) + '_' + STRATEGY_NAME + '_' + MODEL_NAME +  '_' + DATA_NAME\n",
    "\t## load data\n",
    "\ttrain_data, val_data = load_dataset_mrqa(DATA_NAME.lower())\n",
    "else:\n",
    "\t## set dir\n",
    "\tstrategy_model_dir = model_dir + '/' + str(NUM_INIT_LB) + '_' + str(args_input_quota) + '_' + STRATEGY_NAME + '_' + MODEL_NAME +  '_' + DATA_NAME\n",
    "\t## load data\n",
    "\tsquad = load_dataset(DATA_NAME.lower(), cache_dir=CACHE_DIR)\n",
    "\ttrain_data = squad[\"train\"]\n",
    "\tval_data = squad[\"validation\"]\n",
    "\tprint('Use full training data and full testing data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train(num_train_epochs, train_dataloader, device, model, optimizer, lr_scheduler, record_loss=False):\n",
    "\tif LOW_RES:\n",
    "\t\tprint('Training was performed using {} query data, i.e. {} data.'.format(NUM_QUERY, len(train_dataloader.dataset)))\n",
    "\telse:\n",
    "\t\tprint('Training was performed using the sum of {} initial data and {} query data, i.e. {} data.'.format(NUM_INIT_LB, NUM_QUERY, len(train_dataloader.dataset)))\n",
    "\t\n",
    "\tfor epoch in range(num_train_epochs):\n",
    "\t\tmodel.train()\n",
    "\t\tfor step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "\t\t\tbatch = {key: value.to(device) for key, value in batch.items()}\n",
    "\t\t\toutputs = model(**batch)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tlr_scheduler.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tif record_loss:\n",
    "\t\t\tprint('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "\tmodel_to_save = model.module if hasattr(model, 'module') else model \n",
    "\tmodel_to_save.save_pretrained(strategy_model_dir)\n",
    "\tprint('TRAIN done!')\n",
    "\n",
    "def to_pretrain(num_train_epochs, train_dataloader, device, model, optimizer, lr_scheduler, scaler):\n",
    "\tprint('Training was performed using the full dataset ({} data).'.format(len(train_dataloader.dataset)))\n",
    "\tfor epoch in range(num_train_epochs):\n",
    "\t\tmodel.train()\n",
    "\t\tfor step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "\t\t\tbatch = {key: value.to(device) for key, value in batch.items()}\n",
    "\t\t\twith amp.autocast():\n",
    "\t\t\t\toutputs = model(**batch)\n",
    "\t\t\t\tloss = outputs.loss\n",
    "\t\t\t\n",
    "\t\t\tscaler.scale(loss).backward()\n",
    "\n",
    "\t\t\tscaler.step(optimizer)\n",
    "\t\t\tscaler.update()\n",
    "\t\t\tlr_scheduler.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tprint('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "\tmodel_to_save = model.module if hasattr(model, 'module') else model \n",
    "\tmodel_to_save.save_pretrained(pretrain_model_dir)\n",
    "\tprint('TRAIN done!')\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "\n",
    "def compute_metrics_lowRes(start_logits, end_logits, features, examples):\n",
    "    example_to_features = defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = dict()\n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        example_id = example[\"qid\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers[example_id] = best_answer[\"text\"]\n",
    "        else:\n",
    "            predicted_answers[example_id] = \"\"\n",
    "\n",
    "    theoretical_answers = dict()\n",
    "    for ex in examples: theoretical_answers[ex[\"qid\"]] = ex[\"answers\"]\n",
    "    return evaluation(theoretical_answers, predicted_answers)\n",
    "\n",
    "def get_pred(dataloader, device, features, examples):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating_pred\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    if LOW_RES:\n",
    "        return compute_metrics_lowRes(start_logits, end_logits, features, examples)\n",
    "    else:\n",
    "        return compute_metrics(start_logits, end_logits, features, examples)\n",
    "\n",
    "def get_pretrain_pred(dataloader, device, features, examples):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(pretrain_model_dir).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating_pred\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    return compute_metrics(start_logits, end_logits, features, examples)\n",
    "\n",
    "def get_prob(dataloader, device, features, examples):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating_prob\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    prob_dict = {}\n",
    "    example_to_features = defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    for example in tqdm(examples):\n",
    "        if LOW_RES:\n",
    "            example_id = example[\"qid\"]\n",
    "        else:\n",
    "            example_id = example[\"id\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "        \n",
    "            if len(answers) > 1:\n",
    "                prob_dict[feature_index] = softmax(answers)\n",
    "            elif example_to_features[example_id] != []:\n",
    "                prob_dict[feature_index] = np.array([0])\n",
    "    \n",
    "    return prob_dict\n",
    "\n",
    "def get_prob_dropout(dataloader, device, features, examples, n_drop=10):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.train()\n",
    "    prob_dict = {}\n",
    "    \n",
    "    for i in range(n_drop):\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating_prob_dropout\"):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "            end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "        start_logits = np.concatenate(start_logits)\n",
    "        end_logits = np.concatenate(end_logits)\n",
    "        start_logits = start_logits[: len(features)]\n",
    "        end_logits = end_logits[: len(features)]\n",
    "\n",
    "        example_to_features = defaultdict(list)\n",
    "        max_answer_length = 30\n",
    "        n_best = 20\n",
    "            \n",
    "        for idx, feature in enumerate(features):\n",
    "            example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "        n = 0\n",
    "        for example in tqdm(examples):\n",
    "            if LOW_RES:\n",
    "                example_id = example[\"qid\"]\n",
    "            else:\n",
    "                example_id = example[\"id\"]\n",
    "            answers = []\n",
    "\n",
    "            # Loop through all features associated with that example\n",
    "            for feature_index in example_to_features[example_id]:\n",
    "                start_logit = start_logits[feature_index]\n",
    "                end_logit = end_logits[feature_index]\n",
    "                offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # Skip answers that are not fully in the context\n",
    "                        if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                            continue\n",
    "                        # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                        if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                        ):\n",
    "                            continue\n",
    "\n",
    "                        answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "\n",
    "            if 1 < len(answers) < 200: # pad to same numbers of possible answers\n",
    "                zero_list = [0] * (200 - len(answers))\n",
    "                answers.extend(zero_list)\n",
    "            elif len(answers) >= 200:\n",
    "                answers = answers[:200]\n",
    "\n",
    "            if len(answers) > 1:\n",
    "                if example_to_features[example_id][0] not in prob_dict:\n",
    "                    prob_dict[example_to_features[example_id][0]] = softmax(answers)\n",
    "                else:\n",
    "                    prob_dict[example_to_features[example_id][0]] += softmax(answers)\n",
    "            elif example_to_features[example_id] != []:\n",
    "                if example_to_features[example_id][0] not in prob_dict:\n",
    "                    prob_dict[example_to_features[example_id][0]] = np.array([0])   \n",
    "\n",
    "    for key in prob_dict.keys():\n",
    "        prob_dict[key] /= n_drop\n",
    "\n",
    "    return prob_dict\n",
    "\n",
    "def get_prob_dropout_split(dataloader, device, features, examples, n_drop=10):\n",
    "    ## use tensor to save the answers\n",
    "\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    probs = torch.zeros([n_drop, len(dataloader.dataset), 200])\n",
    "    \n",
    "    for i in range(n_drop):\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating_prob_dropout\"):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "            end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "        start_logits = np.concatenate(start_logits)\n",
    "        end_logits = np.concatenate(end_logits)\n",
    "        start_logits = start_logits[: len(features)]\n",
    "        end_logits = end_logits[: len(features)]\n",
    "\n",
    "        example_to_features = defaultdict(list)\n",
    "        max_answer_length = 30\n",
    "        n_best = 20\n",
    "            \n",
    "        for idx, feature in enumerate(features):\n",
    "            example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "        n = 0\n",
    "        for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "            if LOW_RES:\n",
    "                example_id = example[\"qid\"]\n",
    "            else:\n",
    "                example_id = example[\"id\"]\n",
    "            answers = []\n",
    "\n",
    "            # Loop through all features associated with that example\n",
    "            for feature_index in example_to_features[example_id]:\n",
    "                start_logit = start_logits[feature_index]\n",
    "                end_logit = end_logits[feature_index]\n",
    "                offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # Skip answers that are not fully in the context\n",
    "                        if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                            continue\n",
    "                        # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                        if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                        ):\n",
    "                            continue\n",
    "\n",
    "                        answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "\n",
    "            \n",
    "                if 1 < len(answers) < 200: # pad to same numbers of possible answers\n",
    "                    zero_list = [0] * (200 - len(answers))\n",
    "                    answers.extend(zero_list)\n",
    "                elif len(answers) >= 200:\n",
    "                    answers = answers[:200]\n",
    "\n",
    "                probs[i][feature_index] += torch.tensor(softmax(answers))\n",
    "\n",
    "    return probs\n",
    "\n",
    "def get_embeddings(dataloader, device):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir, output_hidden_states=True).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    embeddings = torch.zeros([len(dataloader.dataset), model.config.to_dict()['hidden_size']])\n",
    "    idxs_start = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating_prob\"):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        \n",
    "            outputs = model(**batch)\n",
    "\n",
    "            hidden_states = outputs.hidden_states\n",
    "            embedding_of_last_layer = hidden_states[-2][:, 0, :]\n",
    "\n",
    "            idxs_end = idxs_start + len(hidden_states[-2])\n",
    "            embeddings[idxs_start:idxs_end] = embedding_of_last_layer.cpu()\n",
    "            idxs_start = idxs_end\n",
    "        \n",
    "    return embeddings\n",
    "\n",
    "def get_grad_embeddings(dataloader, device, features, examples):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir, output_hidden_states=True).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    nLab = 20\n",
    "    embDim = model.config.to_dict()['hidden_size']\n",
    "    embeddings = np.zeros([len(dataloader.dataset), embDim * nLab])\n",
    "\n",
    "    prob_dict = []\n",
    "    idxs_start = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating_prob\"):\n",
    "            batch = {key: Variable(value.to(device)) for key, value in batch.items()}\n",
    "                \n",
    "            # deepAL+: out, e1 = self.clf(x)\n",
    "            outputs = model(**batch)\n",
    "            # deepAL+: e1 = e1.data.cpu().numpy()\n",
    "            hidden_states = outputs.hidden_states\n",
    "            embedding_of_last_layer = hidden_states[-2][:, 0, :]\n",
    "            embedding_of_last_layer = embedding_of_last_layer.data.cpu().numpy()\n",
    "\n",
    "            # matually create features batch\n",
    "            data_len_batch = len(outputs.start_logits)\n",
    "            idxs_end = idxs_start + data_len_batch\n",
    "            batch_idx = list(range(idxs_start, idxs_end))\n",
    "            batch_feat = features.select(batch_idx)\n",
    "            idxs_start = idxs_end\n",
    "\n",
    "            out = logits_to_prob(outputs.start_logits.cpu().numpy(), outputs.end_logits.cpu().numpy(), batch_feat, batch_idx, examples, 200)\n",
    "            batchProbs = F.softmax(out, dim=1).data.cpu().numpy()\n",
    "            maxInds = np.argmax(batchProbs, 1)\n",
    "\n",
    "            for j in range(data_len_batch):\n",
    "                for c in range(nLab):\n",
    "                    if c == maxInds[j]:\n",
    "                        embeddings[batch_idx[j]][embDim * c : embDim * (c+1)] = deepcopy(embedding_of_last_layer[j]) * (1 - batchProbs[j][c]) * -1.0\n",
    "                    else:\n",
    "                        embeddings[batch_idx[j]][embDim * c : embDim * (c+1)] = deepcopy(embedding_of_last_layer[j]) * (-1 * batchProbs[j][c]) * -1.0\n",
    "            \n",
    "    return embeddings\n",
    "\n",
    "def logits_to_prob(start_logits, end_logits, features, batch_idx, examples, num_classes):\n",
    "    probs = torch.zeros([len(batch_idx), num_classes])\n",
    "    \n",
    "    example_to_features = defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append((idx, batch_idx[idx]))\n",
    "    \n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        if LOW_RES:\n",
    "            example_id = example[\"qid\"]\n",
    "        else:\n",
    "            example_id = example[\"id\"]\n",
    "        answers = []\n",
    "        \n",
    "        # Loop through all features associated with that example\n",
    "        for (feature_index, i) in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "                    answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "\n",
    "\n",
    "            if 1 < len(answers) < num_classes: # pad to same numbers of possible answers\n",
    "                zero_list = [0] * (num_classes - len(answers))\n",
    "                answers.extend(zero_list)\n",
    "            elif len(answers) >= num_classes:\n",
    "                answers = answers[:num_classes]\n",
    "            probs[feature_index] = torch.tensor(answers)\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling_query(labeled_idxs, n):\n",
    "    print('Random querying starts!')\n",
    "    return np.random.choice(np.where(labeled_idxs==0)[0], n, replace=False)\n",
    "def least_confidence_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n):\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=MODEL_BATCH,\n",
    "\t)\n",
    "\n",
    "    print('LC querying starts!')\n",
    "    print('Query {} data from {} unlabeled training data.'.format(n, len(unlabeled_data)))\n",
    "\n",
    "    prob_dict = get_prob(unlabeled_dataloader, device, unlabeled_features, examples)\n",
    "    print('Got probability!')\n",
    "\n",
    "    confidence_dict = {}\n",
    "    for idx, probs in prob_dict.items():\n",
    "        if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "            confidence_dict[idx] = max(probs)\n",
    "        elif idx:\n",
    "            confidence_dict[idx] = np.array([0])\n",
    "\n",
    "    sorted_confidence_list = sorted(confidence_dict.items(), key=lambda x: x[1])\n",
    "    return unlabeled_idxs[[idx for (idx, confidence) in sorted_confidence_list[:n]]]\n",
    "def least_confidence_dropout_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n):\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=MODEL_BATCH,\n",
    "\t)\n",
    "    \n",
    "    print('LC dropout querying starts!')\n",
    "    print('Query {} data from {} unlabeled training data.'.format(n, len(unlabeled_data)))\n",
    "    \n",
    "    prob_dict = get_prob_dropout(unlabeled_dataloader, device, unlabeled_features, examples, n_drop=10)\n",
    "    print('Got probability!')\n",
    "\n",
    "    confidence_dict = {}\n",
    "    for idx, probs in prob_dict.items():\n",
    "        if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "            confidence_dict[idx] = max(probs)\n",
    "        elif idx:\n",
    "            confidence_dict[idx] = np.array([0])\n",
    "\n",
    "    sorted_confidence_list = sorted(confidence_dict.items(), key=lambda x: x[1])\n",
    "    return unlabeled_idxs[[idx for (idx, confidence) in sorted_confidence_list[:n]]]\n",
    "def bald_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n):\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "        unlabeled_data,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=MODEL_BATCH,\n",
    "    )\n",
    "    print('BALD querying starts!')\n",
    "    print('Query {} data from {} unlabeled training data.'.format(n, len(unlabeled_data)))\n",
    "    \n",
    "    probs = get_prob_dropout_split(unlabeled_dataloader, device, unlabeled_features, examples, n_drop=10)\n",
    "    print('Got probability!')\n",
    "    probs_mean = probs.mean(0)\n",
    "    entropy1 = (-probs_mean*torch.log(probs_mean)).sum(1)\n",
    "    entropy2 = (-probs*torch.log(probs)).sum(2).mean(0)\n",
    "    uncertainties = entropy2 - entropy1\n",
    "    # later on, we can use batch\n",
    "    return unlabeled_idxs[uncertainties.sort()[1][:n]]\n",
    "def kcenter_greedy_query(n_pool, labeled_idxs, train_dataset, device, n):\n",
    "    labeled_idxs_in_query = labeled_idxs.copy()\n",
    "    # train_data = train_dataset\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  collate_fn=default_data_collator,\n",
    "                                  batch_size=MODEL_BATCH,\n",
    "                                )\n",
    "    print('KCenter greedy querying starts!')\n",
    "    print('Query {} data.'.format(n))\n",
    "    \n",
    "    embeddings = get_embeddings(train_dataloader, device)\n",
    "    print('Got embeddings!')\n",
    "    embeddings = embeddings.numpy()\n",
    "\n",
    "    dist_mat = np.matmul(embeddings, embeddings.transpose())\n",
    "    sq = np.array(dist_mat.diagonal()).reshape(len(labeled_idxs_in_query), 1)\n",
    "    dist_mat *= -2\n",
    "    dist_mat += sq\n",
    "    dist_mat += sq.transpose()\n",
    "    dist_mat = np.sqrt(dist_mat)\n",
    "\n",
    "    mat = dist_mat[~labeled_idxs_in_query, :][:, labeled_idxs_in_query]\n",
    "\n",
    "    for i in tqdm(range(n), ncols=100):\n",
    "        mat_min = mat.min(axis=1)\n",
    "        q_idx_ = mat_min.argmax()\n",
    "        q_idx = np.arange(n_pool)[~labeled_idxs_in_query][q_idx_]\n",
    "        labeled_idxs_in_query[q_idx] = True\n",
    "        mat = np.delete(mat, q_idx_, 0)\n",
    "        mat = np.append(mat, dist_mat[~labeled_idxs_in_query, q_idx][:, None], axis=1)\n",
    "        \n",
    "    return np.arange(n_pool)[(labeled_idxs ^ labeled_idxs_in_query)]\n",
    "def badge_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n):\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(unlabeled_data,\n",
    "                                      collate_fn=default_data_collator,\n",
    "                                      batch_size=MODEL_BATCH,\n",
    "                                    )\n",
    "    print('BADGE querying starts!')\n",
    "    print('Query {} data from {} unlabeled training data.'.format(n, len(unlabeled_data)))\n",
    "\n",
    "    gradEmbedding = get_grad_embeddings(unlabeled_dataloader, device, unlabeled_features, examples)\n",
    "    print('Got embeddings!')\n",
    "    chosen = init_centers(gradEmbedding, n)\n",
    "    return unlabeled_idxs[chosen]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will preprocess the dataset (training and evaluation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3642feafac949ca8ebc5e1b1e6c9296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1354 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c76c9f6c0a04a7781446e888bd26cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1354 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4008cfe6a12d4ec5951da1a63af0faa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aee934b3d648cbae223d691e46fe51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(context_dict): 1341\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_features, val_dataset, val_features = preprocess_data(train_data, val_data)\n",
    "context_dict = get_context_id(train_data)\n",
    "print('len(context_dict):', len(context_dict))\n",
    "\n",
    "# get the number of extra data after preprocessing\n",
    "extra = len(train_dataset) - len(train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1127\n",
    "# os.environ['TORCH_HOME']='./basicmodel'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1)\n",
    "\n",
    "# fix random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "all_acc = []\n",
    "acq_time = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_pool: 1442\n",
      "in low res setting\n",
      "BioASQ\n",
      "LeastConfidence\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddc8cd3d39f4ff1bd030a8728f7e943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a50845e391469db951eb9672aa5509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "testing accuracy 57.91499048455571\n",
      "testing accuracy em 35.333333333333336\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## record acc performance \n",
    "# acc = np.zeros(ITERATION + 1) # quota/batch runs + iter_0\n",
    "# acc_em = np.zeros(ITERATION + 1)\n",
    "acc = np.zeros(4) # quota/batch runs + iter_0\n",
    "acc_em = np.zeros(4)\n",
    "\n",
    "## generate initial labeled pool\n",
    "n_pool = len(train_dataset)\n",
    "print('n_pool:', n_pool)\n",
    "labeled_idxs = np.zeros(n_pool, dtype=bool)\n",
    "\n",
    "if LOW_RES:\n",
    "    print('in low res setting')\n",
    "    save_model(device, pretrain_model_dir, strategy_model_dir)\n",
    "    ## print info\n",
    "    print(DATA_NAME)\n",
    "    print(STRATEGY_NAME)\n",
    "else:\n",
    "    print('not in low res setting')\n",
    "    tmp_idxs = np.arange(n_pool)\n",
    "    print('len(tmp_idxs):', len(tmp_idxs))\n",
    "    np.random.shuffle(tmp_idxs)\n",
    "    print('len(tmp_idxs):', len(tmp_idxs))\n",
    "    \n",
    "    if UNIQ_CONTEXT:\n",
    "        print('in uc setting')\n",
    "        tmp_idxs = tmp_idxs[:NUM_INIT_LB+extra]\n",
    "        uc_tmp_idxs = get_unique_context(tmp_idxs, train_features, context_dict) # len() = almost num_query + extra\n",
    "        print('len(uc_tmp_idxs):', len(uc_tmp_idxs))\n",
    "        iter_0_labeled_idxs = get_unique_sample(labeled_idxs, uc_tmp_idxs, n_pool, train_features)\n",
    "        c_id = get_final_c_id(iter_0_labeled_idxs, train_features) # len() = num_query\n",
    "    else:\n",
    "        print('not in uc setting')\n",
    "        iter_0_labeled_idxs = get_unique_sample(labeled_idxs, tmp_idxs, n_pool, train_features)\n",
    "\n",
    "    ## load the selected train data to DataLoader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset.select(indices=iter_0_labeled_idxs),\n",
    "        shuffle=True,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=MODEL_BATCH,\n",
    "    )\n",
    "\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = NUM_TRAIN_EPOCH * num_update_steps_per_epoch\n",
    "\n",
    "    ## network\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(get_model(MODEL_NAME)).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "    ## print info\n",
    "    print(DATA_NAME)\n",
    "    print(STRATEGY_NAME)\n",
    "\n",
    "    ## iteration 0 accuracy\n",
    "    to_train(NUM_TRAIN_EPOCH, train_dataloader, device, model, optimizer, lr_scheduler)\n",
    "\n",
    "## load the selected validation data to DataLoader\n",
    "eval_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=MODEL_BATCH\n",
    ")\n",
    "\n",
    "acc_scores_0 = get_pred(eval_dataloader, device, val_features, val_data) # add i=1 to use model from models_dir\n",
    "acc[0] = acc_scores_0['f1']\n",
    "acc_em[0] = acc_scores_0['exact_match']\n",
    "\n",
    "print('Round 0\\ntesting accuracy {}'.format(acc[0]))\n",
    "print('testing accuracy em {}'.format(acc_em[0]))\n",
    "# time = datetime.datetime.now()\n",
    "# print('Time spent for init training:', (time - start))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LC querying starts!\n",
      "Query 138 data from 1442 unlabeled training data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87ca5769484443daf1149d8c98a37e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob:   0%|          | 0/181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6502b723753f413d98b3255a424a6b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got probability!\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "total_query = NUM_QUERY + extra\n",
    "\t\t\n",
    "## query\n",
    "if STRATEGY_NAME == 'RandomSampling':\n",
    "    q_idxs = random_sampling_query(labeled_idxs, total_query)\n",
    "elif STRATEGY_NAME == 'LeastConfidence':\n",
    "    q_idxs = least_confidence_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n",
    "elif STRATEGY_NAME == 'LeastConfidenceDropout':\n",
    "    q_idxs = least_confidence_dropout_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n",
    "elif STRATEGY_NAME == 'BALDDropout':\n",
    "    q_idxs = bald_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n",
    "elif STRATEGY_NAME == 'KCenterGreedy':\n",
    "    q_idxs = kcenter_greedy_query(n_pool, labeled_idxs, train_dataset, device, total_query)\n",
    "elif STRATEGY_NAME == 'BadgeSampling':\n",
    "    q_idxs = badge_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in uc setting\n",
      "in lr setting\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7c6c7ce65f40ab8acc6b6edf4e031d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating unique context idxs:   0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_query_i in get_unique_sample in LOW_RES: 50\n",
      "len(iter_i_labeled_idxs): 50\n",
      "number of unique example id: 50\n",
      "difference_i 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c69dfcea3074ca389a557f82cebba8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating final context id:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## update\n",
    "if UNIQ_CONTEXT:\n",
    "    print('in uc setting')\n",
    "    if LOW_RES:\n",
    "        print('in lr setting')\n",
    "        uc_q_idxs = get_unique_context(q_idxs, train_features, context_dict)\n",
    "    else:\n",
    "        print('not in lr setting')\n",
    "        uc_q_idxs = get_unique_context(q_idxs, train_features, context_dict, c_id)\n",
    "    \n",
    "    iter_i_labeled_idxs = get_unique_sample(labeled_idxs, uc_q_idxs, n_pool, train_features, i)\n",
    "    c_id = get_final_c_id(iter_i_labeled_idxs, train_features, context_dict)\n",
    "else:\n",
    "    print('not in uc setting')\n",
    "    iter_i_labeled_idxs = get_unique_sample(labeled_idxs, q_idxs, n_pool, train_features, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training was performed using 50 query data, i.e. 50 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd34d108d0e413f9cae323d4554623d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b6823191ae4ffbb92b797c42b65494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f3bf35765c453a9e36b8ccfa405a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "iter_1 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0dcc45ee4684bd1a45d145f7c4ff639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1597a77e5594d46a916386d77de6c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 60.03403810360334\n",
      "testing accuracy em 35.333333333333336\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_i = DataLoader(\n",
    "    train_dataset.select(indices=iter_i_labeled_idxs),\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=MODEL_BATCH,\n",
    ")\n",
    "\n",
    "num_update_steps_per_epoch_i = len(train_dataloader_i)\n",
    "num_training_steps_i = NUM_TRAIN_EPOCH * num_update_steps_per_epoch_i\n",
    "\n",
    "model_i = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "optimizer_i = AdamW(model_i.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "lr_scheduler_i = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer_i,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps_i,\n",
    ")\n",
    "\n",
    "## train\n",
    "to_train(NUM_TRAIN_EPOCH, train_dataloader_i, device, model_i, optimizer_i, lr_scheduler_i)\n",
    "\n",
    "## iteration i accuracy\n",
    "print('iter_{} get_pred!'.format(i))\n",
    "acc_scores_i = get_pred(eval_dataloader, device, val_features, val_data)\n",
    "acc[i] = acc_scores_i['f1']\n",
    "acc_em[i] = acc_scores_i['exact_match']\n",
    "print('testing accuracy {}'.format(acc[i]))\n",
    "print('testing accuracy em {}'.format(acc_em[i]))\n",
    "# print('Time spent for training after querying:', (datetime.datetime.now() - time))\n",
    "# time = datetime.datetime.now()\n",
    "print('\\n')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LC querying starts!\n",
      "Query 138 data from 1392 unlabeled training data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e29e06a7c64b0abf52580b68b206a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob:   0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3d9c3a5d6f44af97f86309db52cc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got probability!\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "total_query = NUM_QUERY + extra\n",
    "\t\t\n",
    "## query\n",
    "if STRATEGY_NAME == 'RandomSampling':\n",
    "    q_idxs = random_sampling_query(labeled_idxs, total_query)\n",
    "elif STRATEGY_NAME == 'LeastConfidence':\n",
    "    q_idxs = least_confidence_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n",
    "elif STRATEGY_NAME == 'LeastConfidenceDropout':\n",
    "    q_idxs = least_confidence_dropout_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n",
    "elif STRATEGY_NAME == 'BALDDropout':\n",
    "    q_idxs = bald_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n",
    "elif STRATEGY_NAME == 'KCenterGreedy':\n",
    "    q_idxs = kcenter_greedy_query(n_pool, labeled_idxs, train_dataset, device, total_query)\n",
    "elif STRATEGY_NAME == 'BadgeSampling':\n",
    "    q_idxs = badge_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n",
    "else:\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0ae240573549b7aeabf169ffdf3b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating unique context idxs:   0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_query_i in get_unique_sample in LOW_RES: 100\n",
      "len(iter_i_labeled_idxs): 100\n",
      "number of unique example id: 99\n",
      "difference_i 1\n",
      "len(iter_i_labeled_idxs): 101\n",
      "number of unique example id: 100\n",
      "difference_i 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e6b8d6f7234a929e2bc63531fe06b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating final context id:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## update\n",
    "if UNIQ_CONTEXT:\n",
    "    if LOW_RES:\n",
    "        uc_q_idxs = get_unique_context(q_idxs, train_features, context_dict)\n",
    "    else:\n",
    "        uc_q_idxs = get_unique_context(q_idxs, train_features, context_dict, c_id)\n",
    "    \n",
    "    iter_i_labeled_idxs = get_unique_sample(labeled_idxs, uc_q_idxs, n_pool, train_features, i)\n",
    "    c_id = get_final_c_id(iter_i_labeled_idxs, train_features, context_dict)\n",
    "else:\n",
    "    iter_i_labeled_idxs = get_unique_sample(labeled_idxs, q_idxs, n_pool, train_features, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training was performed using 50 query data, i.e. 101 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e35a72ca2d4f0dabe0561c0dc0e575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc563e4cdc54488da5d18f64858b5a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74484c568b14c8a88e220b1f45b9f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "iter_2 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e06bcaa8de440581c6379f381fffab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a4608904334635af7cd2e51849bbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 55.64636474636475\n",
      "testing accuracy em 30.666666666666668\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "train_dataloader_i = DataLoader(\n",
    "    train_dataset.select(indices=iter_i_labeled_idxs),\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=MODEL_BATCH,\n",
    ")\n",
    "\n",
    "num_update_steps_per_epoch_i = len(train_dataloader_i)\n",
    "num_training_steps_i = NUM_TRAIN_EPOCH * num_update_steps_per_epoch_i\n",
    "\n",
    "model_i = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "optimizer_i = AdamW(model_i.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "lr_scheduler_i = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer_i,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps_i,\n",
    ")\n",
    "\n",
    "## train\n",
    "to_train(NUM_TRAIN_EPOCH, train_dataloader_i, device, model_i, optimizer_i, lr_scheduler_i)\n",
    "\n",
    "## iteration i accuracy\n",
    "print('iter_{} get_pred!'.format(i))\n",
    "acc_scores_i = get_pred(eval_dataloader, device, val_features, val_data)\n",
    "acc[i] = acc_scores_i['f1']\n",
    "acc_em[i] = acc_scores_i['exact_match']\n",
    "print('testing accuracy {}'.format(acc[i]))\n",
    "print('testing accuracy em {}'.format(acc_em[i]))\n",
    "# print('Time spent for training after querying:', (datetime.datetime.now() - time))\n",
    "# time = datetime.datetime.now()\n",
    "print('\\n')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LC querying starts!\n",
      "Query 138 data from 1341 unlabeled training data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2dd7d1ae9d44b87bad50a2c1602d848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c10e4eefbd4ee89dbb8fd839c44430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got probability!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc059512165483686790635177848fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating unique context idxs:   0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_query_i in get_unique_sample in LOW_RES: 150\n",
      "len(iter_i_labeled_idxs): 151\n",
      "number of unique example id: 150\n",
      "difference_i 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_final_c_id() missing 1 required positional argument: 'context_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mount/studenten/studentenarbeiten/linku/AL_QA/unique_context.ipynb Cell 32\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkiwi.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/unique_context.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m         uc_q_idxs \u001b[39m=\u001b[39m get_unique_context(q_idxs, train_features, context_dict, c_id)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkiwi.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/unique_context.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     iter_i_labeled_idxs \u001b[39m=\u001b[39m get_unique_sample(labeled_idxs, uc_q_idxs, n_pool, train_features, i)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bkiwi.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/unique_context.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     c_id \u001b[39m=\u001b[39m get_final_c_id(iter_i_labeled_idxs, train_features)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkiwi.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/unique_context.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bkiwi.ims.uni-stuttgart.de/mount/studenten/studentenarbeiten/linku/AL_QA/unique_context.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     iter_i_labeled_idxs \u001b[39m=\u001b[39m get_unique_sample(labeled_idxs, q_idxs, n_pool, train_features, i)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_final_c_id() missing 1 required positional argument: 'context_dict'"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "total_query = NUM_QUERY + extra\n",
    "\t\t\n",
    "## query\n",
    "if STRATEGY_NAME == 'RandomSampling':\n",
    "    q_idxs = random_sampling_query(labeled_idxs, total_query)\n",
    "elif STRATEGY_NAME == 'LeastConfidence':\n",
    "    q_idxs = least_confidence_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n",
    "elif STRATEGY_NAME == 'LeastConfidenceDropout':\n",
    "    q_idxs = least_confidence_dropout_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n",
    "elif STRATEGY_NAME == 'BALDDropout':\n",
    "    q_idxs = bald_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n",
    "elif STRATEGY_NAME == 'KCenterGreedy':\n",
    "    q_idxs = kcenter_greedy_query(n_pool, labeled_idxs, train_dataset, device, total_query)\n",
    "elif STRATEGY_NAME == 'BadgeSampling':\n",
    "    q_idxs = badge_query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, total_query)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3ff033fcca4d5382f40accff7e1c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating unique context idxs:   0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_query_i in get_unique_sample in LOW_RES: 150\n",
      "len(iter_i_labeled_idxs): 151\n",
      "number of unique example id: 150\n",
      "difference_i 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35745d1448ba47e79795c406795d83c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating final context id:   0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training was performed using 50 query data, i.e. 151 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6215d63bc99c424f93d20dce3790a8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a2e5e6ee774a69a24c2129ef41181d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48c9daf816d4a44bc37876b1d4a999a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "iter_3 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dffbe579f9a4e35a82e45350fbe10fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c9d04f665b4305883b7849c2ee7840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 52.76296296296296\n",
      "testing accuracy em 26.666666666666668\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## update\n",
    "if UNIQ_CONTEXT:\n",
    "    if LOW_RES:\n",
    "        uc_q_idxs = get_unique_context(q_idxs, train_features, context_dict)\n",
    "    else:\n",
    "        uc_q_idxs = get_unique_context(q_idxs, train_features, context_dict, c_id)\n",
    "    \n",
    "    iter_i_labeled_idxs = get_unique_sample(labeled_idxs, uc_q_idxs, n_pool, train_features, i)\n",
    "    c_id = get_final_c_id(iter_i_labeled_idxs, train_features, context_dict)\n",
    "else:\n",
    "    iter_i_labeled_idxs = get_unique_sample(labeled_idxs, q_idxs, n_pool, train_features, i)\n",
    "    \n",
    "train_dataloader_i = DataLoader(\n",
    "    train_dataset.select(indices=iter_i_labeled_idxs),\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=MODEL_BATCH,\n",
    ")\n",
    "\n",
    "num_update_steps_per_epoch_i = len(train_dataloader_i)\n",
    "num_training_steps_i = NUM_TRAIN_EPOCH * num_update_steps_per_epoch_i\n",
    "\n",
    "model_i = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "optimizer_i = AdamW(model_i.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "lr_scheduler_i = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer_i,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps_i,\n",
    ")\n",
    "\n",
    "## train\n",
    "to_train(NUM_TRAIN_EPOCH, train_dataloader_i, device, model_i, optimizer_i, lr_scheduler_i)\n",
    "\n",
    "## iteration i accuracy\n",
    "print('iter_{} get_pred!'.format(i))\n",
    "acc_scores_i = get_pred(eval_dataloader, device, val_features, val_data)\n",
    "acc[i] = acc_scores_i['f1']\n",
    "acc_em[i] = acc_scores_i['exact_match']\n",
    "print('testing accuracy {}'.format(acc[i]))\n",
    "print('testing accuracy em {}'.format(acc_em[i]))\n",
    "# print('Time spent for training after querying:', (datetime.datetime.now() - time))\n",
    "# time = datetime.datetime.now()\n",
    "print('\\n')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alqa",
   "language": "python",
   "name": "alqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6317ac8a4c65d05b4cf6bac76f72bfaae40b2e9380067c26c20c9afff1d8528e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
