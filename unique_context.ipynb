{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Model \n",
    "## no trainer\n",
    "\n",
    "- dataset\n",
    "- torch\n",
    "- transformers\n",
    "- transformers[torch]\n",
    "- evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, disable_caching\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    AutoModelForQuestionAnswering\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch.cuda import amp\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# for badge_query\n",
    "from scipy import stats\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pdb\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arguments.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATEGY_NAME = 'RandomSampling'\n",
    "STRATEGY_NAME = 'LeastConfidence'\n",
    "# STRATEGY_NAME = 'MarginSampling'\n",
    "# STRATEGY_NAME = 'BALDDropout'\n",
    "DATA_NAME = 'DROP'\n",
    "# EXPE_ROUND = 5\n",
    "MODEL_BATCH = 8\n",
    "MAX_LENGTH = None\n",
    "LEARNING_RATE = 3e-5\n",
    "MODEL_NAME = 'RoBERTa'\n",
    "LOW_RES = True\n",
    "NUM_TRAIN_EPOCH = 3\n",
    "UNIQ_CONTEXT = True\n",
    "if LOW_RES:\n",
    "    args_input_quota = 100\n",
    "    NUM_QUERY = 50\n",
    "else:\n",
    "    NUM_INIT_LB = 500 # 1000\n",
    "    args_input_quota = 2000 # 200\n",
    "    NUM_QUERY = 500 # 50\n",
    "# ITERATION = int(args_input_quota / NUM_QUERY)\n",
    "\n",
    "stride = 128\n",
    "\n",
    "MODEL_DIR = '/mount/studenten/studentenarbeiten/linku/AL_QA/dev_models'\n",
    "CACHE_DIR = '/mount/studenten/studentenarbeiten/linku/AL_QA/.cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples, tokenizer):\n",
    "    # no ['offset_mapping'], for .train() and .eval()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def preprocess_training_features(examples, tokenizer):\n",
    "    # keep [\"offset_mapping\"] and [\"example_id\"], for compute_metrics()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "    contexts = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # added for used in unlabel data predict\n",
    "        example_ids.append(examples[\"id\"][sample_idx]) \n",
    "        # added for unique context filter\n",
    "        contexts.append(examples[\"context\"][sample_idx])\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"context\"] = contexts\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "def preprocess_validation_examples(examples, tokenizer):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs\n",
    "\n",
    "def preprocess_training_examples_lowRes(examples, tokenizer):\n",
    "    # no ['offset_mapping'], for .train() and .eval()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    detected_answers = examples[\"detected_answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = detected_answers[sample_idx]\n",
    "        start_char = answer[\"char_spans\"][0][\"start\"][0]\n",
    "        end_char = answer[\"char_spans\"][0][\"end\"][0]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "def preprocess_training_features_lowRes(examples, tokenizer):\n",
    "    # keep [\"offset_mapping\"] and [\"example_id\"], for compute_metrics()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"detected_answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "    contexts = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"char_spans\"][0][\"start\"][0]\n",
    "        end_char = answer[\"char_spans\"][0][\"end\"][0]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # added for used in unlabel data predict\n",
    "        example_ids.append(examples[\"qid\"][sample_idx])\n",
    "        # added for unique context filter\n",
    "        contexts.append(examples['context'][sample_idx])\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"context\"] = contexts\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "def preprocess_validation_examples_lowRes(examples, tokenizer):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"qid\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aubc(quota, bsize, resseq):\n",
    "\t# it is equal to use np.trapz for calculation\n",
    "\tressum = 0.0\n",
    "\tif quota % bsize == 0:\n",
    "\t\tfor i in range(len(resseq)-1):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\n",
    "\telse:\n",
    "\t\tfor i in range(len(resseq)-2):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\t\tk = quota % bsize\n",
    "\t\tressum = ressum + ((resseq[-1] + resseq[-2]) * k / 2)\n",
    "\tressum = round(ressum / quota,3)\n",
    "\t\n",
    "\treturn ressum\n",
    "\n",
    "\n",
    "def get_mean_stddev(datax):\n",
    "\treturn round(np.mean(datax),4),round(np.std(datax),4)\n",
    "\n",
    "\n",
    "def get_unlabel_data(n_pool, labeled_idxs, train_dataset):\n",
    "    unlabeled_idxs = np.arange(n_pool)[~labeled_idxs]\n",
    "    unlabeled_data = train_dataset.select(indices=unlabeled_idxs)\n",
    "    return unlabeled_idxs, unlabeled_data\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "\n",
    "def get_model(m):\n",
    "\tif m.lower() == 'bert':\n",
    "\t\treturn 'bert-base-uncased'\n",
    "\telif m.lower() == 'bertlarge':\n",
    "\t\treturn 'bert-large-uncased'\n",
    "\telif m.lower() == 'roberta':\n",
    "\t\treturn 'roberta-base'\n",
    "\telif m.lower() == 'robertalarge':\n",
    "\t\treturn 'roberta-large'\n",
    "\n",
    "def preprocess_data(train_data, val_data):\n",
    "\ttokenizer = AutoTokenizer.from_pretrained(get_model(MODEL_NAME))\n",
    "\n",
    "\tif LOW_RES:\n",
    "\t\ttrain_dataset = train_data.map(\n",
    "\t\t\tpreprocess_training_examples_lowRes,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=train_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\ttrain_features = train_data.map(\n",
    "\t\t\tpreprocess_training_features_lowRes,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=train_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\tval_dataset = val_data.map(\n",
    "\t\t\tpreprocess_validation_examples_lowRes,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=val_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\tval_features = val_data.map(\n",
    "\t\t\tpreprocess_validation_examples_lowRes,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=val_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\ttrain_dataset = train_data.map(\n",
    "\t\t\tpreprocess_training_examples,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=train_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\ttrain_features = train_data.map(\n",
    "\t\t\tpreprocess_training_features,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=train_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\tval_dataset = val_data.map(\n",
    "\t\t\tpreprocess_validation_examples,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=val_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\t\tval_features = val_data.map(\n",
    "\t\t\tpreprocess_validation_examples,\n",
    "\t\t\tbatched=True,\n",
    "\t\t\tremove_columns=val_data.column_names,\n",
    "\t\t\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    "\t\t)\n",
    "\n",
    "\ttrain_dataset.set_format(\"torch\")\n",
    "\ttrain_features.set_format(\"torch\")\n",
    "\tval_dataset = val_dataset.remove_columns([\"offset_mapping\"])\n",
    "\tval_dataset.set_format(\"torch\")\n",
    "\tval_features.set_format(\"torch\")\n",
    "\n",
    "\treturn train_dataset, train_features, val_dataset, val_features\n",
    "\n",
    "\n",
    "def load_dataset_mrqa(d):\n",
    "\t'''\n",
    "\treturn train_set, val_set\n",
    "\t'''\n",
    "\tdata = load_dataset(\"mrqa\", cache_dir=CACHE_DIR)\n",
    "\tif d == 'squad':\n",
    "\t\t# the first to 86588th in train set\n",
    "\t\t# the first to 10507th in val set\n",
    "\t\tsquad_train = data['train'].select(range(86588))\n",
    "\t\tsquad_val = data['validation'].select(range(10507))\n",
    "\t\tfor t in squad_train: assert t['subset'] == 'SQuAD', 'Please select corrrect train data for SQuAD.'\n",
    "\t\tfor v in squad_val: assert v['subset'] == 'SQuAD', 'Please select corrrect validation data for SQuAD.'\n",
    "\t\treturn squad_train, squad_val\n",
    "\telif d == 'newsqa':\n",
    "\t\t# the 86589th to 160748th in train set\n",
    "\t\t# the 10508th to 14719th in val set\n",
    "\t\tdata_set = data['train'].select(range(86588, 160748))\n",
    "\t\tnewsqa_train = data_set.shuffle(1127).select(range(10000))\n",
    "\t\tnewsqa_val = data['validation'].select(range(10507, 14719))\n",
    "\t\tfor t in newsqa_train: assert t['subset'] == 'NewsQA', 'Please select corrrect train data for NewQA.'\n",
    "\t\tfor v in newsqa_val: assert v['subset'] == 'NewsQA', 'Please select corrrect validation data for NewQA.'\n",
    "\t\treturn newsqa_train, newsqa_val\n",
    "\telif d == 'searchqa':\n",
    "\t\t# the 222437th to 339820th in train set\n",
    "\t\t# the 22505th to 39484th in val set\n",
    "\t\tdata_set = data['train'].select(range(222436, 339820))\n",
    "\t\tsearchqa_train = data_set.shuffle(1127).select(range(10000))\n",
    "\t\tsearchqa_val = data['validation'].select(range(22504, 39484))\t\n",
    "\t\tfor t in searchqa_train: assert t['subset'] == 'SearchQA', 'Please select corrrect train data for SearchQA.'\n",
    "\t\tfor v in searchqa_val: assert v['subset'] == 'SearchQA', 'Please select corrrect validation data for SearchQA.'\n",
    "\t\treturn searchqa_train, searchqa_val\n",
    "\telif d == 'bioasq':\n",
    "\t\t# the first to the 1504th in the test set\n",
    "\t\tsub = data['test'].select(range(1504))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\tbioasq_train = sub.select(range(len_sub_val, len(sub)))\n",
    "\t\tbioasq_val = sub.select(range(len_sub_val))\n",
    "\t\tfor t in bioasq_train: assert t['subset'] == 'BioASQ', 'Please select corrrect train data for BioASQ.'\n",
    "\t\tfor v in bioasq_val: assert v['subset'] == 'BioASQ', 'Please select corrrect validation data for BioASQ.'\n",
    "\t\treturn bioasq_train, bioasq_val\n",
    "\telif d == 'textbookqa':\n",
    "\t\t# the 8131st to 9633rd\n",
    "\t\tsub = data['test'].select(range(8130, 9633))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\ttextbookqa_train = sub.select(range(len_sub_val, len(sub)))\n",
    "\t\ttextbookqa_val = sub.select(range(len_sub_val)) \n",
    "\t\tfor t in textbookqa_train: assert t['subset'] == 'TextbookQA', 'Please select corrrect train data for TextbookQA.'\n",
    "\t\tfor v in textbookqa_val: assert v['subset'] == 'TextbookQA', 'Please select corrrect validation data for TextbookQA.'\n",
    "\t\treturn textbookqa_train, textbookqa_val\n",
    "\telif d == 'drop': # Discrete Reasoning Over Paragraphs\n",
    "\t\t# the 1505th to 3007th in test set\n",
    "\t\tsub = data['test'].select(range(1504, 3007))\n",
    "\t\tlen_sub_val = len(sub) // 10\n",
    "\t\tdrop_train = sub.select(range(len_sub_val, len(sub)))\n",
    "\t\tdrop_val = sub.select(range(len_sub_val))\n",
    "\t\tfor t in drop_train: assert t['subset'] == 'DROP', 'Please select corrrect train data for DROP.'\n",
    "\t\tfor v in drop_val: assert v['subset'] == 'DROP', 'Please select corrrect validation data for DROP.'\n",
    "\t\treturn drop_train, drop_val\n",
    "\t\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluation(theoretical_answers, predicted_answers, skip_no_answer=False):\n",
    "    '''\n",
    "\ttheoretical_answers, datatype=dict\n",
    "\t{strings of id: list of ground truth answers}\n",
    "\tpredicted_answers, datatype=dict\n",
    "\t{strings of id: strings of prediction text}\n",
    "\t'''\n",
    "    f1 = exact_match = total = 0\n",
    "    for qid, ground_truths in theoretical_answers.items():\n",
    "        if qid not in predicted_answers:\n",
    "            if not skip_no_answer:\n",
    "                message = 'Unanswered question %s will receive score 0.' % qid\n",
    "                print(message)\n",
    "                total += 1\n",
    "            continue\n",
    "        total += 1\n",
    "        prediction = predicted_answers[qid]\n",
    "        exact_match += metric_max_over_ground_truths(\n",
    "            exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(\n",
    "            f1_score, prediction, ground_truths)\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}\n",
    "\n",
    "\n",
    "def save_model(device, pretrain_dir, strategy_dir):\n",
    "    '''\n",
    "    Copy and save model from pretrain_models to current trained models.\n",
    "    '''\n",
    "    pretrain_model = AutoModelForQuestionAnswering.from_pretrained(pretrain_dir).to(device)\n",
    "    model_to_save = pretrain_model.module if hasattr(pretrain_model, 'module') else pretrain_model \n",
    "    model_to_save.save_pretrained(strategy_dir)\n",
    "\n",
    "\n",
    "def init_centers(X, K):\n",
    "    ind = np.argmax([np.linalg.norm(s, 2) for s in X])\n",
    "    mu = [X[ind]]\n",
    "    indsAll = [ind]\n",
    "    centInds = [0.] * len(X)\n",
    "    cent = 0\n",
    "    print('#Samps\\tTotal Distance')\n",
    "    while len(mu) < K:\n",
    "        if len(mu) == 1:\n",
    "            D2 = pairwise_distances(X, mu).ravel().astype(float)\n",
    "        else:\n",
    "            newD = pairwise_distances(X, [mu[-1]]).ravel().astype(float)\n",
    "            for i in range(len(X)):\n",
    "                if D2[i] >  newD[i]:\n",
    "                    centInds[i] = cent\n",
    "                    D2[i] = newD[i]\n",
    "        print(str(len(mu)) + '\\t' + str(sum(D2)), flush=True)\n",
    "        if sum(D2) == 0.0: pdb.set_trace()\n",
    "        D2 = D2.ravel().astype(float)\n",
    "        Ddist = (D2 ** 2)/ sum(D2 ** 2)\n",
    "        customDist = stats.rv_discrete(name='custm', values=(np.arange(len(D2)), Ddist))\n",
    "        ind = customDist.rvs(size=1)[0]\n",
    "        while ind in indsAll: ind = customDist.rvs(size=1)[0]\n",
    "        mu.append(X[ind])\n",
    "        indsAll.append(ind)\n",
    "        cent += 1\n",
    "    return indsAll\n",
    "\n",
    "def query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, i):\n",
    "\tif STRATEGY_NAME == 'RandomSampling':\n",
    "\t\titer_i_labeled_idxs = random_sampling(n_pool, labeled_idxs, train_features, i)\n",
    "\t# elif STRATEGY_NAME == 'MarginSampling':\n",
    "\t# \titer_i_labeled_idxs = margin(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, i)\n",
    "\telif STRATEGY_NAME == 'LeastConfidence':\n",
    "\t\titer_i_labeled_idxs = least_confidence(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, i)\n",
    "\t# elif STRATEGY_NAME == 'EntropySampling':\n",
    "\t# \titer_i_labeled_idxs = entropy(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, i)\n",
    "\t# elif STRATEGY_NAME == 'MarginSamplingDropout':\n",
    "\t# \titer_i_labeled_idxs = margin_dropout(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, i)\n",
    "\t# elif STRATEGY_NAME == 'LeastConfidenceDropout':\n",
    "\t# \titer_i_labeled_idxs = least_confidence_dropout(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, i)\n",
    "\t# elif STRATEGY_NAME == 'EntropySamplingDropout':\n",
    "\t# \titer_i_labeled_idxs = entropy_dropout(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, i)\n",
    "\t# elif STRATEGY_NAME == 'BALDDropout':\n",
    "\t# \titer_i_labeled_idxs = bald(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, i)\n",
    "\t# elif STRATEGY_NAME == 'BatchBALDDropout':\n",
    "\t# \titer_i_labeled_idxs = batch_bald(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, i)\n",
    "\t# elif STRATEGY_NAME == 'MeanSTD':\n",
    "\t# \titer_i_labeled_idxs = mean_std(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, i)\n",
    "\t# elif STRATEGY_NAME == 'KMeansSampling':\n",
    "\t# \titer_i_labeled_idxs = kmeans(n_pool, labeled_idxs, train_dataset, train_features, device, i)\n",
    "\t# elif STRATEGY_NAME == 'KCenterGreedy':\n",
    "\t# \tif args_input.low_resource and i == 1:\n",
    "\t# \t\titer_i_labeled_idxs = random_sampling(n_pool, labeled_idxs, train_features, i)\n",
    "\t# \telse:\n",
    "\t# \t\titer_i_labeled_idxs = kcenter(n_pool, labeled_idxs, train_dataset, train_features, device, i)\n",
    "\t# elif STRATEGY_NAME == 'BadgeSampling':\n",
    "\t# \titer_i_labeled_idxs = badge(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, i)\n",
    "\telse:\n",
    "\t\traise NotImplementedError\n",
    "\treturn iter_i_labeled_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move get_us_uc() for developing\n",
    "\n",
    "def get_us(labeled_idxs, score_ordered_idxs, n_pool, features, iteration=0):\n",
    "\tif LOW_RES:\n",
    "\t\ttotal = NUM_QUERY * iteration\n",
    "\t\tprint('Total num of label pool in LowRes:', total)\n",
    "\telse:\n",
    "\t\ttotal = NUM_QUERY * iteration + NUM_INIT_LB\n",
    "\t\tprint('Total num of label pool in regular:', total)  \n",
    "\n",
    "\t# count if we have enough unique sample to select\n",
    "\tlabeled_idxs_ = labeled_idxs.copy()\n",
    "\tlabeled_idxs_[score_ordered_idxs] = True\n",
    "\tsamples_ = features.select(indices=np.arange(n_pool)[labeled_idxs_])\n",
    "\tssi_ = set(samples_['example_id']) \n",
    "\tprint('\\nWe have {} unique ssi in scored pool.\\n'.format(len(ssi_)))\n",
    "\n",
    "\t# create select_sample_id(ssi) set\n",
    "\tlabeled_idxs[score_ordered_idxs[:NUM_QUERY]] = True\n",
    "\tsamples = features.select(indices=np.arange(n_pool)[labeled_idxs])\n",
    "\tssi = set(samples['example_id']) \n",
    "\tprint('We have {} unique ssi.'.format(len(ssi)))\n",
    "\n",
    "\tsliced_till = NUM_QUERY\n",
    "\twhile len(ssi) < total:\n",
    "\t\tdifference = total - len(ssi)\n",
    "\t\tprint('Not enough ssi, still need {} ssi.'.format(difference))\n",
    "\t\tlabeled_idxs[score_ordered_idxs[sliced_till:sliced_till + difference]] = True\t# get extra\n",
    "\t\tsliced_till += difference\n",
    "\t\tsamples = features.select(indices=np.arange(n_pool)[labeled_idxs])\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tssi.add(sample['example_id'])\n",
    "\t\tprint('End of add extra ssi, now we have {} unique ssi.'.format(len(ssi)))\n",
    "    \n",
    "\tlabeled_idxs[score_ordered_idxs[:sliced_till]] = True\n",
    "\treturn np.arange(n_pool)[labeled_idxs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mrqa (/mount/studenten/studentenarbeiten/linku/AL_QA/.cache/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf3122acd5e498c9b5c3b02205caacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "disable_caching()\n",
    "if LOW_RES:\n",
    "\tinit_pool = 0\n",
    "\tsetting = 'low resource'\n",
    "\t## set dir\n",
    "\tpretrain_model_dir = os.path.abspath('') + '/pretrain_models' + '/' + MODEL_NAME + '_SQuAD_full_dataset_lr_3e-5'\n",
    "\tstrategy_model_dir = MODEL_DIR + '/lowRes_' + str(args_input_quota) + '_' + STRATEGY_NAME + '_' + MODEL_NAME +  '_' + DATA_NAME\n",
    "\t## load data\n",
    "\ttrain_data, val_data = load_dataset_mrqa(DATA_NAME.lower())\n",
    "else:\n",
    "\tinit_pool = NUM_INIT_LB\n",
    "\tsetting = 'regular'\n",
    "\t## set dir\n",
    "\tstrategy_model_dir = MODEL_DIR + '/' + str(NUM_INIT_LB) + '_' + str(args_input_quota) + '_' + STRATEGY_NAME + '_' + MODEL_NAME +  '_' + DATA_NAME\n",
    "\t## load data\n",
    "\tsquad = load_dataset(DATA_NAME.lower(), cache_dir=CACHE_DIR)\n",
    "\t\n",
    "\ttrain_data = squad[\"train\"]\n",
    "\tval_data = squad[\"validation\"]\n",
    "\t# print('Use full training data and full testing data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train(num_train_epochs, train_dataloader, device, model, optimizer, lr_scheduler, record_loss=False):\n",
    "\tif LOW_RES:\n",
    "\t\tprint('Training was performed using {} query data, i.e. {} data.'.format(NUM_QUERY, len(train_dataloader.dataset)))\n",
    "\telse:\n",
    "\t\tprint('Training was performed using the sum of {} initial data and {} query data, i.e. {} data.'.format(NUM_INIT_LB, NUM_QUERY, len(train_dataloader.dataset)))\n",
    "\t\n",
    "\tfor epoch in range(num_train_epochs):\n",
    "\t\tmodel.train()\n",
    "\t\tfor step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "\t\t\tbatch = {key: value.to(device) for key, value in batch.items()}\n",
    "\t\t\toutputs = model(**batch)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tlr_scheduler.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tif record_loss:\n",
    "\t\t\tprint('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "\tmodel_to_save = model.module if hasattr(model, 'module') else model \n",
    "\tmodel_to_save.save_pretrained(strategy_model_dir)\n",
    "\tprint('TRAIN done!')\n",
    "\n",
    "def to_pretrain(num_train_epochs, train_dataloader, device, model, optimizer, lr_scheduler, scaler):\n",
    "\tprint('Training was performed using the full dataset ({} data).'.format(len(train_dataloader.dataset)))\n",
    "\tfor epoch in range(num_train_epochs):\n",
    "\t\tmodel.train()\n",
    "\t\tfor step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "\t\t\tbatch = {key: value.to(device) for key, value in batch.items()}\n",
    "\t\t\twith amp.autocast():\n",
    "\t\t\t\toutputs = model(**batch)\n",
    "\t\t\t\tloss = outputs.loss\n",
    "\t\t\t\n",
    "\t\t\tscaler.scale(loss).backward()\n",
    "\n",
    "\t\t\tscaler.step(optimizer)\n",
    "\t\t\tscaler.update()\n",
    "\t\t\tlr_scheduler.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tprint('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "\tmodel_to_save = model.module if hasattr(model, 'module') else model \n",
    "\tmodel_to_save.save_pretrained(pretrain_model_dir)\n",
    "\tprint('TRAIN done!')\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "\n",
    "def compute_metrics_lowRes(start_logits, end_logits, features, examples):\n",
    "    example_to_features = defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = dict()\n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        example_id = example[\"qid\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers[example_id] = best_answer[\"text\"]\n",
    "        else:\n",
    "            predicted_answers[example_id] = \"\"\n",
    "\n",
    "    theoretical_answers = dict()\n",
    "    for ex in examples: theoretical_answers[ex[\"qid\"]] = ex[\"answers\"]\n",
    "    return evaluation(theoretical_answers, predicted_answers)\n",
    "\n",
    "def get_pred(dataloader, device, features, examples):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating_pred\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    if LOW_RES:\n",
    "        return compute_metrics_lowRes(start_logits, end_logits, features, examples)\n",
    "    else:\n",
    "        return compute_metrics(start_logits, end_logits, features, examples)\n",
    "\n",
    "def get_pretrain_pred(dataloader, device, features, examples):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(pretrain_model_dir).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating_pred\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    return compute_metrics(start_logits, end_logits, features, examples)\n",
    "\n",
    "def get_prob(dataloader, device, features, examples):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating_prob\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    prob_dict = {}\n",
    "    example_to_features = defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    for example in tqdm(examples):\n",
    "        if LOW_RES:\n",
    "            example_id = example[\"qid\"]\n",
    "        else:\n",
    "            example_id = example[\"id\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "        \n",
    "            if len(answers) > 1:\n",
    "                prob_dict[feature_index] = softmax(answers)\n",
    "            elif example_to_features[example_id] != []:\n",
    "                prob_dict[feature_index] = np.array([0])\n",
    "    \n",
    "    return prob_dict\n",
    "\n",
    "def get_prob_dropout(dataloader, device, features, examples, n_drop=10):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.train()\n",
    "    prob_dict = {}\n",
    "    \n",
    "    for i in range(n_drop):\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating_prob_dropout\"):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "            end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "        start_logits = np.concatenate(start_logits)\n",
    "        end_logits = np.concatenate(end_logits)\n",
    "        start_logits = start_logits[: len(features)]\n",
    "        end_logits = end_logits[: len(features)]\n",
    "\n",
    "        example_to_features = defaultdict(list)\n",
    "        max_answer_length = 30\n",
    "        n_best = 20\n",
    "            \n",
    "        for idx, feature in enumerate(features):\n",
    "            example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "        n = 0\n",
    "        for example in tqdm(examples):\n",
    "            if LOW_RES:\n",
    "                example_id = example[\"qid\"]\n",
    "            else:\n",
    "                example_id = example[\"id\"]\n",
    "            answers = []\n",
    "\n",
    "            # Loop through all features associated with that example\n",
    "            for feature_index in example_to_features[example_id]:\n",
    "                start_logit = start_logits[feature_index]\n",
    "                end_logit = end_logits[feature_index]\n",
    "                offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # Skip answers that are not fully in the context\n",
    "                        if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                            continue\n",
    "                        # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                        if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                        ):\n",
    "                            continue\n",
    "\n",
    "                        answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "\n",
    "            if 1 < len(answers) < 200: # pad to same numbers of possible answers\n",
    "                zero_list = [0] * (200 - len(answers))\n",
    "                answers.extend(zero_list)\n",
    "            elif len(answers) >= 200:\n",
    "                answers = answers[:200]\n",
    "\n",
    "            if len(answers) > 1:\n",
    "                if example_to_features[example_id][0] not in prob_dict:\n",
    "                    prob_dict[example_to_features[example_id][0]] = softmax(answers)\n",
    "                else:\n",
    "                    prob_dict[example_to_features[example_id][0]] += softmax(answers)\n",
    "            elif example_to_features[example_id] != []:\n",
    "                if example_to_features[example_id][0] not in prob_dict:\n",
    "                    prob_dict[example_to_features[example_id][0]] = np.array([0])   \n",
    "\n",
    "    for key in prob_dict.keys():\n",
    "        prob_dict[key] /= n_drop\n",
    "\n",
    "    return prob_dict\n",
    "\n",
    "def get_prob_dropout_split(dataloader, device, features, examples, n_drop=10):\n",
    "    ## use tensor to save the answers\n",
    "\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    probs = torch.zeros([n_drop, len(dataloader.dataset), 200])\n",
    "    \n",
    "    for i in range(n_drop):\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating_prob_dropout\"):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "            end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "        start_logits = np.concatenate(start_logits)\n",
    "        end_logits = np.concatenate(end_logits)\n",
    "        start_logits = start_logits[: len(features)]\n",
    "        end_logits = end_logits[: len(features)]\n",
    "\n",
    "        example_to_features = defaultdict(list)\n",
    "        max_answer_length = 30\n",
    "        n_best = 20\n",
    "            \n",
    "        for idx, feature in enumerate(features):\n",
    "            example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "        n = 0\n",
    "        for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "            if LOW_RES:\n",
    "                example_id = example[\"qid\"]\n",
    "            else:\n",
    "                example_id = example[\"id\"]\n",
    "            answers = []\n",
    "\n",
    "            # Loop through all features associated with that example\n",
    "            for feature_index in example_to_features[example_id]:\n",
    "                start_logit = start_logits[feature_index]\n",
    "                end_logit = end_logits[feature_index]\n",
    "                offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "                start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # Skip answers that are not fully in the context\n",
    "                        if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                            continue\n",
    "                        # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                        if (\n",
    "                            end_index < start_index\n",
    "                            or end_index - start_index + 1 > max_answer_length\n",
    "                        ):\n",
    "                            continue\n",
    "\n",
    "                        answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "\n",
    "            \n",
    "                if 1 < len(answers) < 200: # pad to same numbers of possible answers\n",
    "                    zero_list = [0] * (200 - len(answers))\n",
    "                    answers.extend(zero_list)\n",
    "                elif len(answers) >= 200:\n",
    "                    answers = answers[:200]\n",
    "\n",
    "                probs[i][feature_index] += torch.tensor(softmax(answers))\n",
    "\n",
    "    return probs\n",
    "\n",
    "def get_embeddings(dataloader, device):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir, output_hidden_states=True).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    embeddings = torch.zeros([len(dataloader.dataset), model.config.to_dict()['hidden_size']])\n",
    "    idxs_start = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating_prob\"):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        \n",
    "            outputs = model(**batch)\n",
    "\n",
    "            hidden_states = outputs.hidden_states\n",
    "            embedding_of_last_layer = hidden_states[-2][:, 0, :]\n",
    "\n",
    "            idxs_end = idxs_start + len(hidden_states[-2])\n",
    "            embeddings[idxs_start:idxs_end] = embedding_of_last_layer.cpu()\n",
    "            idxs_start = idxs_end\n",
    "        \n",
    "    return embeddings\n",
    "\n",
    "def get_grad_embeddings(dataloader, device, features, examples):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir, output_hidden_states=True).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    nLab = 20\n",
    "    embDim = model.config.to_dict()['hidden_size']\n",
    "    embeddings = np.zeros([len(dataloader.dataset), embDim * nLab])\n",
    "\n",
    "    prob_dict = []\n",
    "    idxs_start = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating_prob\"):\n",
    "            batch = {key: Variable(value.to(device)) for key, value in batch.items()}\n",
    "                \n",
    "            # deepAL+: out, e1 = self.clf(x)\n",
    "            outputs = model(**batch)\n",
    "            # deepAL+: e1 = e1.data.cpu().numpy()\n",
    "            hidden_states = outputs.hidden_states\n",
    "            embedding_of_last_layer = hidden_states[-2][:, 0, :]\n",
    "            embedding_of_last_layer = embedding_of_last_layer.data.cpu().numpy()\n",
    "\n",
    "            # matually create features batch\n",
    "            data_len_batch = len(outputs.start_logits)\n",
    "            idxs_end = idxs_start + data_len_batch\n",
    "            batch_idx = list(range(idxs_start, idxs_end))\n",
    "            batch_feat = features.select(batch_idx)\n",
    "            idxs_start = idxs_end\n",
    "\n",
    "            out = logits_to_prob(outputs.start_logits.cpu().numpy(), outputs.end_logits.cpu().numpy(), batch_feat, batch_idx, examples, 200)\n",
    "            batchProbs = F.softmax(out, dim=1).data.cpu().numpy()\n",
    "            maxInds = np.argmax(batchProbs, 1)\n",
    "\n",
    "            for j in range(data_len_batch):\n",
    "                for c in range(nLab):\n",
    "                    if c == maxInds[j]:\n",
    "                        embeddings[batch_idx[j]][embDim * c : embDim * (c+1)] = deepcopy(embedding_of_last_layer[j]) * (1 - batchProbs[j][c]) * -1.0\n",
    "                    else:\n",
    "                        embeddings[batch_idx[j]][embDim * c : embDim * (c+1)] = deepcopy(embedding_of_last_layer[j]) * (-1 * batchProbs[j][c]) * -1.0\n",
    "            \n",
    "    return embeddings\n",
    "\n",
    "def logits_to_prob(start_logits, end_logits, features, batch_idx, examples, num_classes):\n",
    "    probs = torch.zeros([len(batch_idx), num_classes])\n",
    "    \n",
    "    example_to_features = defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append((idx, batch_idx[idx]))\n",
    "    \n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        if LOW_RES:\n",
    "            example_id = example[\"qid\"]\n",
    "        else:\n",
    "            example_id = example[\"id\"]\n",
    "        answers = []\n",
    "        \n",
    "        # Loop through all features associated with that example\n",
    "        for (feature_index, i) in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "                    answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "\n",
    "\n",
    "            if 1 < len(answers) < num_classes: # pad to same numbers of possible answers\n",
    "                zero_list = [0] * (num_classes - len(answers))\n",
    "                answers.extend(zero_list)\n",
    "            elif len(answers) >= num_classes:\n",
    "                answers = answers[:num_classes]\n",
    "            probs[feature_index] = torch.tensor(answers)\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(n_pool, labeled_idxs, features, i):\n",
    "    print('Random querying starts.')\n",
    "\n",
    "    score_ordered_idxs = np.random.choice(np.where(labeled_idxs==0)[0], NUM_QUERY*2, replace=False)\n",
    "    if UNIQ_CONTEXT:\n",
    "        iter_i_labeled_idxs = get_us_uc(labeled_idxs, score_ordered_idxs, n_pool, features, i)\n",
    "    else:\n",
    "        iter_i_labeled_idxs = get_us(labeled_idxs, score_ordered_idxs, n_pool, features, i)\n",
    "\n",
    "    return iter_i_labeled_idxs\n",
    "\n",
    "def least_confidence(n_pool, labeled_idxs, dataset, features, examples, device, i):\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, dataset)\n",
    "    unlabeled_features = features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=MODEL_BATCH,\n",
    "\t)\n",
    "\n",
    "    print('LC querying starts.')\n",
    "    print('Query {} data from {} unlabeled training data.\\n'.format(NUM_QUERY, len(unlabeled_data)))\n",
    "\n",
    "    prob_dict = get_prob(unlabeled_dataloader, device, unlabeled_features, examples)\n",
    "    print('Got probability.')\n",
    "\n",
    "    confidence_dict = {}\n",
    "    for idx, probs in prob_dict.items():\n",
    "        if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "            confidence_dict[idx] = max(probs)\n",
    "        elif idx:\n",
    "            confidence_dict[idx] = np.array([0])\n",
    "\n",
    "    sorted_confidence_list = sorted(confidence_dict.items(), key=lambda x: x[1])\n",
    "    score_ordered_idxs = unlabeled_idxs[[idx for (idx, _) in sorted_confidence_list[:NUM_QUERY*2]]]\n",
    "    \n",
    "    if UNIQ_CONTEXT:\n",
    "        iter_i_labeled_idxs = get_us_uc(labeled_idxs, score_ordered_idxs, n_pool, features, i)\n",
    "    else:\n",
    "        iter_i_labeled_idxs = get_us(labeled_idxs, score_ordered_idxs, n_pool, features, i)\n",
    "\n",
    "    return iter_i_labeled_idxs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will preprocess the dataset (training and evaluation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4fee190d8c4fac9de7dbf5cf6512b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1353 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab2c620b8fb441dbdfad4c32a3e36b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1353 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b385dfe91eeb40b5a3b90a3c3a501fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44314e0014c4449bac7874a2aa0ce5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset, train_features, val_dataset, val_features = preprocess_data(train_data, val_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1127\n",
    "# os.environ['TORCH_HOME']='./basicmodel'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1)\n",
    "\n",
    "# fix random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "all_acc = []\n",
    "all_acc_dict = {}\n",
    "all_acc_em_dict = {}\n",
    "acq_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1415\n",
      "1415\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(train_features))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_ROUND = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67711a9c143490e8f7bd28c962940ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65489ed46a8f415088cda6b67529f947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterantion 0 done.\n",
      "Testing accuracy: 49.5043\n",
      "Testing accuracy em 40.6667\n"
     ]
    }
   ],
   "source": [
    "## record acc performance \n",
    "# acc = np.zeros(ITERATION + 1) # quota/batch runs + iter_0\n",
    "# acc_em = np.zeros(ITERATION + 1)\n",
    "acc = np.zeros(4) # quota/batch runs + iter_0\n",
    "acc_em = np.zeros(4)\n",
    "\n",
    "## generate initial labeled pool\n",
    "n_pool = len(train_dataset)\n",
    "labeled_idxs = np.zeros(n_pool, dtype=bool)\n",
    "\n",
    "\n",
    "if LOW_RES:\n",
    "    save_model(device, pretrain_model_dir, strategy_model_dir)\n",
    "else:\n",
    "    tmp_idxs = np.arange(n_pool)\n",
    "    np.random.shuffle(tmp_idxs)\n",
    "\n",
    "    \n",
    "    if UNIQ_CONTEXT:\n",
    "        iter_0_labeled_idxs = get_us_uc(labeled_idxs, tmp_idxs, n_pool, train_features)\n",
    "    else:\n",
    "        iter_0_labeled_idxs = get_us(labeled_idxs, tmp_idxs, n_pool, train_features)\n",
    "\n",
    "    ## load the selected train data to DataLoader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset.select(indices=iter_0_labeled_idxs),\n",
    "        shuffle=True,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=MODEL_BATCH,\n",
    "    )\n",
    "\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = NUM_TRAIN_EPOCH * num_update_steps_per_epoch\n",
    "\n",
    "    ## network\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(get_model(MODEL_NAME)).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    ## iteration 0 accuracy\n",
    "    to_train(NUM_TRAIN_EPOCH, train_dataloader, device, model, optimizer, lr_scheduler)\n",
    "\n",
    "## load the selected validation data to DataLoader\n",
    "eval_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=MODEL_BATCH\n",
    ")\n",
    "\n",
    "acc_scores_0 = get_pred(eval_dataloader, device, val_features, val_data) # add i=1 to use model from models_dir\n",
    "acc[0] = round(acc_scores_0['f1'], 4)\n",
    "acc_em[0] = round(acc_scores_0['exact_match'], 4)\n",
    "\n",
    "print('\\nIterantion 0 done.')\n",
    "print('Testing accuracy: {}'.format(acc[0]))\n",
    "print('Testing accuracy em {}'.format(acc_em[0]))\n",
    "# time = datetime.datetime.now()\n",
    "# print('Time spent for init training:', (time - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 1 in exp_round_1 start.\n",
      "LC querying starts.\n",
      "Query 50 data from 1415 unlabeled training data.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fccbcd7288f4a11a9c6bea19e7d7061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_prob:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd983ea2eda44f449f0f1330a87a4dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got probability.\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print('Iteraion {} in exp_round_{} start.'.format(i, EXP_ROUND))\n",
    "\t\t\n",
    "## query\n",
    "# iter_i_labeled_idxs = query(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, i)\n",
    "## iter_i_labeled_idxs = least_confidence(n_pool, labeled_idxs, train_dataset, train_features, train_data, device, i)\n",
    "unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "unlabeled_dataloader = DataLoader(\n",
    "    unlabeled_data,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=MODEL_BATCH,\n",
    ")\n",
    "\n",
    "print('LC querying starts.')\n",
    "print('Query {} data from {} unlabeled training data.\\n'.format(NUM_QUERY, len(unlabeled_data)))\n",
    "\n",
    "prob_dict = get_prob(unlabeled_dataloader, device, unlabeled_features, train_data)\n",
    "print('Got probability.')\n",
    "\n",
    "confidence_dict = {}\n",
    "for idx, probs in prob_dict.items():\n",
    "    if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "        confidence_dict[idx] = max(probs)\n",
    "    elif idx:\n",
    "        confidence_dict[idx] = np.array([0])\n",
    "\n",
    "sorted_confidence_list = sorted(confidence_dict.items(), key=lambda x: x[1])\n",
    "score_ordered_idxs = unlabeled_idxs[[idx for (idx, _) in sorted_confidence_list[:NUM_QUERY*2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_us_uc(labeled_idxs, score_ordered_idxs, n_pool, features, iteration=0):\n",
    "\tif LOW_RES:\n",
    "\t\ttotal = NUM_QUERY * iteration\n",
    "\t\tprint('Total num of label pool in LowRes:', total)\n",
    "\telse:\n",
    "\t\ttotal = NUM_QUERY * iteration + NUM_INIT_LB\n",
    "\t\tprint('Total num of label pool in regular:', total)\n",
    "\n",
    "\tssi = set()\n",
    "\tuc = set()\n",
    "\t\n",
    "\tsamples = features.select(indices=np.arange(n_pool)[labeled_idxs])\n",
    "\tif len(samples):\n",
    "\t\tfor sample in samples:\n",
    "\t\t\tssi.add(sample['example_id'])\n",
    "\t\t\tuc.add(sample['context'])\n",
    "\tassert len(ssi) == len(samples), \"The amount of ssi from previous query is wrong. There are only {} ssi.\".format(len(ssi))\n",
    "\tassert len(uc) == len(samples), \"The amount of uc from previous query is wrong. There are only {} uc.\".format(len(uc))\n",
    "\t\n",
    "\tfiltered_score_ordered_idx = []\n",
    "\tfor soi in score_ordered_idxs:\n",
    "\t\tpool_idxs = np.zeros(len(features), dtype=bool)\n",
    "\t\tpool_idxs[soi] = True\n",
    "\t\tsample = features.select(indices=np.arange(n_pool)[pool_idxs])\n",
    "\n",
    "\t\tif sample[0]['example_id'] not in ssi:\n",
    "\t\t\tif sample[0]['context'] not in uc:\n",
    "\t\t\t\tssi.add(sample[0]['example_id'])\n",
    "\t\t\t\tuc.add(sample[0]['context'])\n",
    "\t\t\t\tfiltered_score_ordered_idx.append(soi)\n",
    "\n",
    "\t\tif len(filtered_score_ordered_idx) == total:\n",
    "\t\t\tprint('Break the loop, we have {} unique ssi having unique context.'.format(len(ssi)))\n",
    "\t\t\tbreak\n",
    "\t\t\t\n",
    "\tprint('\\nFinally, we have {} instances.'.format(len(filtered_score_ordered_idx)))\n",
    "\tlabeled_idxs[filtered_score_ordered_idx] = True\n",
    "\treturn np.arange(n_pool)[labeled_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of label pool in LowRes: 50\n",
      "Break the loop, we have 50 unique ssi having unique context.\n",
      "\n",
      "Finally, we have 50 instances.\n"
     ]
    }
   ],
   "source": [
    "if UNIQ_CONTEXT:\n",
    "    iter_i_labeled_idxs = get_us_uc(labeled_idxs, score_ordered_idxs, n_pool, train_features, i)\n",
    "else:\n",
    "    iter_i_labeled_idxs = get_us(labeled_idxs, score_ordered_idxs, n_pool, train_features, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alqa",
   "language": "python",
   "name": "alqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6317ac8a4c65d05b4cf6bac76f72bfaae40b2e9380067c26c20c9afff1d8528e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
