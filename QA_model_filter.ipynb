{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Model \n",
    "## no trainer\n",
    "\n",
    "- dataset\n",
    "- torch\n",
    "- transformers\n",
    "- transformers[torch]\n",
    "- evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    BertConfig\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/mount/arbeitsdaten31/studenten1/linku/.cache\n",
      "env: HF_MODULES_CACHE=/mount/arbeitsdaten31/studenten1/linku/.cache\n",
      "env: HF_DATASETS_CACHE=/mount/arbeitsdaten31/studenten1/linku/.cache\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR='/mount/arbeitsdaten31/studenten1/linku/.cache'\n",
    "%set_env TRANSFORMERS_CACHE $CACHE_DIR\n",
    "%set_env HF_MODULES_CACHE $CACHE_DIR\n",
    "%set_env HF_DATASETS_CACHE $CACHE_DIR\n",
    "\n",
    "model_dir = '/mount/arbeitsdaten31/studenten1/linku/models'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arguments.py\n",
    "\n",
    "args_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_input_ALstrategy = 'RandomSampling'\n",
    "args_input_initseed = 100 # 1000\n",
    "args_input_quota = 90 # 1000\n",
    "args_input_batch = 30 # 128\n",
    "args_input_dataset_name = 'SQuAD'\n",
    "args_input_iteration = 5\n",
    "args_input_model_batch = 8 # already add in arguments.py\n",
    "args_input_max_length = None\n",
    "args_input_learning_rate = 1e-4\n",
    "args_input_model = 'RoBERTa'\n",
    "\n",
    "stride = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = args_input_max_length\n",
    "NUM_QUERY = args_input_batch\n",
    "NUM_INIT_LB = args_input_initseed\n",
    "NUM_ROUND = int(args_input_quota / args_input_batch)\n",
    "DATA_NAME = args_input_dataset_name\n",
    "STRATEGY_NAME = args_input_ALstrategy\n",
    "MODEL_NAME = args_input_model\n",
    "LEARNING_RATE = args_input_learning_rate\n",
    "strategy_model_dir = model_dir + '/' + str(NUM_INIT_LB) + '_' + str(args_input_quota) + '_' + STRATEGY_NAME + '_' + MODEL_NAME +  '_' + DATA_NAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/mount/arbeitsdaten31/studenten1/linku/.cache/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4584d22311754bb9948983e32ff0603b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = load_dataset(args_input_dataset_name.lower(), cache_dir=CACHE_DIR)\n",
    "# squad[\"train\"] = squad[\"train\"].shuffle(42).select(range(2000))\n",
    "squad[\"train\"] = squad[\"train\"].select(range(4000))\n",
    "squad[\"validation\"] = squad[\"validation\"].select(range(1200))\n",
    "# squad[\"train\"] = squad[\"train\"].select(range(670, 680))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(squad[\"train\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for preprocessing the dataset (training and evaluation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_features(examples, tokenizer):\n",
    "    # keep [\"offset_mapping\"], for compute_metrics()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        example_ids.append(examples[\"id\"][sample_idx]) # newly added for used in unlabel data predict\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples, tokenizer):\n",
    "    # no ['offset_mapping'], for .train() and .eval()\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        example_ids.append(examples[\"id\"][sample_idx]) # newly added for used in unlabel data predict\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples, tokenizer):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /mount/arbeitsdaten31/studenten1/linku/.cache/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-3faa3291c999dafc.arrow\n",
      "Loading cached processed dataset at /mount/arbeitsdaten31/studenten1/linku/.cache/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-5109a7b4f5a5a8e4.arrow\n",
      "Loading cached processed dataset at /mount/arbeitsdaten31/studenten1/linku/.cache/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-5fea3a5e35231e0f.arrow\n",
      "Loading cached processed dataset at /mount/arbeitsdaten31/studenten1/linku/.cache/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-5fea3a5e35231e0f.arrow\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer for dataset preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# preprocess data\n",
    "train_dataset = squad[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"train\"].column_names,\n",
    "\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "train_features = squad[\"train\"].map(\n",
    "    preprocess_training_features,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"train\"].column_names,\n",
    "\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "val_dataset = squad[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"validation\"].column_names,\n",
    "\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "val_features = squad[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"validation\"].column_names,\n",
    "\tfn_kwargs=dict(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "train_features.set_format(\"torch\")\n",
    "val_dataset = val_dataset.remove_columns([\"offset_mapping\"])\n",
    "val_dataset.set_format(\"torch\")\n",
    "val_features.set_format(\"torch\")\n",
    "\n",
    "# get the number of extra data after preprocessing\n",
    "extra = len(train_dataset) - len(squad['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4009"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train(num_train_epochs, train_dataloader, device, model, optimizer, lr_scheduler, record_loss=False):\n",
    "\tprint('Training was performed using the sum of {} initial data and {} query data, i.e. {} data.'.format(NUM_INIT_LB, NUM_QUERY, len(train_dataloader.dataset)))\n",
    "\tfor epoch in range(num_train_epochs):\n",
    "\t\tmodel.train()\n",
    "\t\tfor step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "\t\t\tbatch = {key: value.to(device) for key, value in batch.items()}\n",
    "\t\t\toutputs = model(**batch)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tlr_scheduler.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tif record_loss:\n",
    "\t\t\tprint('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "\tmodel_to_save = model.module if hasattr(model, 'module') else model \n",
    "\tmodel_to_save.save_pretrained(strategy_model_dir)\n",
    "\tprint('TRAIN done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(dataloader, device, features, examples, lowRes=False):\n",
    "    if lowRes:\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(pretrain_model_dir).to(device)\n",
    "    else:\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating_pred\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    return compute_metrics(start_logits, end_logits, features, examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(dataloader, device, features, examples, lowRes=False):\n",
    "    if lowRes:\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(pretrain_model_dir).to(device)\n",
    "    else:\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating_prob\"):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(features)]\n",
    "    end_logits = end_logits[: len(features)]\n",
    "\n",
    "    prob_dict = {}\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answers.append(start_logit[start_index] + end_logit[end_index])\n",
    "        \n",
    "            if len(answers) > 1:\n",
    "                prob_dict[feature_index] = softmax(answers)\n",
    "            elif example_to_features[example_id] != []:\n",
    "                prob_dict[feature_index] = np.array([0])\n",
    "    \n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    max_answer_length = 30\n",
    "    n_best = 20\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples, desc=\"Computing metrics\"):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unlabel_data(n_pool, labeled_idxs, train_dataset):\n",
    "    unlabeled_idxs = np.arange(n_pool)[~labeled_idxs]\n",
    "    unlabeled_data = train_dataset.select(indices=unlabeled_idxs)\n",
    "    return unlabeled_idxs, unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    softmax_num = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    return np.round(softmax_num, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aubc(quota, bsize, resseq):\n",
    "\t# it is equal to use np.trapz for calculation\n",
    "\tressum = 0.0\n",
    "\tif quota % bsize == 0:\n",
    "\t\tfor i in range(len(resseq)-1):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\n",
    "\telse:\n",
    "\t\tfor i in range(len(resseq)-2):\n",
    "\t\t\tressum = ressum + (resseq[i+1] + resseq[i]) * bsize / 2\n",
    "\t\tk = quota % bsize\n",
    "\t\tressum = ressum + ((resseq[-1] + resseq[-2]) * k / 2)\n",
    "\tressum = round(ressum / quota,3)\n",
    "\t\n",
    "\treturn ressum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_stddev(datax):\n",
    "\treturn round(np.mean(datax),4),round(np.std(datax),4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling_query(labeled_idxs, n):\n",
    "    return np.random.choice(np.where(labeled_idxs==0)[0], n, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_confidence_query(n_pool, labeled_idxs, train_dataset, train_features, examples, device, n):\n",
    "    # deepAL+: unlabeled_idxs, unlabeled_data = self.dataset.get_unlabeled_data()\n",
    "    unlabeled_idxs, unlabeled_data = get_unlabel_data(n_pool, labeled_idxs, train_dataset)\n",
    "    unlabeled_features = train_features.select(unlabeled_idxs)\n",
    "    unlabeled_dataloader = DataLoader(\n",
    "\t\tunlabeled_data,\n",
    "\t\tshuffle=False,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=8,\n",
    "\t)\n",
    "    # TODO: print for recording\n",
    "    print('LC querying starts!')\n",
    "    # deepAL+: probs = self.predict_prob(unlabeled_data)\n",
    "    prob_dict = get_prob(unlabeled_dataloader, device, unlabeled_features, examples)\n",
    "    # TODO: print for recording\n",
    "    print('Got probability!')\n",
    "    # deepAL+: uncertainties = probs.max(1)[0]\n",
    "    confidence_dict = {}\n",
    "    for idx, probs in prob_dict.items():\n",
    "        if len(probs) > 1: # if prob_dict['probs'] is not 0\n",
    "            confidence_dict[idx] = max(probs)\n",
    "        elif idx:\n",
    "            confidence_dict[idx] = np.array([0])\n",
    "\n",
    "    # deepAL+: return unlabeled_idxs[uncertainties.sort()[1][:n]]\n",
    "    sorted_confidence_list = sorted(confidence_dict.items, key=lambda x: x[1])\n",
    "    return unlabeled_idxs[[idx for (idx, confidence) in sorted_confidence_list[:n]]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1127\n",
    "# os.environ['TORCH_HOME']='./basicmodel'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(2)\n",
    "\n",
    "# fix random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.enabled  = True\n",
    "# torch.backends.cudnn.benchmark= True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATION = args_input_iteration\n",
    "MODEL_BATCH = args_input_model_batch\n",
    "# NUM_TRAIN_EPOCH = args_input.train_epochs\n",
    "NUM_TRAIN_EPOCH = 3\n",
    "\n",
    "all_acc = []\n",
    "acq_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD\n",
      "RandomSampling\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 100 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553c893e85ae42cf96445e09e85835a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba5a49cd8d14bb683275ff139270d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53cfb4bdac244b6aa1da3c6589d516c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d919ea43c1b4311a96a3943923b0fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddf3464793648a4ad9089fea2f3c24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "testing accuracy 19.615624405461663\n",
      "testing accuracy em 13.083333333333334\n",
      "Time spent for init training: 0:01:28.881604\n",
      "\n",
      "\n",
      "Round 1 in Iteration 1\n",
      "Time spent for querying: 0:00:00.000888\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 130 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8a9416ac524e7b847c6dee36c57954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647f9b6f74ad444ab2668493a1f377c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8d00187a2e493cab404a2cc4e68da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_1 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d3f56def5349939bba3b7fe0c26029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467806b7144d49c28198652da2af3698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 29.60985733247591\n",
      "testing accuracy em 22.0\n",
      "Time spent for training after querying: 0:01:32.936987\n",
      "\n",
      "\n",
      "Round 2 in Iteration 1\n",
      "Time spent for querying: 0:00:00.289659\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 160 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649965fc4ca4418fbb4e1bbebaff9bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660d864bdf6047aabae179bdca3cdbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3de85519f4a467fbf13034e1784e150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_2 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c519db75b2fb4075929fdbbd746d053e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89910e5139dc4b5cae83addef64c61a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 40.80897464294515\n",
      "testing accuracy em 31.583333333333332\n",
      "Time spent for training after querying: 0:01:42.923820\n",
      "\n",
      "\n",
      "Round 3 in Iteration 1\n",
      "Time spent for querying: 0:00:00.281211\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 190 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7bf4b837584e698324cf252037fbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805fae01d82c447bbcc2570998f7be52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcac3712035440489d040d70aac5ac35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_3 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e99a92a1a664bbebb7729ec83ca07df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7329245d9f6648cf8b554028c846287f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 42.06004921442168\n",
      "testing accuracy em 33.583333333333336\n",
      "Time spent for training after querying: 0:01:49.175038\n",
      "\n",
      "\n",
      "SEED 1127\n",
      "RandomSampling\n",
      "[19.61562441 29.60985733 40.80897464 42.06004921]\n",
      "Time spent in iteration 1: 0:06:34.789439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD\n",
      "RandomSampling\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 100 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93db9b2e6d224dc2a1a24c51e95506b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1d09a2eedb495798a02b42fab0543b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdc49f149bb45c1a0503d00bca1451a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f0555e61574b04a4c4b694b348c00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9743f05354d4befad915edf47c93b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "testing accuracy 11.921907115317625\n",
      "testing accuracy em 7.166666666666667\n",
      "Time spent for init training: 0:01:26.528355\n",
      "\n",
      "\n",
      "Round 1 in Iteration 2\n",
      "Time spent for querying: 0:00:00.001152\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 130 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10db44c9d6f24b6cbb6b02782c6a994d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8806dbc1eb3e49749d533c338b3dc136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec7e12631a741afad1e4572f5cc74a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_1 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a839f58cc4b4576987f2fa2f3de6968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42893037d3648dfb3522c5e1a8f0fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 15.724019813892113\n",
      "testing accuracy em 10.75\n",
      "Time spent for training after querying: 0:01:34.228541\n",
      "\n",
      "\n",
      "Round 2 in Iteration 2\n",
      "Time spent for querying: 0:00:00.325824\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 160 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73823ce46f504bd5a4f597edc64fdad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a794dd105554971a2eb1702e6a2fbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cf288f3e4d4d2097a86db4ab8a2842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_2 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef0c291893e42008a33946e6503bed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6b8b6aed0d48079e3126e42cf0c403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 20.846701033628175\n",
      "testing accuracy em 15.5\n",
      "Time spent for training after querying: 0:01:46.380528\n",
      "\n",
      "\n",
      "Round 3 in Iteration 2\n",
      "Time spent for querying: 0:00:00.288264\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 190 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79be7a2e5db94c3d900a7b4889ca7e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06fbccde324475b884ed80e4295049a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6b50086cdd490d80f0a875e5a455fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_3 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c947a62449024202a3634fa7ab563f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e229a3427e4e21970e9378c3a43833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 35.29815255375614\n",
      "testing accuracy em 27.666666666666668\n",
      "Time spent for training after querying: 0:01:49.708837\n",
      "\n",
      "\n",
      "SEED 1127\n",
      "RandomSampling\n",
      "[11.92190712 15.72401981 20.84670103 35.29815255]\n",
      "Time spent in iteration 2: 0:13:18.843675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD\n",
      "RandomSampling\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 100 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03140f340bd243cd9a50366e5b27873a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20433dc300d4386aeaa9a05ba0a90e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b36e47c8cf44f1dba69796c3b3d10bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9321afa31e7463aa8c4ca300c653e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c08232100ca456d83035e470601edb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "testing accuracy 16.96558218089185\n",
      "testing accuracy em 10.416666666666666\n",
      "Time spent for init training: 0:01:26.225082\n",
      "\n",
      "\n",
      "Round 1 in Iteration 3\n",
      "Time spent for querying: 0:00:00.000804\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 130 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff91a80df17f4c30b00117d518cb99bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ead47aaa009430d89ce652aa7458131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b5d3cee2b44b92864ee0a094915cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_1 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5785173cf04d388ed3cacf73e0b8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4001f6d1864f6db33a964e9d702e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 28.151801593119618\n",
      "testing accuracy em 22.083333333333332\n",
      "Time spent for training after querying: 0:01:36.513933\n",
      "\n",
      "\n",
      "Round 2 in Iteration 3\n",
      "Time spent for querying: 0:00:00.317919\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 160 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088486f83c7e451a8b7cca2f35749b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8ce2f2338d44598136aefe8da1c308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3721205fc05f4309a08a391d57a5701b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_2 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3665f12af57145ee81e1e52631dafc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312007de52af495a869a6070d6b7bc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 37.35432185348445\n",
      "testing accuracy em 29.833333333333332\n",
      "Time spent for training after querying: 0:01:50.367065\n",
      "\n",
      "\n",
      "Round 3 in Iteration 3\n",
      "Time spent for querying: 0:00:00.327477\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 190 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583bc2306b9d479a9eb946d19c756af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223a952d976c42678265581903c5b62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb36dd25a0954a18856a187941c5a3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_3 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd245506db3463d97a33850157cd0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2184cecb217a48c9ac9d1bebffbbbc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 46.74545300741741\n",
      "testing accuracy em 39.583333333333336\n",
      "Time spent for training after querying: 0:01:48.434524\n",
      "\n",
      "\n",
      "SEED 1127\n",
      "RandomSampling\n",
      "[16.96558218 28.15180159 37.35432185 46.74545301]\n",
      "Time spent in iteration 3: 0:20:07.839963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD\n",
      "RandomSampling\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 100 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878eb6f38b9f414d8c35c5d338dc860e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf05f076a314faea853e852c62df927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b323d42a55d44721a85d867d417d81ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bfcaed024d4d9aa53a7dd32910b54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af72f6d9f9494c41970bfe869fb2cd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "testing accuracy 11.59979995202721\n",
      "testing accuracy em 7.833333333333333\n",
      "Time spent for init training: 0:01:26.242925\n",
      "\n",
      "\n",
      "Round 1 in Iteration 4\n",
      "Time spent for querying: 0:00:00.000932\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 130 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61947a4187dd457b89972cd124d7b2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b4385bef804c0eb168e030dc128f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142cf774b5414a4e85129d381f204c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_1 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73557f162afb481c807549a404ff4a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3554b735f34cf18d1e1d0f5e285cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 17.19505919268186\n",
      "testing accuracy em 12.666666666666666\n",
      "Time spent for training after querying: 0:01:32.914909\n",
      "\n",
      "\n",
      "Round 2 in Iteration 4\n",
      "Time spent for querying: 0:00:00.313557\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 160 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3b068b2b564b7f9fc0dced1efdb871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1839de6b6f834b2e832803dd9f952475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3424e4185223404ab03b22e53d65e23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_2 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cd6310f71d47049df0c2b53680d918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd770964b9546d3942940c416e83523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 28.208959186128315\n",
      "testing accuracy em 21.416666666666668\n",
      "Time spent for training after querying: 0:01:43.053982\n",
      "\n",
      "\n",
      "Round 3 in Iteration 4\n",
      "Time spent for querying: 0:00:00.280128\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 190 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea15ab7dcb954e769828886a91ce956f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08a898b4947485b9c152994721f583f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d539f145e7c047b299522c454c9fda2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_3 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f725ed1e572c461f9c8b14faad81c9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4dd174a968c4a8a8bd051384e573a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 31.673036499827024\n",
      "testing accuracy em 23.916666666666668\n",
      "Time spent for training after querying: 0:01:53.032492\n",
      "\n",
      "\n",
      "SEED 1127\n",
      "RandomSampling\n",
      "[11.59979995 17.19505919 28.20895919 31.6730365 ]\n",
      "Time spent in iteration 4: 0:26:50.285096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD\n",
      "RandomSampling\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 100 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dae48e205814be0a30f74867d8af8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bf6fedc35c4f46a1e99eaf3c1e9dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19935666c7a04cda914e0491b2c4a5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e533914dea14c00b334a420d4ea1dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799e557a4a304b60bc5b3ecbcf91be90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "testing accuracy 22.698700551223624\n",
      "testing accuracy em 15.5\n",
      "Time spent for init training: 0:01:29.311562\n",
      "\n",
      "\n",
      "Round 1 in Iteration 5\n",
      "Time spent for querying: 0:00:00.000872\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 130 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf6948c55a442ddb40a8f5ff767c7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ebce3a8d394bdbb43124951a267d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c432a19810448aabc248332ef3de276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_1 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeed41ee352b4bb1be792cc6fbbb48b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f9ce3955b944889fffb5d87d521954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 32.21582186841597\n",
      "testing accuracy em 22.666666666666668\n",
      "Time spent for training after querying: 0:01:35.666288\n",
      "\n",
      "\n",
      "Round 2 in Iteration 5\n",
      "Time spent for querying: 0:00:00.302900\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 160 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0966f95d7e984851bc6bc75a4405aaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf854eec682944808192d844497be4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc0c30470c346fdab953fc2e52ae1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_2 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461864d7c295400ba08ef19294e57a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4010d1e3b55549078fded51a8714e9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 36.851237826586065\n",
      "testing accuracy em 28.916666666666668\n",
      "Time spent for training after querying: 0:01:42.545570\n",
      "\n",
      "\n",
      "Round 3 in Iteration 5\n",
      "Time spent for querying: 0:00:00.274390\n",
      "Training was performed using the sum of 100 initial data and 30 query data, i.e. 190 data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39a9f8e038c4f768bc0570f09af3c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddce2d6e9f404b7c858c1d49a12d016f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764d10cd107c499e834bc7cd183f086d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN done!\n",
      "rd_3 get_pred!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77464d570eb74b01beff2487a8599aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating_pred:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4385620c065e43999b1d1bf0f54eb7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy 35.88593562640517\n",
      "testing accuracy em 27.833333333333332\n",
      "Time spent for training after querying: 0:01:54.154897\n",
      "\n",
      "\n",
      "SEED 1127\n",
      "RandomSampling\n",
      "[22.69870055 32.21582187 36.85123783 35.88593563]\n",
      "Time spent in iteration 5: 0:33:38.954103\n",
      "Time spent in total: 0:33:44.804133\n",
      "0: 33.752\n",
      "1: 20.06\n",
      "2: 32.454\n",
      "3: 22.347\n",
      "4: 32.786\n",
      "mean AUBC(acc): 28.2798. std dev AUBC(acc): 5.8385\n",
      "mean time: 398.2. std dev time: 3.2496\n"
     ]
    }
   ],
   "source": [
    "begin = datetime.datetime.now()\n",
    "\n",
    "# repeate # iteration trials\n",
    "while (ITERATION > 0): \n",
    "\tITERATION = ITERATION - 1\n",
    "\t\n",
    "\tstart = datetime.datetime.now()\n",
    "\n",
    "\t## generate initial labeled pool\n",
    "\tn_pool = len(train_dataset)\n",
    "\tlabeled_idxs = np.zeros(n_pool, dtype=bool)\n",
    "\n",
    "\ttmp_idxs = np.arange(n_pool)\n",
    "\tnp.random.shuffle(tmp_idxs)\n",
    "\t\n",
    "\tdifference_0 = 0\n",
    "\tnum_set_ex_id_0 = 0\n",
    "\n",
    "\twhile num_set_ex_id_0 != NUM_INIT_LB:        \n",
    "\t\tlabeled_idxs[tmp_idxs[:NUM_INIT_LB + difference_0]] = True\n",
    "\t\trun_0_labeled_idxs = np.arange(n_pool)[labeled_idxs]\n",
    "\n",
    "\t\trun_0_samples = train_features.select(indices=run_0_labeled_idxs)\n",
    "\t\tnum_set_ex_id_0 = len(set(run_0_samples['example_id']))\n",
    "\n",
    "\t\tdifference_0 = NUM_INIT_LB - num_set_ex_id_0\n",
    "\n",
    "\t## record acc performance \n",
    "\tacc = np.zeros(NUM_ROUND + 1) # quota/batch runs + run_0\n",
    "\tacc_em = np.zeros(NUM_ROUND + 1)\n",
    "\n",
    "\t## load the selected train data to DataLoader\n",
    "\ttrain_dataloader = DataLoader(\n",
    "\t\ttrain_dataset.select(indices=run_0_labeled_idxs),\n",
    "\t\tshuffle=True,\n",
    "\t\tcollate_fn=default_data_collator,\n",
    "\t\tbatch_size=MODEL_BATCH,\n",
    "\t)\n",
    "\n",
    "\teval_dataloader = DataLoader(\n",
    "\t\tval_dataset, \n",
    "\t\tcollate_fn=default_data_collator, \n",
    "\t\tbatch_size=MODEL_BATCH\n",
    "\t)\n",
    "\n",
    "\tnum_update_steps_per_epoch = len(train_dataloader)\n",
    "\tnum_training_steps = NUM_TRAIN_EPOCH * num_update_steps_per_epoch\n",
    "\n",
    "    ## network\n",
    "\tmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\toptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\t\n",
    "\tlr_scheduler = get_scheduler(\n",
    "\t\t\"linear\",\n",
    "\t\toptimizer=optimizer,\n",
    "\t\tnum_warmup_steps=0,\n",
    "\t\tnum_training_steps=num_training_steps,\n",
    "\t)\n",
    "\n",
    "\t## print info\n",
    "\tprint(DATA_NAME)\n",
    "\tprint(STRATEGY_NAME)\n",
    "\t\n",
    "\t## round 0 accuracy\n",
    "\tto_train(NUM_TRAIN_EPOCH, train_dataloader, device, model, optimizer, lr_scheduler)\n",
    "\t\n",
    "\tacc_scores_0 = get_pred(eval_dataloader, device, val_features, squad['validation']) # add rd=1 to use model from models_dir\n",
    "\tacc[0] = acc_scores_0['f1']\n",
    "\tacc_em[0] = acc_scores_0['exact_match']\n",
    "\n",
    "\tprint('Round 0\\ntesting accuracy {}'.format(acc[0]))\n",
    "\tprint('testing accuracy em {}'.format(acc_em[0]))\n",
    "\ttime = datetime.datetime.now()\n",
    "\tprint('Time spent for init training:', (time - start))\n",
    "\tprint('\\n')\n",
    "\t\n",
    "\t## round 1 to rd\n",
    "\tfor rd in range(1, NUM_ROUND+1):\n",
    "\t\tprint('Round {} in Iteration {}'.format(rd, 5 - ITERATION))\n",
    "\n",
    "\t\t## use total_query (NUM_QUERY + extra) to query instead of just NUM_QUERY\n",
    "\t\ttotal_query = NUM_QUERY + extra\n",
    "\t\t\n",
    "\t\t## query\n",
    "\t\tif STRATEGY_NAME == 'RandomSampling':\n",
    "\t\t\tq_idxs = random_sampling_query(labeled_idxs, total_query)\n",
    "\t\telif STRATEGY_NAME == 'LeastConfidence':\n",
    "\t\t\tq_idxs = least_confidence_query(n_pool, labeled_idxs, train_dataset, train_features, squad['train'], device, total_query)\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError\n",
    "\n",
    "\t\tprint('Time spent for querying:', (datetime.datetime.now() - time))\n",
    "\t\ttime = datetime.datetime.now()\n",
    "\n",
    "\t\t## update\n",
    "\t\t \n",
    "\t\t## goal of total query data: sum NUM_QUERY and the number of set run_0_data\n",
    "\t\tnum_set_query_rd = NUM_QUERY * rd + NUM_INIT_LB\n",
    "\t\t\n",
    "\t\tdifference_rd = 0\n",
    "\t\tnum_set_ex_id_rd = 0\n",
    "\n",
    "\t\twhile num_set_ex_id_rd != num_set_query_rd:        \n",
    "\t\t\tlabeled_idxs[q_idxs[:NUM_QUERY + difference_rd]] = True\n",
    "\t\t\trun_rd_labeled_idxs = np.arange(n_pool)[labeled_idxs]\n",
    "\n",
    "\t\t\trun_rd_samples = train_features.select(indices=run_rd_labeled_idxs)\n",
    "\t\t\tnum_set_ex_id_rd = len(set(run_rd_samples['example_id']))\n",
    "\n",
    "\t\t\tdifference_rd = num_set_query_rd - num_set_ex_id_rd\n",
    "\n",
    "\t\ttrain_dataloader_rd = DataLoader(\n",
    "\t\t\ttrain_dataset.select(indices=run_rd_labeled_idxs),\n",
    "\t\t\tshuffle=True,\n",
    "\t\t\tcollate_fn=default_data_collator,\n",
    "\t\t\tbatch_size=MODEL_BATCH,\n",
    "\t\t)\n",
    "\n",
    "\t\tnum_update_steps_per_epoch_rd = len(train_dataloader_rd)\n",
    "\t\tnum_training_steps_rd = NUM_TRAIN_EPOCH * num_update_steps_per_epoch_rd\n",
    "\n",
    "\t\tmodel_rd = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "\t\toptimizer_rd = AdamW(model_rd.parameters(), lr=LEARNING_RATE)\n",
    "\t\t\n",
    "\t\tlr_scheduler_rd = get_scheduler(\n",
    "\t\t\t\"linear\",\n",
    "\t\t\toptimizer=optimizer_rd,\n",
    "\t\t\tnum_warmup_steps=0,\n",
    "\t\t\tnum_training_steps=num_training_steps_rd,\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t## train\n",
    "\t\tto_train(NUM_TRAIN_EPOCH, train_dataloader_rd, device, model_rd, optimizer_rd, lr_scheduler_rd)\n",
    "\n",
    "\t\t## round rd accuracy\n",
    "\t\tprint('rd_{} get_pred!'.format(rd))\n",
    "\t\tacc_scores_rd = get_pred(eval_dataloader, device, val_features, squad['validation'])\n",
    "\t\tacc[rd] = acc_scores_rd['f1']\n",
    "\t\tacc_em[rd] = acc_scores_rd['exact_match']\n",
    "\t\tprint('testing accuracy {}'.format(acc[rd]))\n",
    "\t\tprint('testing accuracy em {}'.format(acc_em[rd]))\n",
    "\t\tprint('Time spent for training after querying:', (datetime.datetime.now() - time))\n",
    "\t\ttime = datetime.datetime.now()\n",
    "\t\tprint('\\n')\n",
    "\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\t\n",
    "\t## print results\n",
    "\tprint('SEED {}'.format(SEED))\n",
    "\tprint(STRATEGY_NAME)\n",
    "\tprint(acc)\n",
    "\tall_acc.append(acc)\n",
    "\t\n",
    "\t## save model and record acq time\n",
    "\ttimestamp = re.sub('\\.[0-9]*', '_', str(datetime.datetime.now())).replace(\" \", \"_\").replace(\"-\", \"\").replace(\":\",\"\")\n",
    "\tfinal_model_dir = model_dir + '/' + timestamp + str(NUM_INIT_LB) + '_' + str(args_input_quota) + '_' + STRATEGY_NAME + '_' + MODEL_NAME + '_' + DATA_NAME\n",
    "\tos.makedirs(final_model_dir, exist_ok=True)\n",
    "\tend = datetime.datetime.now()\n",
    "\tprint('Time spent in iteration {}: {}'.format(5 - ITERATION, datetime.datetime.now() - begin))\n",
    "\tacq_time.append(round(float((end-start).seconds), 3))\n",
    "\n",
    "\tfinal_model = AutoModelForQuestionAnswering.from_pretrained(strategy_model_dir).to(device)\n",
    "\tmodel_to_save = final_model.module if hasattr(final_model, 'module') else final_model \n",
    "\tmodel_to_save.save_pretrained(final_model_dir)\n",
    "\n",
    "# cal mean & standard deviation\n",
    "print('Time spent in total:', (datetime.datetime.now() - begin))\n",
    "acc_m = []\n",
    "file_name_res = str(NUM_INIT_LB) + '_' + str(args_input_quota) + '_' + STRATEGY_NAME + '_' + MODEL_NAME + '_' + DATA_NAME + '_normal_res.txt'\n",
    "file_res =  open(os.path.join(os.path.abspath('') + '/results', '%s' % file_name_res),'w')\n",
    "\n",
    "# save result\n",
    "for i in range(len(all_acc)):\n",
    "\tacc_m.append(get_aubc(args_input_quota, NUM_QUERY, all_acc[i]))\n",
    "\tprint(str(i) + ': ' + str(acc_m[i]))\n",
    "\tfile_res.writelines(str(i) + ': ' + str(acc_m[i]) + '\\n')\n",
    "mean_acc, stddev_acc = get_mean_stddev(acc_m)\n",
    "mean_time, stddev_time = get_mean_stddev(acq_time)\n",
    "\n",
    "print('mean AUBC(acc): ' + str(mean_acc) + '. std dev AUBC(acc): ' + str(stddev_acc))\n",
    "print('mean time: ' + str(mean_time) + '. std dev time: ' + str(stddev_time))\n",
    "\n",
    "avg_acc = np.mean(np.array(all_acc),axis=0)\n",
    "for i in range(len(avg_acc)):\n",
    "\ttmp = 'Size of training set is ' + str(NUM_INIT_LB + i * NUM_QUERY) + ', ' + 'accuracy is ' + str(round(avg_acc[i],4)) + '.' + '\\n'\n",
    "\tfile_res.writelines(tmp)\n",
    "\n",
    "file_res.writelines('mean acc: ' + str(mean_acc) + '. std dev acc: ' + str(stddev_acc) + '\\n')\n",
    "file_res.writelines('mean time: ' + str(mean_time) + '. std dev acc: ' + str(stddev_time) + '\\n')\n",
    "\n",
    "file_res.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alqa",
   "language": "python",
   "name": "alqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6317ac8a4c65d05b4cf6bac76f72bfaae40b2e9380067c26c20c9afff1d8528e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
